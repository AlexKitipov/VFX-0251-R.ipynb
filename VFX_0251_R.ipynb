{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKitipov/VFX-0251-R.ipynb/blob/main/VFX_0251_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qg-VNcWrGqOJ"
      },
      "outputs": [],
      "source": [
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ AI –ü–ª–∞—Ç—Ñ–æ—Ä–º–∏\n",
        "\n",
        "#–ï—Ç–æ –ø—Ä–µ–≥–ª–µ–¥ –Ω–∞ –Ω—è–∫–æ–∏ –æ—Ç –≤–æ–¥–µ—â–∏—Ç–µ –æ–±–ª–∞—á–Ω–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ –∑–∞ –∏–∑–∫—É—Å—Ç–≤–µ–Ω –∏–Ω—Ç–µ–ª–µ–∫—Ç:\n",
        "\n",
        "#üß† **1. Google Cloud AI Platform**\n",
        "#- –ü—Ä–µ–¥–ª–∞–≥–∞ AutoML, TensorFlow, PyTorch –∏ JAX —Å—Ä–µ–¥–∏.\n",
        "#- –ú–æ–∂–µ—Ç–µ –¥–∞ –∫–∞—á–∏—Ç–µ —Å–≤–æ–∏ –¥–∞–Ω–Ω–∏ –∏ –¥–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–∏—Ä–µ–∫—Ç–Ω–æ –≤ –æ–±–ª–∞–∫–∞.\n",
        "#- –ò–º–∞ –≤–∏–∑—É–∞–ª–µ–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ —Ö–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.\n",
        "#- üîó [–û—Ñ–∏—Ü–∏–∞–ª–µ–Ω —Å–∞–π—Ç –Ω–∞ Google Cloud AI](https://cloud.google.com/ai-platform)\n",
        "\n",
        "#‚òÅÔ∏è **2. Microsoft Azure AI**\n",
        "#- –ü–æ–¥–¥—ä—Ä–∂–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª–∏ —Å ML Studio, AutoML –∏ OpenAI API.\n",
        "#- –ò–º–∞ drag-and-drop –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞ –Ω–∞—á–∏–Ω–∞–µ—â–∏.\n",
        "#- –ú–æ–∂–µ –¥–∞ —Å–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–∞ —Å—ä—Å Streamlit, Power BI –∏ –¥—Ä—É–≥–∏.\n",
        "#- üîó [–ü—Ä–µ–≥–ª–µ–¥ –Ω–∞ Azure AI –∏ –¥—Ä—É–≥–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏](https://azure.microsoft.com/en-us/overview/ai-platform/)\n",
        "\n",
        "#üß™ **3. Amazon SageMaker**\n",
        "#- –ü–æ–∑–≤–æ–ª—è–≤–∞ –æ–±—É—á–µ–Ω–∏–µ, —Ç–µ—Å—Ç–≤–∞–Ω–µ –∏ –¥–µ–ø–ª–æ–π –Ω–∞ –º–æ–¥–µ–ª–∏ –≤ –µ–¥–Ω–∞ —Å—Ä–µ–¥–∞.\n",
        "#- –ò–º–∞ –≥–æ—Ç–æ–≤–∏ Jupyter notebook —à–∞–±–ª–æ–Ω–∏.\n",
        "#- –ü–æ–¥—Ö–æ–¥—è—â–æ –∑–∞ NLP, CV –∏ —Ç–∞–±–ª–∏—á–Ω–∏ –¥–∞–Ω–Ω–∏.\n",
        "\n",
        "#üß© **4. IBM Watson Studio**\n",
        "#- –°–∏–ª–µ–Ω —Ñ–æ–∫—É—Å –≤—ä—Ä—Ö—É –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.\n",
        "#- –ú–æ–∂–µ –¥–∞ –æ–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª–∏ —Å AutoAI –∏ –≤–∏–∑—É–∞–ª–Ω–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏.\n",
        "#- –ü–æ–¥–¥—ä—Ä–∂–∞ Python, R –∏ Scala."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rYCCiApi6zO6"
      },
      "outputs": [],
      "source": [
        "# üìò ValkyrieFX Core Engine ‚Äî –°—Ç–∞—Ä—Ç–æ–≤ –±–µ–ª–µ–∂–Ω–∏–∫\n",
        "# –ê–≤—Ç–æ—Ä: –ê–ª–µ–∫—Å–∞–Ω–¥–∞—Ä\n",
        "# –í–µ—Ä—Å–∏—è: 1.0\n",
        "# –î–∞—Ç–∞: 2025-09-04\n",
        "# –û–ø–∏—Å–∞–Ω–∏–µ:\n",
        "# –¢–æ–∑–∏ –±–µ–ª–µ–∂–Ω–∏–∫ —Å—ä–¥—ä—Ä–∂–∞ –º–æ–¥—É–ª–Ω–æ—Ç–æ —è–¥—Ä–æ –Ω–∞ ValkyrieFX ‚Äî –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∑–∞ —Ñ–æ—Ä–µ–∫—Å —Ç—ä—Ä–≥–æ–≤–∏—è —Å –ø–æ–º–æ—â—Ç–∞ –Ω–∞ RL –∞–≥–µ–Ω—Ç–∏.\n",
        "# –í–∫–ª—é—á–≤–∞: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –¥–∞–Ω–Ω–∏, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏, —Å–∏–º—É–ª–∞—Ü–∏–æ–Ω–Ω–∞ —Å—Ä–µ–¥–∞, –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞–≥–µ–Ω—Ç, Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.\n",
        "# –ü–æ–¥–≥–æ—Ç–≤–µ–Ω –∑–∞ —Ä–∞–∑—à–∏—Ä–µ–Ω–∏–µ —Å GUI, API –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.\n",
        "\n",
        "# ‚úÖ –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "!pip install pandas numpy yfinance ta gymnasium stable-baselines3 streamlit plotly --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%writefile agent_utils.py\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Function to create the RL agent\n",
        "def create_agent(agent_name, env, agent_params=None):\n",
        "    \"\"\"\n",
        "    Creates an instance of a Stable-Baselines3 RL agent.\n",
        "\n",
        "    Args:\n",
        "        agent_name (str): Name of the RL agent algorithm ('PPO', 'DQN', 'A2C').\n",
        "        env (stable_baselines3.common.vec_env.VecEnv): The vectorized training environment.\n",
        "        agent_params (dict, optional): Dictionary of agent-specific parameters. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm or None: The created agent instance, or None if agent name is unknown or an error occurs.\n",
        "    \"\"\"\n",
        "    model = None\n",
        "    # Get agent-specific parameters, defaulting to an empty dict if not provided\n",
        "    current_agent_params = agent_params if agent_params else {}\n",
        "\n",
        "    try:\n",
        "        if agent_name == \"PPO\":\n",
        "            # Define default PPO parameters and override with provided ones\n",
        "            ppo_defaults = {\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"n_steps\": 2048,\n",
        "                \"batch_size\": 64,\n",
        "                \"n_epochs\": 10,\n",
        "                \"gamma\": 0.99,\n",
        "                \"gae_lambda\": 0.95,\n",
        "                \"clip_range\": 0.2,\n",
        "            }\n",
        "            # Combine defaults and provided parameters, provided parameters take precedence\n",
        "            final_ppo_params = {**ppo_defaults, **current_agent_params}\n",
        "            model = PPO(\"MlpPolicy\", env, verbose=0, **final_ppo_params) # Create PPO model\n",
        "        elif agent_name == \"DQN\":\n",
        "            # Define default DQN parameters and override with provided ones\n",
        "            dqn_defaults = {\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"buffer_size\": 10000,\n",
        "                \"learning_starts\": 100,\n",
        "                \"batch_size\": 32,\n",
        "                \"gamma\": 0.99,\n",
        "                \"train_freq\": 1,\n",
        "                \"gradient_steps\": 1,\n",
        "            }\n",
        "            # Combine defaults and provided parameters\n",
        "            final_dqn_params = {**dqn_defaults, **current_agent_params}\n",
        "            model = DQN(\"MlpPolicy\", env, verbose=0, **final_dqn_params) # Create DQN model\n",
        "            print(f\"Using DQN with MlpPolicy for {agent_name}. Ensure observation space is compatible with DQN's MlpPolicy or consider a different policy.\")\n",
        "        elif agent_name == \"A2C\":\n",
        "            # Define default A2C parameters and override with provided ones\n",
        "            a2c_defaults = {\n",
        "                \"learning_rate\": 7e-4,\n",
        "                \"n_steps\": 5,\n",
        "                \"gamma\": 0.99,\n",
        "                \"gae_lambda\": 0.95,\n",
        "                \"vf_coef\": 0.25,\n",
        "                \"ent_coef\": 0.01,\n",
        "            }\n",
        "            # Combine defaults and provided parameters\n",
        "            final_a2c_params = {**a2c_defaults, **current_agent_params}\n",
        "            model = A2C(\"MlpPolicy\", env, verbose=0, **final_a2c_params) # Create A2C model\n",
        "\n",
        "        # Add other agents here (e.g., DDPG) if needed\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç: {agent_name}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "         print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ {agent_name}: {e}\")\n",
        "         print(e)\n",
        "         return None\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to train the RL agent\n",
        "def train_agent(agent, total_timesteps, progress_callback=None):\n",
        "    \"\"\"\n",
        "    Trains the provided RL agent.\n",
        "\n",
        "    Args:\n",
        "        agent (stable_baselines3.common.base.BaseAlgorithm): The agent instance to train.\n",
        "        total_timesteps (int): The total number of timesteps for training.\n",
        "        progress_callback (function, optional): A callback function to update progress.\n",
        "                                                Takes current_step and total_steps as arguments. Defaults to None.\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm or None: The trained agent, or None if training fails.\n",
        "    \"\"\"\n",
        "    if agent is None:\n",
        "        print(\"üö´ –ì—Ä–µ—à–∫–∞: –ê–≥–µ–Ω—Ç—ä—Ç –Ω–µ –µ –Ω–∞–ª–∏—á–µ–Ω –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nüß† –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ –∑–∞ {type(agent).__name__} –∞–≥–µ–Ω—Ç –∑–∞ {total_timesteps} —Å—Ç—ä–ø–∫–∏...\")\n",
        "        # Start the training process.\n",
        "        agent.learn(total_timesteps=total_timesteps, callback=progress_callback)\n",
        "        print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ—Ç–æ –ø—Ä–∏–∫–ª—é—á–∏.\")\n",
        "        return agent\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ –í—ä–∑–Ω–∏–∫–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ: {e}\")\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "# Function to save the trained agent\n",
        "def save_agent(agent, path):\n",
        "    \"\"\"\n",
        "    Saves the trained agent model.\n",
        "\n",
        "    Args:\n",
        "        agent (stable_baselines3.common.base.BaseAlgorithm): The agent instance to save.\n",
        "        path (str): The directory or file path to save the model.\n",
        "    \"\"\"\n",
        "    if agent is None:\n",
        "        print(\"üö´ –ù—è–º–∞ –æ–±—É—á–µ–Ω –∞–≥–µ–Ω—Ç –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ.\")\n",
        "        return\n",
        "    try:\n",
        "        agent.save(path)\n",
        "        print(f\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ –∑–∞–ø–∞–∑–µ–Ω —É—Å–ø–µ—à–Ω–æ –≤: {path}.zip\") # Stable-Baselines3 adds .zip\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ –í—ä–∑–Ω–∏–∫–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ—Ç–æ –Ω–∞ –∞–≥–µ–Ω—Ç–∞: {e}\")\n",
        "        print(e)\n",
        "\n",
        "# Function to load a saved agent\n",
        "def load_agent(path, env):\n",
        "    \"\"\"\n",
        "    Loads a trained agent model.\n",
        "\n",
        "    Args:\n",
        "        path (str): The directory or file path to load the model from (without .zip extension).\n",
        "        env (stable_baselines3.common.vec_env.VecEnv): The environment the agent was trained on or is compatible with.\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm or None: The loaded agent instance, or None if loading fails.\n",
        "    \"\"\"\n",
        "    full_path = path + \".zip\" if not path.lower().endswith('.zip') else path\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"‚ö†Ô∏è –ó–∞–ø–∞–∑–µ–Ω –º–æ–¥–µ–ª –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω –Ω–∞: {full_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Automatically detect agent type from the saved file (if available)\n",
        "        # Stable-Baselines3 save method doesn't inherently store agent type easily.\n",
        "        # Need to know the agent type beforehand or store it separately.\n",
        "        # For simplicity, assume PPO for now or pass agent_name to load.\n",
        "        # Let's modify to accept agent_name for loading the correct class.\n",
        "        print(f\"‚öôÔ∏è –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç –æ—Ç: {full_path}\")\n",
        "        # PPO.load requires the environment\n",
        "        loaded_agent = PPO.load(full_path, env=env) # Assuming PPO for now\n",
        "\n",
        "        print(\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ –∑–∞—Ä–µ–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ.\")\n",
        "        return loaded_agent\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ—Ç–æ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç {full_path}: {e}\")\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "# Note: For loading different agent types (A2C, DQN), you would need\n",
        "# to know the type before calling load and use the correct class (A2C.load, DQN.load).\n",
        "# A better load function would infer the type or require it as an argument.\n",
        "# Example: def load_agent(path, env, agent_name): ... then use if/elif to call correct load method."
      ],
      "metadata": {
        "id": "guyghXDBddsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-cPG-AcDZGh",
        "outputId": "c4c54b06-f039-4f50-c7f8-e609e51f9366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 03:55:56--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.9.1/cloudflared-linux-amd64 [following]\n",
            "--2025-10-07 03:55:56--  https://github.com/cloudflare/cloudflared/releases/download/2025.9.1/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/e30ab3bb-4e6a-464a-8db5-d5cabe6a2f8d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-07T04%3A46%3A10Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-07T03%3A45%3A42Z&ske=2025-10-07T04%3A46%3A10Z&sks=b&skv=2018-11-09&sig=EDoDOwOUY4E0LS8QcJ34c6a8FJKwzhCl0hSfYa%2B7Jdg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTgxMTE1NywibmJmIjoxNzU5ODA5MzU3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.sqgg5bd_zZSEMIQwhk900W0rXb_EzhYkHo03pZ-XzQc&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-07 03:55:57--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/e30ab3bb-4e6a-464a-8db5-d5cabe6a2f8d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-07T04%3A46%3A10Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-07T03%3A45%3A42Z&ske=2025-10-07T04%3A46%3A10Z&sks=b&skv=2018-11-09&sig=EDoDOwOUY4E0LS8QcJ34c6a8FJKwzhCl0hSfYa%2B7Jdg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTgxMTE1NywibmJmIjoxNzU5ODA5MzU3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.sqgg5bd_zZSEMIQwhk900W0rXb_EzhYkHo03pZ-XzQc&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41192406 (39M) [application/octet-stream]\n",
            "Saving to: ‚Äòcloudflared‚Äô\n",
            "\n",
            "cloudflared         100%[===================>]  39.28M  65.0MB/s    in 0.6s    \n",
            "\n",
            "2025-10-07 03:55:58 (65.0 MB/s) - ‚Äòcloudflared‚Äô saved [41192406/41192406]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L80rul1jRp_2",
        "outputId": "275d4ee6-abcd-45bb-feb6-3af1107fe72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b3e2da"
      },
      "source": [
        "# Task\n",
        "Implement a checkpointing mechanism for the agent training process to save models at regular intervals and automatically resume training from the latest checkpoint upon restart. The checkpoints should be saved in the \"/content/drive/MyDrive/\" directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2734a84c",
        "outputId": "deb94a40-3d15-4eaf-fb57-215f585dd85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: env\n"
          ]
        }
      ],
      "source": [
        "# Create the 'env' directory if it doesn't exist\n",
        "import os\n",
        "if not os.path.exists('env'):\n",
        "    os.makedirs('env')\n",
        "    print(\"Created directory: env\")\n",
        "else:\n",
        "    print(\"Directory 'env' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmpjpWsLZBXW",
        "outputId": "5515656c-2264-49b9-93ed-7c4e073f4c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing env/forex_env.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile env/forex_env.py\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pandas as pd\n",
        "import numpy as np # Added import for numpy\n",
        "import ta # Import ta for technical indicators\n",
        "from collections import deque # For tracking open positions\n",
        "import time # For timestamp in trades log\n",
        "\n",
        "class ForexTradingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gymnasium Environment for Forex Trading.\n",
        "    The environment interacts with a pandas DataFrame containing historical price data\n",
        "    and technical indicators.\n",
        "\n",
        "    Observation Space:\n",
        "    - Concatenation of:\n",
        "        - Lookback window of 'close' prices (normalized).\n",
        "        - Lookback window of selected technical indicators (normalized).\n",
        "        - Current account balance (normalized).\n",
        "        - Current open position size (normalized).\n",
        "        - Current average entry price (normalized).\n",
        "\n",
        "    Action Space: Discrete(3)\n",
        "    - 0: Hold (do nothing)\n",
        "    - 1: Buy (enter a long position or increase existing long position)\n",
        "    - 2: Sell (exit a long position or enter a short position - currently only long positions are supported)\n",
        "\n",
        "    Reward:\n",
        "    - Change in portfolio value (cash + unrealized PnL), potentially with bonuses/penalties.\n",
        "\n",
        "    Termination:\n",
        "    - When max drawdown limit is reached.\n",
        "    - When the end of the data is reached.\n",
        "    \"\"\"\n",
        "    metadata = {'render_modes': ['human']} # Define render modes if needed\n",
        "\n",
        "\n",
        "    def __init__(self, df, initial_amount=100000, lookback_window=20,\n",
        "                 buy_cost_pct=0.001, sell_cost_pct=0.001, max_drawdown_limit_pct=0.10,\n",
        "                 position_size_pct=0.1, stop_loss_pct=0.02, take_profit_pct=0.04,\n",
        "                 trailing_sl_pct=0.005, lot_model='percent_of_capital', # Added lot_model\n",
        "                 tp_reward_bonus=0.01, sl_penalty=0.01, render_mode=None): # Added reward shaping params\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Environment Parameters ---\n",
        "        self.df = df.copy() # Use a copy to avoid modifying the original DataFrame\n",
        "        self.initial_amount = float(initial_amount)\n",
        "        self.lookback_window = int(lookback_window)\n",
        "        self.buy_cost_pct = float(buy_cost_pct)\n",
        "        self.sell_cost_pct = float(sell_cost_pct)\n",
        "        self.max_drawdown_limit_pct = float(max_drawdown_limit_pct)\n",
        "        self.position_size_pct = float(position_size_pct) # For percent_of_capital model\n",
        "        self.stop_loss_pct = float(stop_loss_pct)\n",
        "        self.take_profit_pct = float(take_profit_pct)\n",
        "        self.trailing_sl_pct = float(trailing_sl_pct) # For Trailing SL\n",
        "        self.lot_model = lot_model # 'percent_of_capital' or 'volatility'\n",
        "        self.tp_reward_bonus = float(tp_reward_bonus) # Reward bonus for hitting TP\n",
        "        self.sl_penalty = float(sl_penalty) # Penalty for hitting SL\n",
        "\n",
        "\n",
        "        # --- Internal State ---\n",
        "        self.current_step = self.lookback_window # Start after the lookback window\n",
        "        self.account_balance = self.initial_amount\n",
        "        self.net_worth = self.initial_amount # Cash + unrealized PnL\n",
        "        self.max_net_worth = self.initial_amount # Track for drawdown\n",
        "        self.open_position_units = 0 # Units of base currency (e.g., EUR in EUR/USD)\n",
        "        self.average_entry_price = 0 # Average price of the current open position\n",
        "        self.trailing_stop_loss_price = 0 # For Trailing SL\n",
        "\n",
        "\n",
        "        # --- Observation Space Definition ---\n",
        "        # Determine which technical indicators are present in the DataFrame\n",
        "        # Exclude standard OHLCV columns and internal columns\n",
        "        exclude_cols = ['open', 'high', 'low', 'close', 'volume', 'date', 'original_index', 'sequential_index']\n",
        "        self.indicator_cols = [col for col in self.df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Define the observation dimension\n",
        "        # lookback window of close prices + lookback window of each indicator + account_balance + open_position_units + average_entry_price\n",
        "        # The number of features per step is 1 (close) + number of indicator columns\n",
        "        num_features_per_step = 1 + len(self.indicator_cols)\n",
        "        self.observation_dim = self.lookback_window * num_features_per_step + 3 # Added 3 for balance, units, entry price\n",
        "\n",
        "        # Define the observation space (using Box for continuous values)\n",
        "        # Use -np.inf and np.inf as bounds since values can vary widely, normalization is handled internally\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.observation_dim,), dtype=np.float32)\n",
        "\n",
        "        # --- Action Space Definition ---\n",
        "        # 0: Hold, 1: Buy, 2: Sell (Exit long position)\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # --- Backtesting/Logging ---\n",
        "        self.trades = [] # List to store trade details\n",
        "        self.portfolio_history = [] # List to store portfolio value at each step\n",
        "\n",
        "\n",
        "        # --- Volatility Model Specifics ---\n",
        "        # atr_window is used in data_utils for calculation, assuming it's already added to df.\n",
        "        # We need the ATR column if lot_model is 'volatility'\n",
        "        if self.lot_model == 'volatility' and 'atr' not in self.df.columns:\n",
        "            print(\"‚ö†Ô∏è Warning: 'atr' column not found in DataFrame but lot_model is 'volatility'. Falling back to 'percent_of_capital'.\")\n",
        "            self.lot_model = 'percent_of_capital'\n",
        "\n",
        "\n",
        "    def _get_observation(self):\n",
        "        \"\"\"\n",
        "        Generates the current observation for the agent.\n",
        "        Includes lookback window of prices and indicators, and current state variables.\n",
        "        Normalizes observations.\n",
        "        \"\"\"\n",
        "        # Ensure we have enough data for the lookback window\n",
        "        if self.current_step < self.lookback_window:\n",
        "            # Not enough data for a full observation, return zeros or handle appropriately\n",
        "            # Returning zeros might confuse the agent, but allows simulation to continue.\n",
        "            # A better approach for real applications might be to pad or return None.\n",
        "            # Given current_step starts at lookback_window, this check is mostly a safeguard.\n",
        "            print(f\"‚ö†Ô∏è Warning: Not enough data for lookback window at step {self.current_step}. Returning zero observation.\")\n",
        "            return np.zeros(self.observation_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "        # Get the relevant historical data slice\n",
        "        history_slice = self.df.iloc[self.current_step - self.lookback_window + 1 : self.current_step + 1]\n",
        "\n",
        "        # Extract close prices and indicators for the lookback window\n",
        "        close_prices = history_slice['close'].values\n",
        "        indicators_data = history_slice[self.indicator_cols].values # Shape: (lookback_window, num_indicators)\n",
        "\n",
        "        # Flatten and concatenate price and indicator data\n",
        "        # Ensure all values are finite before flattening\n",
        "        close_prices = np.nan_to_num(close_prices, nan=0.0, posinf=1e10, neginf=-1e10) # Handle potential non-finite values\n",
        "        indicators_data = np.nan_to_num(indicators_data, nan=0.0, posinf=1e10, neginf=-1e10) # Handle potential non-finite values\n",
        "\n",
        "        # Concatenate close prices and indicator data for each step in the lookback window\n",
        "        # The structure should be [price_t-LW+1, ind1_t-LW+1, ind2_t-LW+1, ..., price_t, ind1_t, ind2_t, ...]\n",
        "        # Need to stack price and indicators vertically for each timestep first\n",
        "        price_and_indicators_stacked = np.hstack((close_prices.reshape(-1, 1), indicators_data)) # Shape: (lookback_window, 1 + num_indicators)\n",
        "        price_indicator_features = price_and_indicators_stacked.flatten() # Flatten to 1D array\n",
        "\n",
        "\n",
        "        # Get current state variables\n",
        "        current_balance = self.account_balance\n",
        "        current_position_units = self.open_position_units\n",
        "        current_entry_price = self.average_entry_price\n",
        "\n",
        "        # Combine all features\n",
        "        raw_observation = np.concatenate([price_indicator_features,\n",
        "                                          [current_balance, current_position_units, current_entry_price]])\n",
        "\n",
        "\n",
        "        # --- Normalization (Simple example: Min-Max or Z-score could be better) ---\n",
        "        # A simple normalization might involve scaling by initial amount for cash/position,\n",
        "        # and by recent price ranges for prices/indicators.\n",
        "        # For simplicity here, we'll use a basic scaling or leave as is, assuming the agent's network can handle it.\n",
        "        # Proper normalization is crucial for RL performance.\n",
        "        # Let's implement a basic normalization: scale prices/indicators by the last close price,\n",
        "        # and scale balance/position by initial amount.\n",
        "\n",
        "        last_close = close_prices[-1] if len(close_prices) > 0 and close_prices[-1] != 0 else 1.0 # Avoid division by zero\n",
        "        if last_close == 0:\n",
        "             # If last close is 0, scaling by it is impossible/meaningless.\n",
        "             # Replace with a small value or handle as a special case.\n",
        "             # For now, if last_close is 0, use 1.0 for scaling to avoid errors, but this might indicate data issues.\n",
        "             last_close = 1.0\n",
        "\n",
        "\n",
        "        # Scale prices and indicators by the last close price\n",
        "        # Ensure price_indicator_features is not empty and has correct shape for reshaping\n",
        "        if price_indicator_features.size > 0:\n",
        "             # The price_indicator_features are already flattened in the desired order.\n",
        "             # We need to iterate through the flattened array and scale price/indicator values\n",
        "             # corresponding to each timestep in the lookback window by the *last* close price.\n",
        "             # This is a simplification; ideally, you might scale each value by the price at its own timestep,\n",
        "             # or use a more robust normalization like Z-score or scaling by recent volatility.\n",
        "             # For now, scaling by the last close price as originally intended in the comment:\n",
        "             normalized_price_indicator_features = price_indicator_features / last_close # Scale all features by the last close price\n",
        "\n",
        "\n",
        "        else:\n",
        "             normalized_price_indicator_features = price_indicator_features # If no features, keep empty\n",
        "\n",
        "\n",
        "        # Scale balance and position by initial amount\n",
        "        # Avoid division by zero if initial_amount is 0 (should not happen based on min_value in UI)\n",
        "        initial_amount_safe = self.initial_amount if self.initial_amount > 0 else 1.0\n",
        "        normalized_balance = current_balance / initial_amount_safe\n",
        "        normalized_position_units = current_position_units / initial_amount_safe # Scaling units by initial amount might not be intuitive, maybe by max possible units? Let's keep it simple for now.\n",
        "        normalized_entry_price = current_entry_price / last_close if last_close > 0 else 0.0 # Scale entry price by last close\n",
        "\n",
        "\n",
        "        # Combine all normalized features\n",
        "        normalized_observation = np.concatenate([normalized_price_indicator_features,\n",
        "                                                 [normalized_balance, normalized_position_units, normalized_entry_price]])\n",
        "\n",
        "\n",
        "        # Ensure the final observation has the correct shape\n",
        "        if normalized_observation.shape[0] != self.observation_dim:\n",
        "             # This is the error condition reported by the user.\n",
        "             # Let's add more detailed logging here to help diagnose the mismatch.\n",
        "             print(f\"üö´ Error: Mismatch in observation dimension at step {self.current_step}.\")\n",
        "             print(f\"  Expected dimension: {self.observation_dim}\")\n",
        "             print(f\"  Actual dimension: {normalized_observation.shape[0]}\")\n",
        "             print(f\"  Lookback Window: {self.lookback_window}\")\n",
        "             print(f\"  Number of Indicator Columns: {len(self.indicator_cols)}\")\n",
        "             num_features_per_step_check = 1 + len(self.indicator_cols) # Re-calculate for logging\n",
        "             print(f\"  Number of features per step (1 + num_indicators): {num_features_per_step_check}\")\n",
        "             print(f\"  Calculated expected dimension (LW * features_per_step + 3): {self.lookback_window} * {num_features_per_step_check} + 3 = {self.lookback_window * num_features_per_step_check + 3}\")\n",
        "             print(f\"  Shape of price_indicator_features: {price_indicator_features.shape}\")\n",
        "             print(f\"  Shape of state variables ([balance, units, entry_price]): {(3,)}\")\n",
        "             print(f\"  Shape of concatenated raw_observation: {raw_observation.shape}\")\n",
        "             print(f\"  Shape of normalized_observation: {normalized_observation.shape}\")\n",
        "\n",
        "\n",
        "             # Returning zero observation on error might hide the problem.\n",
        "             # Let's raise an error or return None to make the issue clearer during debugging/training.\n",
        "             # For now, returning zero observation as before, but with improved logging.\n",
        "             return np.zeros(self.observation_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "        # Ensure all values in the final observation are finite\n",
        "        if not np.isfinite(normalized_observation).all():\n",
        "             print(f\"‚ö†Ô∏è Warning: Non-finite values detected in observation at step {self.current_step}. Replacing with zeros.\")\n",
        "             normalized_observation = np.nan_to_num(normalized_observation, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "\n",
        "\n",
        "        return normalized_observation.astype(np.float32) # Ensure dtype is float32\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Resets the environment to an initial state.\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed) # For Gymnasium compatibility\n",
        "\n",
        "        # Reset internal state\n",
        "        self.current_step = self.lookback_window # Start simulation after the lookback window\n",
        "        self.account_balance = self.initial_amount\n",
        "        self.net_worth = self.initial_amount\n",
        "        self.max_net_worth = self.initial_amount\n",
        "        self.open_position_units = 0\n",
        "        self.average_entry_price = 0\n",
        "        self.trailing_stop_loss_price = 0\n",
        "\n",
        "        # Clear logs\n",
        "        self.trades = []\n",
        "        self.portfolio_history = []\n",
        "        # Log initial state (should be before the first step)\n",
        "        self.portfolio_history.append({'date': self._get_current_date(),\n",
        "                                        'portfolio_value': self.net_worth,\n",
        "                                        'account_balance': self.account_balance, # Added for info\n",
        "                                        'open_position_units': self.open_position_units, # Added for info\n",
        "                                        'average_entry_price': self.average_entry_price, # Added for info\n",
        "                                        'current_price': self._get_current_price(), # Added for info\n",
        "                                        'step': self.current_step, # Step at reset (lookback_window)\n",
        "                                        'action': 'Reset', # Indicate reset\n",
        "                                        'reward': 0, # Reward at reset is 0\n",
        "                                        'trade_executed': False,\n",
        "                                        'trade_type': None,\n",
        "                                        'trade_pnl': 0\n",
        "                                       })\n",
        "\n",
        "\n",
        "        # Get the initial observation\n",
        "        observation = self._get_observation()\n",
        "\n",
        "        # Return observation and info (Gymnasium standard)\n",
        "        # The info dictionary returned by reset corresponds to the state *before* the first step()\n",
        "        # Let's return a simplified info here and ensure the first entry in backtesting_results in dashboard\n",
        "        # captures the full initial state.\n",
        "        info = {'account_balance': self.account_balance,\n",
        "                'net_worth': self.net_worth,\n",
        "                'open_position_units': self.open_position_units,\n",
        "                'average_entry_price': self.average_entry_price,\n",
        "                'current_price': self._get_current_price(),\n",
        "                'date': self._get_current_date(),\n",
        "                'step': self.current_step} # Step at reset\n",
        "\n",
        "\n",
        "        # Ensure observation is valid before returning\n",
        "        if observation is None or not isinstance(observation, np.ndarray) or observation.shape[0] != self.observation_dim or not np.isfinite(observation).all():\n",
        "             print(f\"üö´ Error during reset: Invalid initial observation. Returning zero observation.\")\n",
        "             observation = np.zeros(self.observation_dim, dtype=np.float32) # Return zero observation on error\n",
        "             # Ensure info is also consistent if observation is invalid\n",
        "             info = {'account_balance': self.initial_amount,\n",
        "                     'net_worth': self.initial_amount,\n",
        "                     'open_position_units': 0,\n",
        "                     'average_entry_price': 0,\n",
        "                     'current_price': np.nan,\n",
        "                     'date': self._get_current_date(),\n",
        "                     'step': self.current_step,\n",
        "                     'message': 'Invalid initial observation'}\n",
        "\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Performs one step in the environment based on the chosen action.\n",
        "\n",
        "        Args:\n",
        "            action (int): The action chosen by the agent (0: Hold, 1: Buy, 2: Sell).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (observation, reward, done, truncated, info)\n",
        "            - observation (np.ndarray): The new observation.\n",
        "            - reward (float): The reward received in this step.\n",
        "            - done (bool): Whether the episode has ended (e.g., max drawdown reached, end of data).\n",
        "            - truncated (bool): Whether the episode was truncated (e.g., time limit reached - not used here).\n",
        "            - info (dict): Additional information.\n",
        "        \"\"\"\n",
        "        # Ensure action is a valid integer\n",
        "        if not isinstance(action, (int, np.integer)) or action not in [0, 1, 2]:\n",
        "            print(f\"‚ö†Ô∏è Warning: Invalid action received: {action}. Treating as Hold (0).\")\n",
        "            action = 0 # Default to Hold for invalid actions\n",
        "\n",
        "        # Store previous net worth for reward calculation\n",
        "        previous_net_worth = self.net_worth\n",
        "        previous_account_balance = self.account_balance # Also track balance change if needed for reward\n",
        "\n",
        "\n",
        "        # Get current market data (data point BEFORE the action is applied)\n",
        "        # The action taken at step 't' uses observation from step 't', and affects the state at step 't+1'\n",
        "        # The environment's current_step is the index of the data point being *processed*.\n",
        "        # The observation generated by _get_observation() at step 't' uses data up to index 't'.\n",
        "        # The step() function updates the state based on data at index 'self.current_step'.\n",
        "        # So, current_price, current_date, current_atr should be from self.df.iloc[self.current_step]\n",
        "\n",
        "        current_price_at_step = self._get_current_price() # Price at the data point BEFORE incrementing current_step\n",
        "        current_date_at_step = self._get_current_date() # Date at the data point BEFORE incrementing current_step\n",
        "        current_atr_at_step = self._get_current_atr() # ATR at the data point BEFORE incrementing current_step\n",
        "\n",
        "\n",
        "        # Ensure current_price is valid\n",
        "        if pd.isna(current_price_at_step) or current_price_at_step <= 0:\n",
        "             print(f\"‚ö†Ô∏è Warning: Invalid price ({current_price_at_step}) at step {self.current_step}. Cannot process action {action}. Ending episode.\")\n",
        "             # If price is invalid, we cannot trade. End the episode.\n",
        "             # Increment current_step first before returning\n",
        "             self.current_step += 1\n",
        "             # Get observation for the *next* state (which might be invalid)\n",
        "             observation = self._get_observation()\n",
        "             reward = -self.initial_amount * 0.05 # Large penalty for invalid state\n",
        "             done = True\n",
        "             truncated = False\n",
        "             info = {'account_balance': self.account_balance,\n",
        "                     'net_worth': self.net_worth, # Log net_worth *before* invalid step? Or after? Let's log current state.\n",
        "                     'open_position_units': self.open_position_units,\n",
        "                     'average_entry_price': self.average_entry_price,\n",
        "                     'current_price': current_price_at_step, # Log the invalid price\n",
        "                     'date': current_date_at_step, # Log the date\n",
        "                     'step': self.current_step - 1, # Log the step number where invalidity was detected\n",
        "                     'action': self._action_to_text(action),\n",
        "                     'reward': reward,\n",
        "                     'message': f\"Episode ended due to invalid price ({current_price_at_step})\",\n",
        "                     'portfolio_value': self.net_worth # Ensure portfolio_value is present\n",
        "                    }\n",
        "             return observation, reward, done, truncated, info\n",
        "\n",
        "\n",
        "        # --- Process Action ---\n",
        "        trade_executed = False # Flag to check if a trade happened in this step\n",
        "        trade_type = None # 'buy', 'sell', 'sl', 'tp', 'trailing_sl'\n",
        "        trade_price = None\n",
        "        trade_units = 0\n",
        "        trade_cost = 0\n",
        "        trade_pnl = 0 # Profit/Loss for closed positions\n",
        "\n",
        "        # Check for Stop Loss, Take Profit, Trailing Stop Loss first (applies if position is open)\n",
        "        # These are evaluated based on the current_price_at_step\n",
        "        if self.open_position_units > 0:\n",
        "            # Calculate unrealized PnL *at this step's price*\n",
        "            unrealized_pnl = (current_price_at_step - self.average_entry_price) * self.open_position_units\n",
        "\n",
        "            # Check Take Profit\n",
        "            if self.take_profit_pct > 0 and current_price_at_step >= self.average_entry_price * (1 + self.take_profit_pct):\n",
        "                trade_type = 'tp'\n",
        "                trade_price = self.average_entry_price * (1 + self.take_profit_pct) # Execute at TP price\n",
        "                trade_units = self.open_position_units\n",
        "                trade_cost = trade_units * trade_price * self.sell_cost_pct\n",
        "                trade_pnl = unrealized_pnl - trade_cost # PnL includes cost\n",
        "                self.account_balance += (trade_units * trade_price) - trade_cost # Add cash from selling\n",
        "                self.open_position_units = 0\n",
        "                self.average_entry_price = 0\n",
        "                self.trailing_stop_loss_price = 0 # Reset Trailing SL\n",
        "                trade_executed = True\n",
        "                # print(f\"TP Hit at step {self.current_step} ({current_date_at_step}): Price={trade_price:.5f}, PnL={trade_pnl:.2f}\") # Debug print\n",
        "\n",
        "\n",
        "            # Check Trailing Stop Loss (only if TP not hit)\n",
        "            elif self.trailing_sl_pct > 0 and self.trailing_stop_loss_price > 0 and current_price_at_step <= self.trailing_stop_loss_price:\n",
        "                 trade_type = 'trailing_sl'\n",
        "                 trade_price = self.trailing_stop_loss_price # Execute at Trailing SL price\n",
        "                 # Ensure execution price is not higher than current price if market gaps down\n",
        "                 trade_price = min(trade_price, current_price_at_step) # Ensure realistic execution price on gap down\n",
        "\n",
        "                 trade_units = self.open_position_units\n",
        "                 trade_cost = trade_units * trade_price * self.sell_cost_pct\n",
        "                 unrealized_pnl_at_sl = (trade_price - self.average_entry_price) * trade_units # PnL based on SL price\n",
        "                 trade_pnl = unrealized_pnl_at_sl - trade_cost # PnL includes cost\n",
        "                 self.account_balance += (trade_units * trade_price) - trade_cost # Add cash from selling\n",
        "                 self.open_position_units = 0\n",
        "                 self.average_entry_price = 0\n",
        "                 self.trailing_stop_loss_price = 0 # Reset Trailing SL\n",
        "                 trade_executed = True\n",
        "                 # print(f\"Trailing SL Hit at step {self.current_step} ({current_date_at_step}): Price={trade_price:.5f}, PnL={trade_pnl:.2f}\") # Debug print\n",
        "\n",
        "            # Check Stop Loss (only if TP and Trailing SL not hit)\n",
        "            elif self.stop_loss_pct > 0 and current_price_at_step <= self.average_entry_price * (1 - self.stop_loss_pct):\n",
        "                trade_type = 'sl'\n",
        "                trade_price = self.average_entry_price * (1 - self.stop_loss_pct) # Execute at SL price\n",
        "                 # Ensure execution price is not higher than current price if market gaps down\n",
        "                trade_price = min(trade_price, current_price_at_step) # Ensure realistic execution price on gap down\n",
        "\n",
        "                trade_units = self.open_position_units\n",
        "                trade_cost = trade_units * trade_price * self.sell_cost_pct\n",
        "                unrealized_pnl_at_sl = (trade_price - self.average_entry_price) * trade_units # PnL based on SL price\n",
        "                trade_pnl = unrealized_pnl_at_sl - trade_cost # PnL includes cost\n",
        "                self.account_balance += (trade_units * trade_price) - trade_cost # Add cash from selling\n",
        "                self.open_position_units = 0\n",
        "                self.average_entry_price = 0\n",
        "                self.trailing_stop_loss_price = 0 # Reset Trailing SL\n",
        "                trade_executed = True\n",
        "                # print(f\"SL Hit at step {self.current_step} ({current_date_at_step}): Price={trade_price:.5f}, PnL={trade_pnl:.2f}\") # Debug print\n",
        "\n",
        "\n",
        "            # If position is open and no automatic exit, update Trailing SL\n",
        "            if self.open_position_units > 0 and not trade_executed and self.trailing_sl_pct > 0:\n",
        "                 # Update Trailing SL: moves up if price makes a new high\n",
        "                 current_trailing_sl = current_price_at_step * (1 - self.trailing_sl_pct)\n",
        "                 if current_trailing_sl > self.trailing_stop_loss_price:\n",
        "                      self.trailing_stop_loss_price = current_trailing_sl\n",
        "\n",
        "\n",
        "        # If no automatic exit occurred, process the agent's action\n",
        "        if not trade_executed:\n",
        "            if action == 1: # Buy\n",
        "                # Determine position size based on lot_model\n",
        "                if self.lot_model == 'percent_of_capital':\n",
        "                     # Buy using a percentage of current account balance\n",
        "                     amount_to_spend = self.account_balance * self.position_size_pct\n",
        "                     # Calculate how many units can be bought with the available amount\n",
        "                     # Ensure current_price is not zero to avoid division by zero\n",
        "                     if current_price_at_step > 0:\n",
        "                          units_to_buy = amount_to_spend / current_price_at_step\n",
        "                     else:\n",
        "                          units_to_buy = 0\n",
        "                          print(f\"‚ö†Ô∏è Warning: Cannot calculate units to buy due to zero price at step {self.current_step}.\")\n",
        "\n",
        "\n",
        "                elif self.lot_model == 'volatility':\n",
        "                     # Buy based on Volatility (e.g., Risk per trade / ATR)\n",
        "                     # Assuming 'atr' column is available and valid\n",
        "                     # Let's use a simplified ATR-based sizing: Units = (Account Balance * Risk %) / (ATR * Multiplier * Price per unit)\n",
        "                     # Assuming Position Size Pct is interpreted as Risk % of Account Balance\n",
        "                     atr_multiplier = 2 # Example multiplier for stop distance\n",
        "                     if current_atr_at_step is not None and current_atr_at_step > 0 and self.position_size_pct > 0:\n",
        "                           # Stop Loss Distance in price based on ATR\n",
        "                           stop_distance_price = current_atr_at_step * atr_multiplier\n",
        "                           if stop_distance_price > 0 and current_price_at_step > 0: # Also ensure price is valid\n",
        "                                # Units = Risk Amount / Stop Loss Distance in Price\n",
        "                                risk_amount = self.account_balance * self.position_size_pct\n",
        "                                units_to_buy = risk_amount / stop_distance_price\n",
        "                           else:\n",
        "                                units_to_buy = 0 # Avoid division by zero or invalid calculation\n",
        "                                print(f\"‚ö†Ô∏è Warning: Cannot calculate units to buy (Volatility Model) due to invalid stop distance ({stop_distance_price}) or price ({current_price_at_step}) at step {self.current_step}.\")\n",
        "                     else:\n",
        "                          units_to_buy = 0\n",
        "                          print(f\"‚ö†Ô∏è Warning: Cannot calculate units to buy (Volatility Model) due to invalid ATR ({current_atr_at_step}) or position_size_pct ({self.position_size_pct}) at step {self.current_step}.\")\n",
        "\n",
        "\n",
        "                else: # Fallback to percent_of_capital if lot_model is unknown or invalid\n",
        "                    amount_to_spend = self.account_balance * self.position_size_pct\n",
        "                    if current_price_at_step > 0:\n",
        "                         units_to_buy = amount_to_spend / current_price_at_step\n",
        "                    else:\n",
        "                         units_to_buy = 0\n",
        "                         print(f\"‚ö†Ô∏è Warning: Cannot calculate units to buy (Default Model) due to zero price at step {self.current_step}.\")\n",
        "\n",
        "\n",
        "                # Ensure units_to_buy is non-negative and finite\n",
        "                units_to_buy = max(0, np.nan_to_num(units_to_buy, nan=0.0, posinf=0.0, neginf=0.0))\n",
        "\n",
        "\n",
        "                # Execute Buy if units_to_buy > 0 and enough balance\n",
        "                cost = units_to_buy * current_price_at_step * (1 + self.buy_cost_pct) # Total cost includes fee\n",
        "                if units_to_buy > 0 and self.account_balance >= cost:\n",
        "                    # Calculate new average entry price\n",
        "                    total_cost_of_existing_position = self.open_position_units * self.average_entry_price\n",
        "                    new_total_units = self.open_position_units + units_to_buy\n",
        "                    if new_total_units > 0:\n",
        "                         # Weighted average entry price\n",
        "                         self.average_entry_price = (total_cost_of_existing_position + (units_to_buy * current_price_at_step)) / new_total_units\n",
        "                    else:\n",
        "                         self.average_entry_price = 0 # Should not happen if units_to_buy > 0\n",
        "\n",
        "                    self.account_balance -= cost\n",
        "                    self.open_position_units += units_to_buy\n",
        "                    trade_executed = True\n",
        "                    trade_type = 'buy'\n",
        "                    trade_price = current_price_at_step\n",
        "                    trade_units = units_to_buy\n",
        "                    trade_cost = cost # Store total cost including fee\n",
        "                    trade_pnl = 0 # PnL is 0 at entry\n",
        "\n",
        "                    # Set initial Trailing SL if enabled and this is a new position (or adding to existing)\n",
        "                    if self.trailing_sl_pct > 0:\n",
        "                         # When buying more, the trailing stop should be based on the *new* entry price\n",
        "                         # or potentially adjusted based on the new total position.\n",
        "                         # A simple approach: recalculate from the new average entry price and current price.\n",
        "                         # Or, if adding to a winning position, maintain or raise the TSL.\n",
        "                         # Let's use a simple method: update TSL based on current price and trailing_sl_pct IF it results in a higher TSL.\n",
        "                         current_tsl_candidate = current_price_at_step * (1 - self.trailing_sl_pct)\n",
        "                         if current_tsl_candidate > self.trailing_stop_loss_price:\n",
        "                              self.trailing_stop_loss_price = current_tsl_candidate\n",
        "                         # If it's a brand new position, set the TSL\n",
        "                         if self.open_position_units == units_to_buy:\n",
        "                              self.trailing_stop_loss_price = current_price_at_step * (1 - self.trailing_sl_pct)\n",
        "\n",
        "\n",
        "                    # print(f\"Buy executed at step {self.current_step} ({current_date_at_step}): Price={trade_price:.5f}, Units={trade_units:.2f}, Cost={cost:.2f}\") # Debug print\n",
        "\n",
        "                # else:\n",
        "                    # print(f\"Buy signal at step {self.current_step} ({current_date_at_step}), but no trade executed (units_to_buy={units_to_buy:.2f}, cost={cost:.2f}, balance={self.account_balance:.2f}).\") # Debug print\n",
        "\n",
        "\n",
        "            elif action == 2: # Sell (Exit long position)\n",
        "                if self.open_position_units > 0:\n",
        "                    # Sell the entire open position\n",
        "                    amount_from_selling = self.open_position_units * current_price_at_step\n",
        "                    cost = amount_from_selling * self.sell_cost_pct\n",
        "                    amount_received = amount_from_selling - cost\n",
        "                    unrealized_pnl = (current_price_at_step - self.average_entry_price) * self.open_position_units\n",
        "                    trade_pnl = unrealized_pnl - cost # PnL includes cost\n",
        "\n",
        "                    self.account_balance += amount_received\n",
        "                    trade_units = self.open_position_units # Units being sold\n",
        "                    self.open_position_units = 0\n",
        "                    self.average_entry_price = 0\n",
        "                    self.trailing_stop_loss_price = 0 # Reset Trailing SL\n",
        "                    trade_executed = True\n",
        "                    trade_type = 'sell' # Agent initiated sell\n",
        "                    trade_price = current_price_at_step\n",
        "                    trade_cost = cost # Store selling cost\n",
        "                    # trade_pnl already calculated\n",
        "\n",
        "                    # print(f\"Sell executed at step {self.current_step} ({current_date_at_step}): Price={trade_price:.5f}, Units={trade_units:.2f}, PnL={trade_pnl:.2f}\") # Debug print\n",
        "\n",
        "                # else:\n",
        "                    # print(f\"Sell signal at step {self.current_step} ({current_date_at_step}), but no position to sell.\") # Debug print\n",
        "\n",
        "\n",
        "            # Action 0: Hold (do nothing, SL/TP/TSL check already handled)\n",
        "            elif action == 0:\n",
        "                 # print(f\"Hold at step {self.current_step} ({current_date_at_step}).\") # Debug print\n",
        "                 pass # No action taken, SL/TP/TSL already checked before this block\n",
        "\n",
        "\n",
        "        # --- Update Net Worth ---\n",
        "        # Net worth is cash + value of open position (unrealized PnL) based on the price *at this step*\n",
        "        unrealized_pnl_current = (current_price_at_step - self.average_entry_price) * self.open_position_units\n",
        "        self.net_worth = self.account_balance + unrealized_pnl_current\n",
        "\n",
        "        # Update max net worth for drawdown calculation\n",
        "        self.max_net_worth = max(self.max_net_worth, self.net_worth)\n",
        "\n",
        "        # --- Log Trade (if executed) ---\n",
        "        # Log the trade using the details gathered during action processing for this step\n",
        "        if trade_executed:\n",
        "            self.trades.append({\n",
        "                'date': current_date_at_step, # Date of the step when trade occurred\n",
        "                'step': self.current_step, # Step number when trade occurred\n",
        "                'type': 'entry' if trade_type == 'buy' else 'exit',\n",
        "                'action': trade_type, # 'buy', 'sell', 'sl', 'tp', 'trailing_sl'\n",
        "                'price': trade_price, # Price at which trade was executed\n",
        "                'units': trade_units, # Units bought/sold\n",
        "                'cost': trade_cost, # Transaction cost\n",
        "                'pnl': trade_pnl if trade_type != 'buy' else 0, # PnL is for exit trades\n",
        "                'account_balance': self.account_balance, # Balance AFTER trade\n",
        "                'net_worth': self.net_worth # Net worth AFTER trade\n",
        "            })\n",
        "\n",
        "\n",
        "        # --- Calculate Reward ---\n",
        "        # Reward is the change in net worth from the *previous* step to the *current* step\n",
        "        # The net_worth calculated just above is the net worth *at the end of* the current step.\n",
        "        # Previous net worth was stored at the beginning of step().\n",
        "        reward = (self.net_worth - previous_net_worth) / self.initial_amount # Scale by initial capital\n",
        "\n",
        "        # Add reward shaping bonuses/penalties based on the trade type that *executed* in this step\n",
        "        if trade_executed:\n",
        "             if trade_type == 'tp':\n",
        "                  reward += self.tp_reward_bonus # Add bonus for hitting TP\n",
        "                  # print(f\"TP Bonus added: +{self.tp_reward_bonus * self.initial_amount:.2f}\") # Debug print\n",
        "             elif trade_type == 'sl' or trade_type == 'trailing_sl':\n",
        "                  reward -= self.sl_penalty # Apply penalty for hitting SL/TSL\n",
        "                  # print(f\"SL/TSL Penalty applied: -{self.sl_penalty * self.initial_amount:.2f}\") # Debug print\n",
        "\n",
        "\n",
        "        # --- Check for Termination ---\n",
        "        done = False\n",
        "        truncated = False # Not using truncation for time limits in this env\n",
        "\n",
        "        # Check max drawdown\n",
        "        if self.max_net_worth > 0: # Avoid division by zero\n",
        "             current_drawdown = (self.max_net_worth - self.net_worth) / self.max_net_worth\n",
        "             if current_drawdown >= self.max_drawdown_limit_pct:\n",
        "                 print(f\"üö´ Max Drawdown Limit ({self.max_drawdown_limit_pct:.2%}) reached at step {self.current_step} ({current_date_at_step}). Net Worth: {self.net_worth:.2f}, Max Net Worth: {self.max_net_worth:.2f}\")\n",
        "                 # Optional: Add a penalty for hitting max drawdown - already included in reward calculation if net_worth drops significantly\n",
        "                 # reward -= self.initial_amount * 0.10 # Decide if this penalty is separate or part of net_worth change\n",
        "                 done = True\n",
        "\n",
        "\n",
        "        # Increment current_step *after* processing the current step's data and calculating rewards/termination\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check if end of data is reached for the *next* step\n",
        "        if self.current_step >= len(self.df):\n",
        "            done = True\n",
        "            # print(f\"‚úÖ End of data reached. Next step ({self.current_step}) is beyond data length ({len(self.df)}).\")\n",
        "\n",
        "\n",
        "        # --- Log Portfolio History ---\n",
        "        # Log portfolio value *after* processing the step and updating net_worth,\n",
        "        # but conceptually corresponds to the state *at the end of* the step just processed.\n",
        "        # Use the date of the *processed* step.\n",
        "        self.portfolio_history.append({'date': current_date_at_step, # Use date of the step just processed\n",
        "                                        'portfolio_value': self.net_worth,\n",
        "                                        'account_balance': self.account_balance, # Added for info\n",
        "                                        'open_position_units': self.open_position_units, # Added for info\n",
        "                                        'average_entry_price': self.average_entry_price, # Added for info\n",
        "                                        'current_price': current_price_at_step, # Added for info\n",
        "                                        'step': self.current_step -1 # Log the step number just processed\n",
        "                                       })\n",
        "\n",
        "\n",
        "        # --- Prepare next observation and info ---\n",
        "        # Get observation for the *next* step (self.current_step is now the index for the next observation)\n",
        "        observation = self._get_observation() # _get_observation uses self.current_step\n",
        "\n",
        "\n",
        "        # Ensure observation is valid before returning\n",
        "        if observation is None or not isinstance(observation, np.ndarray) or observation.shape[0] != self.observation_dim or not np.isfinite(observation).all():\n",
        "             print(f\"üö´ Error getting observation for next step ({self.current_step}): Invalid observation. Setting done=True.\")\n",
        "             observation = np.zeros(self.observation_dim, dtype=np.float32) # Return zero observation on error\n",
        "             done = True # End episode if next observation is invalid\n",
        "\n",
        "\n",
        "        # The info dictionary returned by step should reflect the state *after* the action and updates for the current step.\n",
        "        # It should also contain information needed by the dashboard for logging/metrics.\n",
        "        # Let's build the info dictionary here based on the state *after* the current step is processed.\n",
        "        info = {'account_balance': self.account_balance,\n",
        "                'net_worth': self.net_worth,\n",
        "                'open_position_units': self.open_position_units,\n",
        "                'average_entry_price': self.average_entry_price,\n",
        "                'current_price': current_price_at_step, # Price of the step just processed\n",
        "                'date': current_date_at_step, # Date of the step just processed\n",
        "                'step': self.current_step - 1, # Step number just processed\n",
        "                'action': self._action_to_text(action), # Log the action taken in this step\n",
        "                'reward': reward, # Log the reward for this step\n",
        "                'trade_executed': trade_executed, # Indicate if a trade happened\n",
        "                'trade_type': trade_type, # Type of trade if executed\n",
        "                'trade_pnl': trade_pnl, # PnL if position was closed\n",
        "                'portfolio_value': self.net_worth # Explicitly include portfolio_value for the dashboard\n",
        "               }\n",
        "\n",
        "\n",
        "        return observation, reward, done, truncated, info\n",
        "\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"\n",
        "        Renders the environment state (optional).\n",
        "        \"\"\"\n",
        "        # In a dashboard context, rendering is handled by the UI (e.g., Plotly chart)\n",
        "        # This method might not be needed or could print state info.\n",
        "        if mode == 'human':\n",
        "            pass\n",
        "            # print(f\"Step: {self.current_step}, Date: {self._get_current_date()}, Price: {self._get_current_price():.5f}, Net Worth: {self.net_worth:.2f}, Position: {self.open_position_units:.2f}\")\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Cleans up resources (if any).\n",
        "        \"\"\"\n",
        "        pass # No external resources to close\n",
        "\n",
        "\n",
        "    # --- Helper methods ---\n",
        "    def _get_current_price(self):\n",
        "        \"\"\"Gets the 'close' price for the current step.\"\"\"\n",
        "        # Ensure current_step is within bounds and 'close' column exists\n",
        "        if self.current_step < len(self.df) and 'close' in self.df.columns:\n",
        "             return float(self.df.iloc[self.current_step]['close']) # Ensure float type\n",
        "        return np.nan # Return NaN if out of bounds or column missing\n",
        "\n",
        "    def _get_current_date(self):\n",
        "        \"\"\"Gets the 'date' for the current step.\"\"\"\n",
        "        # Ensure current_step is within bounds and 'date' column exists or index is datetime\n",
        "        if self.current_step < len(self.df):\n",
        "             if 'date' in self.df.columns:\n",
        "                  return self.df.iloc[self.current_step]['date']\n",
        "             elif isinstance(self.df.index, pd.DatetimeIndex):\n",
        "                  return self.df.index[self.current_step]\n",
        "        return None # Return None if out of bounds or date not found\n",
        "\n",
        "    def _get_current_atr(self):\n",
        "        \"\"\"Gets the 'atr' value for the current step, if available.\"\"\"\n",
        "        if self.current_step < len(self.df) and 'atr' in self.df.columns:\n",
        "             # Ensure ATR is a valid number before returning\n",
        "             atr_value = self.df.iloc[self.current_step]['atr']\n",
        "             if pd.notna(atr_value) and atr_value is not None and np.isfinite(atr_value):\n",
        "                 return float(atr_value) # Ensure float type\n",
        "             return None # Return None if ATR is NaN, None, or Inf\n",
        "        return None # Return None if out of bounds or 'atr' column missing\n",
        "\n",
        "\n",
        "    def _action_to_text(self, action):\n",
        "        \"\"\"Converts action integer to descriptive text.\"\"\"\n",
        "        if action == 0: return 'Hold'\n",
        "        if action == 1: return 'Buy'\n",
        "        if action == 2: return 'Sell'\n",
        "        return 'Unknown'\n",
        "\n",
        "\n",
        "# Function to calculate performance metrics\n",
        "def calculate_metrics(portfolio_values, trades_log, initial_amount):\n",
        "    \"\"\"\n",
        "    Calculates various trading performance metrics.\n",
        "\n",
        "    Args:\n",
        "        portfolio_values (pd.Series): Series of portfolio total values over time (indexed by date).\n",
        "        trades_log (list of dict): List of executed trades.\n",
        "        initial_amount (float): The initial capital.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of calculated metrics.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Check if portfolio_values is a pandas Series and has enough data points\n",
        "    # Corrected check to only accept pandas Series with at least 2 data points\n",
        "    if not isinstance(portfolio_values, pd.Series) or len(portfolio_values) < 2:\n",
        "        print(\"‚ö†Ô∏è Warning: Invalid or insufficient portfolio_values data for metrics calculation. Expected pandas Series with >= 2 points.\")\n",
        "        # Return default metrics\n",
        "        metrics['Total Return (%)'] = 0\n",
        "        metrics['CAGR (%)'] = 0\n",
        "        metrics['Sharpe Ratio'] = 0\n",
        "        metrics['Sortino Ratio'] = 0\n",
        "        metrics['Max Drawdown (%)'] = 0\n",
        "        metrics['Total Trades'] = len(trades_log) if trades_log else 0 # Ensure trades_log is not None\n",
        "        metrics['Winning Trades'] = 0\n",
        "        metrics['Losing Trades'] = 0\n",
        "        metrics['Win Rate (%)'] = 0\n",
        "        metrics['Average PnL per Trade'] = 0\n",
        "        metrics['Average Winning Trade'] = 0\n",
        "        metrics['Average Losing Trade'] = 0\n",
        "        metrics['Profit Factor'] = 0\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    # Ensure initial amount is positive to avoid division by zero or log(0) issues\n",
        "    initial_amount_safe = initial_amount if initial_amount > 0 else 1.0\n",
        "\n",
        "    # Ensure portfolio values are all finite numbers\n",
        "    if not np.isfinite(portfolio_values).all():\n",
        "         print(\"‚ö†Ô∏è Warning: Non-finite values found in portfolio_values. Metrics might be inaccurate.\")\n",
        "         # Optionally replace non-finite values or handle as error\n",
        "         portfolio_series = np.nan_to_num(portfolio_values, nan=initial_amount_safe, posinf=portfolio_values.max(), neginf=portfolio_values.min()) # Simple handling\n",
        "         portfolio_series = pd.Series(portfolio_series, index=portfolio_values.index) # Recreate series with original index\n",
        "    else:\n",
        "         portfolio_series = portfolio_values.copy() # Work on a copy\n",
        "\n",
        "\n",
        "    total_return = (portfolio_series.iloc[-1] - initial_amount_safe) / initial_amount_safe * 100\n",
        "    metrics['Total Return (%)'] = round(total_return, 2)\n",
        "\n",
        "    # CAGR (Compound Annual Growth Rate)\n",
        "    # Requires DatetimeIndex\n",
        "    if isinstance(portfolio_series.index, pd.DatetimeIndex) and len(portfolio_series.index) > 1:\n",
        "        # Check if there's a meaningful time duration\n",
        "        duration_years = (portfolio_series.index[-1] - portfolio_series.index[0]).days / 365.25\n",
        "        # Ensure initial amount is positive for ratio calculation\n",
        "        if duration_years > 0 and initial_amount_safe > 0:\n",
        "             # Avoid log(0) if initial_amount is 0 or becomes 0\n",
        "             if portfolio_series.iloc[-1] > 0:\n",
        "                  cagr = ((portfolio_series.iloc[-1] / initial_amount_safe) ** (1 / duration_years) - 1) * 100\n",
        "                  metrics['CAGR (%)'] = round(cagr, 2)\n",
        "             else:\n",
        "                  metrics['CAGR (%)'] = -100 # Represents total loss\n",
        "        else:\n",
        "             metrics['CAGR (%)'] = 0 # Cannot calculate if duration is zero or initial amount is zero (or data is only one point)\n",
        "    else:\n",
        "         # Fallback or warning if date/frequency info is not available or only one data point\n",
        "         metrics['CAGR (%)'] = 0 # Cannot calculate accurately without sufficient dates\n",
        "\n",
        "\n",
        "    # Daily returns for volatility/risk metrics\n",
        "    # Ensure portfolio_series has DatetimeIndex for resampling if needed for daily returns\n",
        "    # Or calculate simple period returns\n",
        "    if isinstance(portfolio_series.index, pd.DatetimeIndex):\n",
        "        # Attempt daily resampling if data is higher frequency than daily\n",
        "        try:\n",
        "             # Resample to daily, using the last value of the day\n",
        "             daily_portfolio_values = portfolio_series.resample('D').ffill().dropna()\n",
        "             if len(daily_portfolio_values) > 1:\n",
        "                  returns = daily_portfolio_values.pct_change().dropna()\n",
        "             else:\n",
        "                  # If resampling results in too few points, use period returns\n",
        "                  returns = portfolio_series.pct_change().dropna() # Use original frequency returns\n",
        "                  print(\"‚ö†Ô∏è Warning: Daily resampling failed or resulted in insufficient data. Using original frequency returns for Sharpe/Sortino.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"‚ö†Ô∏è Warning: Error during daily resampling: {e}. Using original frequency returns for Sharpe/Sortino.\")\n",
        "             returns = portfolio_series.pct_change().dropna() # Fallback to original frequency\n",
        "\n",
        "    else:\n",
        "         print(\"‚ö†Ô∏è Warning: Portfolio values do not have DatetimeIndex. Using raw period returns for Sharpe/Sortino.\")\n",
        "         returns = portfolio_series.pct_change().dropna() # Use original frequency returns\n",
        "\n",
        "\n",
        "    # Annualization factors (assuming daily for Sharpe/Sortino if daily returns were calculated)\n",
        "    # If using original frequency returns, need to know the frequency\n",
        "    annualization_factor = 1 # Default if frequency is unknown or single period\n",
        "    if isinstance(returns.index, pd.DatetimeIndex):\n",
        "        # Try to infer frequency or assume daily if resampled\n",
        "        inferred_freq = pd.infer_freq(returns.index)\n",
        "        if inferred_freq in ['D', 'B']: # Daily or Business Daily\n",
        "             annualization_factor = np.sqrt(252) # Trading days\n",
        "        elif inferred_freq in ['H']: # Hourly\n",
        "             annualization_factor = np.sqrt(252 * 24) # Trading hours (approx)\n",
        "        # Add other frequencies as needed\n",
        "\n",
        "    # Sharpe Ratio\n",
        "    # Assuming risk-free rate is 0 for simplicity\n",
        "    if not returns.empty and returns.std() != 0:\n",
        "         sharpe_ratio = returns.mean() / returns.std() * annualization_factor\n",
        "         metrics['Sharpe Ratio'] = round(sharpe_ratio, 2)\n",
        "    else:\n",
        "         metrics['Sharpe Ratio'] = 0 # Or np.nan\n",
        "\n",
        "\n",
        "    # Sortino Ratio (uses downside deviation)\n",
        "    downside_returns = returns[returns < 0]\n",
        "    if not downside_returns.empty:\n",
        "         downside_std = downside_returns.std()\n",
        "         if downside_std != 0:\n",
        "              sortino_ratio = (returns.mean() / downside_std) * annualization_factor\n",
        "              metrics['Sortino Ratio'] = round(sortino_ratio, 2)\n",
        "         else:\n",
        "              metrics['Sortino Ratio'] = 0 # Downside std is 0 means no losing periods\n",
        "    else:\n",
        "         metrics['Sortino Ratio'] = 0 # No downside deviation if no negative returns\n",
        "\n",
        "\n",
        "    # Max Drawdown (already calculated correctly above)\n",
        "    # Recalculate Max Drawdown just in case the passed series is different\n",
        "    peak = portfolio_series.expanding(min_periods=1).max()\n",
        "    drawdown = (portfolio_series - peak) / peak\n",
        "    max_drawdown = drawdown.min() * 100 if not drawdown.empty else 0\n",
        "    metrics['Max Drawdown (%)'] = round(max_drawdown, 2)\n",
        "\n",
        "\n",
        "    # Trade Analysis\n",
        "    exit_trades = [t for t in trades_log if t.get('type') == 'exit'] # Use .get to avoid KeyError\n",
        "    metrics['Total Trades'] = len(exit_trades)\n",
        "\n",
        "    if exit_trades:\n",
        "        # Ensure 'pnl' key exists and is a number\n",
        "        trade_pnls = [t.get('pnl', 0) for t in exit_trades if isinstance(t.get('pnl', 0), (int, float))] # Filter for valid pnl\n",
        "\n",
        "        winning_trades = [pnl for pnl in trade_pnls if pnl > 0]\n",
        "        losing_trades = [pnl for pnl in trade_pnls if pnl < 0]\n",
        "\n",
        "        metrics['Winning Trades'] = len(winning_trades)\n",
        "        metrics['Losing Trades'] = len(losing_trades)\n",
        "        # Ensure total trades is not zero for division\n",
        "        metrics['Win Rate (%)'] = round((metrics['Winning Trades'] / metrics['Total Trades']) * 100, 2) if metrics['Total Trades'] > 0 else 0\n",
        "\n",
        "        metrics['Average PnL per Trade'] = round(sum(trade_pnls) / metrics['Total Trades'], 2) if metrics['Total Trades'] > 0 else 0\n",
        "        metrics['Average Winning Trade'] = round(sum(winning_trades) / metrics['Winning Trades'], 2) if metrics['Winning Trades'] > 0 else 0\n",
        "        metrics['Average Losing Trade'] = round(sum(losing_trades) / metrics['Losing Trades'], 2) if metrics['Losing Trades'] > 0 else 0 # Keep as negative average\n",
        "\n",
        "        total_winnings = sum(winning_trades)\n",
        "        total_losses = abs(sum(losing_trades)) # Use absolute value for profit factor denominator\n",
        "        metrics['Profit Factor'] = round(total_winnings / total_losses, 2) if total_losses > 0 else (100 if total_winnings > 0 else 0) # Handle division by zero\n",
        "\n",
        "\n",
        "    else: # No exit trades\n",
        "        metrics['Winning Trades'] = 0\n",
        "        metrics['Losing Trades'] = 0\n",
        "        metrics['Win Rate (%)'] = 0\n",
        "        metrics['Average PnL per Trade'] = 0\n",
        "        metrics['Average Winning Trade'] = 0\n",
        "        metrics['Average Losing Trade'] = 0\n",
        "        metrics['Profit Factor'] = 0 # Or 1.0 if no losses, but 0 is safer\n",
        "\n",
        "\n",
        "    # Ensure all metric values are finite before returning\n",
        "    for key, value in metrics.items():\n",
        "         if not np.isfinite(value):\n",
        "              metrics[key] = 0 # Replace non-finite values with 0 or another sensible default\n",
        "\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Function to run a single experiment (for main.py or testing)\n",
        "# This function is less relevant for the Streamlit dashboard's main loop,\n",
        "# but kept here for completeness or potential use elsewhere.\n",
        "# Removed this function as it's not used by the Streamlit dashboard and might cause confusion.\n",
        "# The Streamlit dashboard implements its own backtesting loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fe27f16"
      },
      "source": [
        "!pip install -q ta"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88cyigx8YdPN",
        "outputId": "d41a9c01-d4e9-4184-c9aa-a63d196abc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_utils.py\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import ta\n",
        "import numpy as np\n",
        "import re # Import regex for cleaning column names\n",
        "\n",
        "def download_forex_data(ticker=\"EURUSD=X\", start_date=\"2015-01-01\", end_date=\"2023-12-31\", timeframe=\"1d\"):\n",
        "    \"\"\"\n",
        "    Downloads historical Forex data using yfinance and performs initial cleaning.\n",
        "\n",
        "    Args:\n",
        "        ticker (str): Ticker symbol for the Forex pair (default: \"EURUSD=X\").\n",
        "        start_date (str): Start date for data download (YYYY-MM-DD).\n",
        "        end_date (str): End date for data download (YYYY-MM-DD).\n",
        "        timeframe (str): Data interval (e.g., \"1d\", \"1h\", \"15m\", \"5m\", \"1m\"). Default is \"1d\".\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: Downloaded and initially cleaned DataFrame, or None if download fails.\n",
        "    \"\"\"\n",
        "    print(f\"üìä –ò–∑—Ç–µ–≥–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏ –∑–∞ {ticker} –≤ —Ç–∞–π–º—Ñ—Ä–µ–π–º {timeframe}...\")\n",
        "    try:\n",
        "        # Download historical data for the specified ticker and timeframe\n",
        "        # Using auto_adjust=True to get adjusted close prices directly\n",
        "        data = yf.download(ticker, start=start_date, end=end_date, interval=timeframe, auto_adjust=True) # Added interval parameter\n",
        "        # Drop rows with any missing values immediately after download\n",
        "        data = data.dropna()\n",
        "        print(\"‚úÖ –î–∞–Ω–Ω–∏—Ç–µ —Å–∞ –∏–∑—Ç–µ–≥–ª–µ–Ω–∏ –∏ –ø–æ—á–∏—Å—Ç–µ–Ω–∏ –æ—Ç –ª–∏–ø—Å–≤–∞—â–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏.\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—Ç–µ–≥–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏ –∑–∞ {ticker} ({timeframe}): {e}\")\n",
        "        return None\n",
        "\n",
        "# === Helper Functions for Indicators/Patterns ===\n",
        "\n",
        "# === Classic Pivot Points ===\n",
        "def calculate_classic_pivots(df):\n",
        "    \"\"\"\n",
        "    Calculates Classic Pivot Points (P, R1, S1, R2, S2) based on daily OHLC.\n",
        "    Merges the daily pivot levels back into the original DataFrame.\n",
        "    \"\"\"\n",
        "    # Ensure the input DataFrame has a DatetimeIndex for resampling\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "         print(\"‚ö†Ô∏è Warning: DataFrame index is not DatetimeIndex. Cannot calculate daily pivots.\")\n",
        "         # Return original DataFrame with NaN columns for pivots\n",
        "         df['pivot_P'] = np.nan\n",
        "         df['pivot_R1'] = np.nan\n",
        "         df['pivot_S1'] = np.nan\n",
        "         df['pivot_R2'] = np.nan\n",
        "         df['pivot_S2'] = np.nan\n",
        "         return df\n",
        "\n",
        "    try:\n",
        "        # Aggregate to daily data to get High, Low, Close for pivot calculation\n",
        "        # Use .reset_index() to handle potential MultiIndex from yfinance\n",
        "        daily_df = df.copy().reset_index()\n",
        "        # Ensure date column is datetime before setting as index for resampling\n",
        "        if 'Date' in daily_df.columns: # Handle potential default yfinance 'Date' column\n",
        "             daily_df.rename(columns={'Date': 'date'}, inplace=True)\n",
        "        if 'date' in daily_df.columns:\n",
        "             daily_df['date'] = pd.to_datetime(daily_df['date'], errors='coerce')\n",
        "             daily_df.dropna(subset=['date'], inplace=True)\n",
        "             daily_df = daily_df.set_index('date').sort_index()\n",
        "             # Check if required OHLC columns are present after potential MultiIndex flattening\n",
        "             required_ohlc_daily = ['open', 'high', 'low', 'close']\n",
        "             if not all(col in daily_df.columns for col in required_ohlc_daily):\n",
        "                  print(f\"üö´ Error aggregating daily data for pivots: Missing required OHLC columns. Found: {list(daily_df.columns)}\")\n",
        "                  # Return original DataFrame with NaN columns for pivots\n",
        "                  df['pivot_P'] = np.nan\n",
        "                  df['pivot_R1'] = np.nan\n",
        "                  df['pivot_S1'] = np.nan\n",
        "                  df['pivot_R2'] = np.nan\n",
        "                  df['pivot_S2'] = np.nan\n",
        "                  return df\n",
        "\n",
        "             daily = daily_df.resample('1D').agg({'high': 'max', 'low': 'min', 'close': 'last'}).dropna()\n",
        "\n",
        "             if daily.empty:\n",
        "                  print(\"‚ö†Ô∏è Warning: Daily aggregated data is empty after dropna. Cannot calculate pivots.\")\n",
        "                  # Return original DataFrame with NaN columns for pivots\n",
        "                  df['pivot_P'] = np.nan\n",
        "                  df['pivot_R1'] = np.nan\n",
        "                  df['pivot_S1'] = np.nan\n",
        "                  df['pivot_R2'] = np.nan\n",
        "                  df['pivot_S2'] = np.nan\n",
        "                  return df\n",
        "\n",
        "\n",
        "             # Calculate Pivot Points\n",
        "             pivots_daily = pd.DataFrame(index=daily.index)\n",
        "             pivots_daily['P'] = (daily['high'] + daily['low'] + daily['close']) / 3\n",
        "             pivots_daily['R1'] = 2 * pivots_daily['P'] - daily['low']\n",
        "             pivots_daily['S1'] = 2 * pivots_daily['P'] - daily['high']\n",
        "             pivots_daily['R2'] = pivots_daily['P'] + (daily['high'] - daily['low'])\n",
        "             pivots_daily['S2'] = pivots_daily['P'] - (daily['high'] - daily['low'])\n",
        "\n",
        "             # Shift pivots by one day so that today's pivots are based on yesterday's data\n",
        "             pivots_daily = pivots_daily.shift(1)\n",
        "\n",
        "             # Rename columns to avoid conflicts and clarify they are pivot levels\n",
        "             pivots_daily.rename(columns={\n",
        "                 'P': 'pivot_P',\n",
        "                 'R1': 'pivot_R1',\n",
        "                 'S1': 'pivot_S1',\n",
        "                 'R2': 'pivot_R2',\n",
        "                 'S2': 'pivot_S2'\n",
        "             }, inplace=True)\n",
        "\n",
        "             # Reindex the daily pivots DataFrame to match the original DataFrame's index (which is higher frequency)\n",
        "             # This will forward-fill the daily pivot values to all rows within that day\n",
        "             # Ensure both DataFrames have DatetimeIndex and are sorted for correct merging/reindexing\n",
        "             df_sorted = df.sort_index()\n",
        "             pivots_daily_sorted = pivots_daily.sort_index()\n",
        "\n",
        "             # Use reindex and ffill to merge daily pivots into the high-frequency data\n",
        "             # Create a combined index to ensure all timestamps are covered\n",
        "             combined_index = df_sorted.index.union(pivots_daily_sorted.index)\n",
        "             pivots_reindexed = pivots_daily_sorted.reindex(combined_index).ffill()\n",
        "\n",
        "             # Now merge the reindexed pivots back into the original DataFrame\n",
        "             # We only need the pivot columns\n",
        "             pivot_cols_to_merge = ['pivot_P', 'pivot_R1', 'pivot_S1', 'pivot_R2', 'pivot_S2']\n",
        "             # Select only the rows from pivots_reindexed whose index is present in df_sorted\n",
        "             merged_df = df_sorted.join(pivots_reindexed[pivot_cols_to_merge], how='left')\n",
        "\n",
        "             print(\"‚úÖ –ö–ª–∞—Å–∏—á–µ—Å–∫–∏ –ø–∏–≤–æ—Ç –ø–æ–π–Ω—Ç–∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏.\")\n",
        "             return merged_df # Return the DataFrame with pivot columns added\n",
        "\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è Warning: Could not find or convert date column for daily pivot calculation.\")\n",
        "             # Return original DataFrame with NaN columns for pivots\n",
        "             df['pivot_P'] = np.nan\n",
        "             df['pivot_R1'] = np.nan\n",
        "             df['pivot_S1'] = np.nan\n",
        "             df['pivot_R2'] = np.nan\n",
        "             df['pivot_S2'] = np.nan\n",
        "             return df\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –ö–ª–∞—Å–∏—á–µ—Å–∫–∏ –ø–∏–≤–æ—Ç –ø–æ–π–Ω—Ç–∏: {e}\")\n",
        "        # Return original DataFrame with NaN columns for pivots on error\n",
        "        df['pivot_P'] = np.nan\n",
        "        df['pivot_R1'] = np.nan\n",
        "        df['pivot_S1'] = np.nan\n",
        "        df['pivot_R2'] = np.nan\n",
        "        df['pivot_S2'] = np.nan\n",
        "        return df\n",
        "\n",
        "\n",
        "# === Candlestick Patterns ===\n",
        "# Using the 'ta' library for pattern detection as it's more efficient\n",
        "# This function will add binary columns (0 or 1) for detected patterns.\n",
        "def add_candlestick_patterns(df):\n",
        "    \"\"\"\n",
        "    Adds binary columns for various candlestick patterns using the 'ta' library.\n",
        "    \"\"\"\n",
        "    processed_data = df.copy()\n",
        "\n",
        "    try:\n",
        "        # Ensure OHLC columns are present and in lowercase\n",
        "        required_ohlc = ['open', 'high', 'low', 'close']\n",
        "        if not all(col in processed_data.columns for col in required_ohlc):\n",
        "            print(f\"üö´ Error adding candlestick patterns: Missing required OHLC columns. Found: {list(processed_data.columns)}\")\n",
        "            # Return original DataFrame without pattern columns\n",
        "            return processed_data\n",
        "\n",
        "\n",
        "        print(\"‚ÑπÔ∏è –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —è–ø–æ–Ω—Å–∫–∏ —Å–≤–µ—â–Ω–∏ –ø–∞—Ç–µ—Ä–Ω–∏...\")\n",
        "\n",
        "        # Example patterns using ta.earliest_signal() which returns 1 if pattern is true, 0 otherwise\n",
        "        # You can add more patterns from ta.clean.candlestick.TACCandlestick\n",
        "        # Note: ta.earliest_signal requires specific column names (Open, High, Low, Close) - ensure case matches or use .lower() consistently\n",
        "\n",
        "        # Ensure OHLC columns are correctly cased if ta expects specific case\n",
        "        # Let's assume ta expects lowercase based on typical usage, but it's good to double-check documentation if issues arise.\n",
        "        # No need to do complex casing checks here, assuming previous steps ensured lowercase.\n",
        "\n",
        "        # Bullish Patterns\n",
        "        try:\n",
        "            # Existing patterns\n",
        "            processed_data['pattern_bullish_engulfing'] = ta.cdl_engulfing(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "            processed_data['pattern_hammer'] = ta.cdl_hammer(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "            processed_data['pattern_morning_star'] = ta.cdl_morningstar(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "            processed_data['pattern_three_white_soldiers'] = ta.cdl_3whitesoldiers(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "\n",
        "            # New patterns from user request\n",
        "            processed_data['pattern_doji'] = ta.cdl_doji(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) != 0 # Doji is non-zero for Doji\n",
        "            # Note: ta.cdl_morningstar and ta.cdl_eveningstar are already included above\n",
        "            processed_data['pattern_piercing_line'] = ta.cdl_piercing(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "\n",
        "\n",
        "            # Bearish Patterns\n",
        "            # Existing patterns\n",
        "            processed_data['pattern_bearish_engulfing'] = ta.cdl_engulfing(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) < 0\n",
        "            processed_data['pattern_hanging_man'] = ta.cdl_hangingman(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "            processed_data['pattern_evening_star'] = ta.cdl_eveningstar(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "            processed_data['pattern_three_black_crows'] = ta.cdl_3blackcrows(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) > 0\n",
        "\n",
        "            # New patterns from user request\n",
        "            processed_data['pattern_dark_cloud_cover'] = ta.cdl_darkcloudcover(processed_data['open'], processed_data['high'], processed_data['low'], processed_data['close']) < 0\n",
        "\n",
        "\n",
        "            # Convert boolean results to integer (1 for true, 0 for false)\n",
        "            pattern_cols = [col for col in processed_data.columns if col.startswith('pattern_')]\n",
        "            for col in pattern_cols:\n",
        "                 # ta returns integer values, not boolean. Convert to binary (1 or 0).\n",
        "                 # Any non-zero value from ta pattern function indicates the pattern.\n",
        "                 processed_data[col] = (processed_data[col] != 0).astype(int)\n",
        "\n",
        "\n",
        "            print(\"‚úÖ –Ø–ø–æ–Ω—Å–∫–∏ —Å–≤–µ—â–Ω–∏ –ø–∞—Ç–µ—Ä–Ω–∏ –¥–æ–±–∞–≤–µ–Ω–∏.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤—è–Ω–µ –Ω–∞ —è–ø–æ–Ω—Å–∫–∏ —Å–≤–µ—â–Ω–∏ –ø–∞—Ç–µ—Ä–Ω–∏ —Å 'ta' –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ç–∞: {e}\")\n",
        "            # Ensure pattern columns are not left with partial results or errors\n",
        "            pattern_cols_created = [col for col in processed_data.columns if col.startswith('pattern_')]\n",
        "            processed_data.drop(columns=pattern_cols_created, errors='ignore', inplace=True) # Drop any partially created columns\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ –ù–µ–æ—á–∞–∫–≤–∞–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞ –∑–∞ –¥–æ–±–∞–≤—è–Ω–µ –Ω–∞ –ø–∞—Ç–µ—Ä–Ω–∏: {e}\")\n",
        "\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "def add_technical_indicators(df, atr_window=14):\n",
        "    \"\"\"\n",
        "    Adds technical indicators, Classic Pivot Points, and Candlestick Patterns\n",
        "    to the DataFrame.\n",
        "    Assumes input df has 'open', 'high', 'low', 'close', 'volume' columns (case-insensitive).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame with raw price data.\n",
        "        atr_window (int): Window size for ATR calculation. Default is 14.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: DataFrame with added technical indicators, pivots, and patterns,\n",
        "                              and rows with initial NaNs removed, or None if required columns\n",
        "                              are missing or all rows are dropped.\n",
        "    \"\"\"\n",
        "    processed_data = df.copy()\n",
        "\n",
        "    # --- Add logging for raw columns immediately ---\n",
        "    print(f\"‚ÑπÔ∏è Raw columns upon entering add_technical_indicators: {list(processed_data.columns)}\")\n",
        "    print(f\"‚ÑπÔ∏è Raw column dtypes upon entering add_technical_indicators: {processed_data.dtypes}\")\n",
        "    print(f\"‚ÑπÔ∏è Number of rows upon entering add_technical_indicators: {len(processed_data)}\")\n",
        "\n",
        "\n",
        "    # --- Corrected Column Handling ---\n",
        "    # Check if columns are a MultiIndex (often happens with yfinance)\n",
        "    if isinstance(processed_data.columns, pd.MultiIndex):\n",
        "        print(\"‚ÑπÔ∏è Detected MultiIndex columns. Attempting to flatten.\")\n",
        "        # Flatten MultiIndex by taking the first level name (e.g., 'Open', 'High', 'Close')\n",
        "        # If a column name is empty after flattening (e.g., only ticker was present), use the full original name\n",
        "        # Also handle potential 'Adj Close' from yfinance by renaming it to 'close'\n",
        "        new_columns = []\n",
        "        for col in processed_data.columns:\n",
        "             if isinstance(col, tuple):\n",
        "                  col_name = col[0] if col[0] else '_'.join(str(c) for c in col)\n",
        "                  if col_name == 'Adj Close': # Handle Adj Close specifically\n",
        "                       col_name = 'close'\n",
        "                  new_columns.append(col_name)\n",
        "             else:\n",
        "                  col_name = str(col)\n",
        "                  if col_name == 'Adj Close': # Handle Adj Close specifically\n",
        "                       col_name = 'close'\n",
        "                  new_columns.append(col_name)\n",
        "        processed_data.columns = new_columns\n",
        "        print(f\"‚úÖ Columns flattened: {list(processed_data.columns)}\")\n",
        "\n",
        "\n",
        "    # Ensure columns are strings and strip whitespace\n",
        "    processed_data.columns = [str(col).strip() for col in processed_data.columns]\n",
        "\n",
        "    # Remove emojis or special characters from column names using regex\n",
        "    processed_data.columns = [re.sub(r'[^\\w\\s]', '', col) for col in processed_data.columns]\n",
        "    print(f\"‚úÖ Special characters/emojis removed from column names: {list(processed_data.columns)}\")\n",
        "\n",
        "\n",
        "    # Convert column names to lowercase\n",
        "    processed_data.columns = processed_data.columns.str.lower()\n",
        "    print(f\"‚úÖ Column names converted to lowercase: {list(processed_data.columns)}\")\n",
        "\n",
        "\n",
        "    # Add translation map for Bulgarian column names\n",
        "    translation_map = {\n",
        "        '–æ—Ç–≤–æ—Ä–∏': 'open',\n",
        "        '–≤—Ä—ä—Ö': 'high',\n",
        "        '–¥—ä–Ω–æ': 'low',\n",
        "        '–∑–∞—Ç–≤–æ—Ä–∏': 'close',\n",
        "        '–æ–±–µ–º': 'volume',\n",
        "        '–≤—Ä–µ–º–µ': 'date'\n",
        "    }\n",
        "\n",
        "    # Rename columns using the translation map if they exist\n",
        "    # Use a dictionary comprehension to filter for columns that actually exist in the DataFrame\n",
        "    columns_to_rename = {col: translation_map[col] for col in translation_map if col in processed_data.columns}\n",
        "    if columns_to_rename:\n",
        "        processed_data.rename(columns=columns_to_rename, inplace=True)\n",
        "        print(f\"‚úÖ Bulgarian column names translated: {columns_to_rename}\")\n",
        "        print(f\"‚úÖ Columns after translation: {list(processed_data.columns)}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No Bulgarian columns found to translate.\")\n",
        "\n",
        "\n",
        "    # Check for required columns after all renaming/cleaning\n",
        "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "    missing_cols = [col for col in required_cols if col not in processed_data.columns]\n",
        "    if missing_cols:\n",
        "         print(f\"üö´ Error in preprocessing: Missing required price columns after cleaning and translation. Missing: {missing_cols}, Found: {list(processed_data.columns)}\")\n",
        "         return None\n",
        "\n",
        "    # Check if the 'volume' column is all zeros or NaN\n",
        "    if 'volume' in processed_data.columns:\n",
        "        if processed_data['volume'].sum() == 0:\n",
        "            print(\"Warning: 'volume' column contains only zeros. Technical indicators relying on volume might be affected.\")\n",
        "        elif processed_data['volume'].isnull().all():\n",
        "             print(\"Warning: 'volume' column contains only NaN values.\")\n",
        "\n",
        "\n",
        "    # --- Date Handling: Preserve original index/date ---\n",
        "    # Check if a 'date' column already exists or if the index is a DatetimeIndex\n",
        "    original_date_col_exists = 'date' in processed_data.columns\n",
        "    original_index_is_datetime = isinstance(processed_data.index, pd.DatetimeIndex)\n",
        "\n",
        "    # If the original index is a DatetimeIndex, reset and rename it to 'date' if no 'date' column exists\n",
        "    if original_index_is_datetime and not original_date_col_exists:\n",
        "         print(\"‚ÑπÔ∏è Original index is DatetimeIndex and no 'date' column exists. Resetting index and renaming.\")\n",
        "         processed_data = processed_data.reset_index()\n",
        "         # Check if the resulting column is named 'index' or something else and rename to 'date'\n",
        "         # Use the original index name if available, otherwise default to 'index'\n",
        "         index_col_name = df.index.name if df.index.name is not None else 'index'\n",
        "         if index_col_name in processed_data.columns:\n",
        "              processed_data.rename(columns={index_col_name: 'date'}, inplace=True)\n",
        "              original_date_col_exists = True\n",
        "              print(f\"‚úÖ Renamed index column '{index_col_name}' to 'date'.\")\n",
        "         else:\n",
        "              print(f\"‚ö†Ô∏è Warning: DatetimeIndex found but could not rename index column '{index_col_name}' to 'date' after reset.\")\n",
        "\n",
        "\n",
        "    # If after attempting to rename index, there's still no 'date' column and the index is not DatetimeIndex,\n",
        "    # and no 'original_index' or 'level_0' (from default reset_index) exists,\n",
        "    # create a simple sequential index column as a fallback.\n",
        "    if not original_date_col_exists and not isinstance(processed_data.index, pd.DatetimeIndex) and 'original_index' not in processed_data.columns and 'level_0' not in processed_data.columns:\n",
        "         print(\"‚ÑπÔ∏è No 'date' column and index is not DatetimeIndex. Creating 'original_index' column.\")\n",
        "         processed_data = processed_data.reset_index()\n",
        "         if 'index' in processed_data.columns:\n",
        "              processed_data.rename(columns={'index': 'original_index'}, inplace=True)\n",
        "              print(\"‚ÑπÔ∏è Could not identify or convert original index to datetime. Proceeding with 'original_index' column.\")\n",
        "         elif 'level_0' in processed_data.columns: # Handle default name from reset_index\n",
        "              processed_data.rename(columns={'level_0': 'original_index'}, inplace=True)\n",
        "              print(\"‚ÑπÔ∏è Could not identify or convert original index to datetime. Proceeding with 'original_index' column.\")\n",
        "\n",
        "\n",
        "    # Ensure 'date' column is datetime if it exists\n",
        "    if 'date' in processed_data.columns:\n",
        "        try:\n",
        "            processed_data['date'] = pd.to_datetime(processed_data['date'], errors='coerce')\n",
        "            processed_data.dropna(subset=['date'], inplace=True) # Drop rows where date conversion failed\n",
        "            if not processed_data.empty:\n",
        "                 # Set date as index temporarily for resampling/joining if needed\n",
        "                 processed_data = processed_data.set_index('date').sort_index()\n",
        "                 original_index_is_datetime = True # Now the index is datetime\n",
        "                 print(\"‚úÖ 'date' column converted to datetime and set as index.\")\n",
        "            else:\n",
        "                 print(\"üö´ All rows dropped after converting 'date' column to datetime and dropping NaT.\")\n",
        "                 return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ Error converting 'date' column to datetime: {e}. Keeping original index/columns.\")\n",
        "            # If date conversion fails, revert to the state before setting index\n",
        "            # If the index was already datetime, keep it as is. Otherwise, reset.\n",
        "            if 'date' in processed_data.columns and not isinstance(processed_data.index, pd.DatetimeIndex):\n",
        "                # Only reset index if it wasn't already a DatetimeIndex\n",
        "                # Check if 'date' column was created from reset_index before\n",
        "                if processed_data.index.name == 'level_0' or processed_data.index.name == 'index': # Heuristic for default reset_index names\n",
        "                     processed_data = processed_data.reset_index(drop=False) # Keep date column\n",
        "                elif 'date' in processed_data.columns: # If 'date' column existed originally\n",
        "                     pass # Keep the existing 'date' column, index is not datetime\n",
        "                print(\"‚ÑπÔ∏è Date conversion failed, proceeding without DatetimeIndex.\")\n",
        "\n",
        "            original_index_is_datetime = False\n",
        "\n",
        "\n",
        "    # Add logging before indicator calculation\n",
        "    print(f\"‚ÑπÔ∏è –î–∞–Ω–Ω–∏ –ø—Ä–µ–¥–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏. –ü—ä—Ä–≤–∏ 5 —Ä–µ–¥–∞:\")\n",
        "    print(processed_data.head())\n",
        "    print(f\"‚ÑπÔ∏è –ö–æ–ª–æ–Ω–∏ –≤ –¥–∞–Ω–Ω–∏—Ç–µ: {list(processed_data.columns)}\")\n",
        "    print(f\"‚ÑπÔ∏è –¢–∏–ø–æ–≤–µ –¥–∞–Ω–Ω–∏ –Ω–∞ –∫–æ–ª–æ–Ω–∏: {processed_data.dtypes}\")\n",
        "    print(f\"‚ÑπÔ∏è –ë—Ä–æ–π —Ä–µ–¥–æ–≤–µ –ø—Ä–µ–¥–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏: {len(processed_data)}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # --- Calculate Standard Technical Indicators ---\n",
        "        # Add try-except for each indicator calculation\n",
        "\n",
        "        # Simple Moving Average (SMA)\n",
        "        window_length_sma = 20\n",
        "        try:\n",
        "            print(f\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ SMA (window={window_length_sma})...\")\n",
        "            sma_indicator = ta.trend.SMAIndicator(close=processed_data['close'], window=window_length_sma)\n",
        "            processed_data['sma'] = sma_indicator.sma_indicator()\n",
        "            print(\"‚úÖ SMA –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ SMA: {e}\")\n",
        "            processed_data['sma'] = np.nan\n",
        "\n",
        "        # Relative Strength Index (RSI)\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ RSI...\")\n",
        "            rsi_indicator = ta.momentum.RSIIndicator(close=processed_data['close'])\n",
        "            processed_data['rsi'] = rsi_indicator.rsi()\n",
        "            print(\"‚úÖ RSI –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ RSI: {e}\")\n",
        "            processed_data['rsi'] = np.nan\n",
        "\n",
        "        # Moving Average Convergence Divergence (MACD)\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ MACD...\")\n",
        "            macd_indicator = ta.trend.MACD(close=processed_data['close'])\n",
        "            processed_data['macd'] = macd_indicator.macd()\n",
        "            processed_data['macd_signal'] = macd_indicator.macd_signal()\n",
        "            processed_data['macd_diff'] = macd_indicator.macd_diff()\n",
        "            print(\"‚úÖ MACD –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ MACD: {e}\")\n",
        "            processed_data['macd'] = np.nan\n",
        "            processed_data['macd_signal'] = np.nan\n",
        "            processed_data['macd_diff'] = np.nan\n",
        "\n",
        "        # Bollinger Bands\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ Bollinger Bands...\")\n",
        "            bb = ta.volatility.BollingerBands(close=processed_data['close'])\n",
        "            processed_data['bb_upper'] = bb.bollinger_hband()\n",
        "            processed_data['bb_lower'] = bb.bollinger_lband()\n",
        "            processed_data['bb_mavg'] = bb.bollinger_mavg()\n",
        "            print(\"‚úÖ Bollinger Bands –∏–∑—á–∏—Å–ª–µ–Ω–∏.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ Bollinger Bands: {e}\")\n",
        "            processed_data['bb_upper'] = np.nan\n",
        "            processed_data['bb_lower'] = np.nan\n",
        "            processed_data['bb_mavg'] = np.nan\n",
        "\n",
        "        # Exponential Moving Average (EMA)\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ EMA...\")\n",
        "            ema_indicator_short = ta.trend.EMAIndicator(close=processed_data['close'], window=12)\n",
        "            ema_indicator_long = ta.trend.EMAIndicator(close=processed_data['close'], window=26)\n",
        "            processed_data['ema_12'] = ema_indicator_short.ema_indicator()\n",
        "            processed_data['ema_26'] = ema_indicator_long.ema_indicator()\n",
        "            print(\"‚úÖ EMA –∏–∑—á–∏—Å–ª–µ–Ω–∏.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ EMA: {e}\")\n",
        "            processed_data['ema_12'] = np.nan\n",
        "            processed_data['ema_26'] = np.nan\n",
        "\n",
        "        # Commodity Channel Index (CCI)\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ CCI...\")\n",
        "            cci_indicator = ta.trend.CCIIndicator(high=processed_data['high'],\n",
        "                                   low=processed_data['low'],\n",
        "                                   close=processed_data['close'], window=20)\n",
        "            processed_data['cci'] = cci_indicator.cci()\n",
        "            print(\"‚úÖ CCI –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ CCI: {e}\")\n",
        "            processed_data['cci'] = np.nan\n",
        "\n",
        "        # Average Directional Index (ADX)\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ ADX...\")\n",
        "            adx_indicator = ta.trend.ADXIndicator(high=processed_data['high'],\n",
        "                                       low=processed_data['low'],\n",
        "                                       close=processed_data['close'], window=14)\n",
        "            processed_data['adx'] = adx_indicator.adx()\n",
        "            processed_data['adx_pos'] = adx_indicator.adx_pos()\n",
        "            processed_data['adx_neg'] = adx_indicator.adx_neg()\n",
        "            print(\"‚úÖ ADX –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as adx_e:\n",
        "            print(f\"üö´ Could not calculate ADX indicator: {adx_e}\")\n",
        "            processed_data['adx'] = np.nan\n",
        "            processed_data['adx_pos'] = np.nan\n",
        "            processed_data['adx_neg'] = np.nan\n",
        "\n",
        "        # Stochastic Oscillator (STOCH)\n",
        "        try:\n",
        "            print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ Stochastic Oscillator...\")\n",
        "            stoch_indicator = ta.momentum.StochasticOscillator(high=processed_data['high'], low=processed_data['low'], close=processed_data['close'])\n",
        "            processed_data['stoch_k'] = stoch_indicator.stoch()\n",
        "            processed_data['stoch_d'] = stoch_indicator.stoch_signal()\n",
        "            print(\"‚úÖ Stochastic Oscillator –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ Stochastic Oscillator: {e}\")\n",
        "            processed_data['stoch_k'] = np.nan\n",
        "            processed_data['stoch_d'] = np.nan\n",
        "\n",
        "        # Average True Range (ATR) - Added for Volatility Lot Model\n",
        "        try:\n",
        "            print(f\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ ATR (window={atr_window})...\")\n",
        "            atr_indicator = ta.volatility.AverageTrueRange(high=processed_data['high'], low=processed_data['low'], close=processed_data['close'], window=atr_window)\n",
        "            processed_data['atr'] = atr_indicator.average_true_range()\n",
        "            print(\"‚úÖ ATR –∏–∑—á–∏—Å–ª–µ–Ω.\")\n",
        "        except Exception as e:\n",
        "            print(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ ATR: {e}\")\n",
        "            processed_data['atr'] = np.nan\n",
        "\n",
        "\n",
        "        # --- Calculate Classic Pivot Points ---\n",
        "        # This function assumes the DataFrame has a DatetimeIndex.\n",
        "        # If not, it will add NaN columns.\n",
        "        if isinstance(processed_data.index, pd.DatetimeIndex): # Check again after potential date column handling\n",
        "             print(\"‚ÑπÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –ö–ª–∞—Å–∏—á–µ—Å–∫–∏ –ø–∏–≤–æ—Ç –ø–æ–π–Ω—Ç–∏ (–∏–∑–∏—Å–∫–≤–∞ DatetimeIndex)...\")\n",
        "             processed_data = calculate_classic_pivots(processed_data)\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è Skipping Classic Pivot Point calculation as DataFrame index is not DatetimeIndex after processing.\")\n",
        "             # Add NaN pivot columns to maintain consistent structure even if not calculated\n",
        "             processed_data['pivot_P'] = np.nan\n",
        "             processed_data['pivot_R1'] = np.nan\n",
        "             processed_data['pivot_S1'] = np.nan\n",
        "             processed_data['pivot_R2'] = np.nan\n",
        "             processed_data['pivot_S2'] = np.nan\n",
        "\n",
        "\n",
        "        # --- Add Candlestick Patterns ---\n",
        "        # Ensure the DataFrame has the required OHLC columns and is not empty before adding patterns\n",
        "        if not processed_data.empty and all(col in processed_data.columns for col in ['open', 'high', 'low', 'close']):\n",
        "             print(\"‚ÑπÔ∏è –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —è–ø–æ–Ω—Å–∫–∏ —Å–≤–µ—â–Ω–∏ –ø–∞—Ç–µ—Ä–Ω–∏...\")\n",
        "             processed_data = add_candlestick_patterns(processed_data)\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è Skipping Candlestick Pattern calculation as DataFrame is empty or missing OHLC columns.\")\n",
        "\n",
        "\n",
        "        # --- Check for missing values after adding indicators and patterns ---\n",
        "        missing_pct = processed_data.isnull().mean() * 100\n",
        "        print(f\"‚ÑπÔ∏è –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–∞ –ª–∏–ø—Å–≤–∞—â–∏ —Å—Ç–æ–π–Ω–æ—Å—Ç–∏ –ø–æ –∫–æ–ª–æ–Ω–∞ —Å–ª–µ–¥ –¥–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∏ –ø–∞—Ç–µ—Ä–Ω–∏:\\n{missing_pct[missing_pct > 0]}\") # Only print columns with missing values\n",
        "\n",
        "        # Drop rows with NaN values introduced by indicators and patterns.\n",
        "        # Update the list of columns to check for NaNs\n",
        "        # Include OHLC columns in the check in case they became NaN somehow\n",
        "        cols_to_check_for_nan = ['open', 'high', 'low', 'close', 'volume',\n",
        "                                   'sma', 'rsi', 'macd', 'macd_signal', 'macd_diff',\n",
        "                                   'bb_upper', 'bb_lower', 'bb_mavg',\n",
        "                                   'ema_12', 'ema_26', 'cci', 'adx', 'adx_pos', 'adx_neg',\n",
        "                                   'stoch_k', 'stoch_d', 'atr',\n",
        "                                   'pivot_P', 'pivot_R1', 'pivot_S1', 'pivot_R2', 'pivot_S2', # Pivot columns\n",
        "                                   # Candlestick pattern columns are binary (0 or 1), should not have NaNs unless calculation failed completely\n",
        "                                   # Add them here if you want to drop rows where pattern calculation failed for *any* pattern\n",
        "                                   # Example: [col for col in processed_data.columns if col.startswith('pattern_')]\n",
        "                                  ]\n",
        "\n",
        "        # Filter for columns that actually exist in the DataFrame\n",
        "        cols_to_check_for_nan_present = [col for col in cols_to_check_for_nan if col in processed_data.columns]\n",
        "\n",
        "\n",
        "        initial_rows = len(processed_data)\n",
        "        if cols_to_check_for_nan_present:\n",
        "            print(f\"‚ÑπÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ NaNs –≤ –∫–æ–ª–æ–Ω–∏: {cols_to_check_for_nan_present}\")\n",
        "            # Use .loc[:] or similar if dropping NaNs introduces fragmented index\n",
        "            processed_data.dropna(subset=cols_to_check_for_nan_present, inplace=True)\n",
        "            rows_after_dropna = len(processed_data)\n",
        "            if initial_rows > rows_after_dropna:\n",
        "                print(f\"‚ÑπÔ∏è Dropped {initial_rows - rows_after_dropna} rows with NaN values after adding indicators and patterns.\")\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è No rows with NaNs found in checked columns after adding indicators and patterns.\")\n",
        "\n",
        "        else:\n",
        "             print(\"‚ÑπÔ∏è No columns specified to check for NaNs after adding indicators and patterns.\")\n",
        "\n",
        "\n",
        "        # Check if DataFrame is empty after dropping NaNs\n",
        "        if processed_data.empty:\n",
        "             print(\"‚ö†Ô∏è Warning: All rows dropped after adding indicators/patterns and removing NaNs.\")\n",
        "             return None\n",
        "\n",
        "        # Reset index after dropping rows if index is DatetimeIndex, to make it a regular column again for the environment\n",
        "        # Only reset if the index is currently a DatetimeIndex\n",
        "        if isinstance(processed_data.index, pd.DatetimeIndex):\n",
        "            print(\"‚ÑπÔ∏è Resetting DatetimeIndex to a regular column after dropping NaNs.\")\n",
        "            processed_data = processed_data.reset_index(drop=False) # Keep the date column\n",
        "            processed_data.rename(columns={'index': 'date'}, inplace=True) # Ensure it's named 'date'\n",
        "            print(f\"‚úÖ Index reset. Columns now: {list(processed_data.columns)}\")\n",
        "\n",
        "\n",
        "        # Re-add sequential index if it was lost and no date column exists\n",
        "        # Check for both 'date' and 'original_index' before adding sequential_index\n",
        "        if 'date' not in processed_data.columns and 'original_index' not in processed_data.columns:\n",
        "             print(\"‚ÑπÔ∏è No 'date' or 'original_index' column found. Adding 'sequential_index'.\")\n",
        "             processed_data['sequential_index'] = processed_data.index\n",
        "             print(\"‚ÑπÔ∏è Re-added 'sequential_index' column as no 'date' or 'original_index' column is present.\")\n",
        "\n",
        "\n",
        "        # Final check on data types and column names\n",
        "        processed_data.columns = processed_data.columns.astype(str) # Ensure column names are strings\n",
        "        print(\"‚úÖ Column names ensured to be strings.\")\n",
        "\n",
        "\n",
        "        # --- Reorder columns ---\n",
        "        base_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
        "        # Ensure base_cols are actually present in the DataFrame before trying to select them\n",
        "        present_base_cols = [col for col in base_cols if col in processed_data.columns]\n",
        "        indicator_cols = [col for col in processed_data.columns if col not in present_base_cols]\n",
        "\n",
        "        # Ensure indicator_cols does not contain 'original_index' or 'sequential_index' if they exist\n",
        "        indicator_cols = [col for col in indicator_cols if col not in ['original_index', 'sequential_index']]\n",
        "\n",
        "        # Combine and reorder\n",
        "        ordered_cols = present_base_cols + sorted(indicator_cols) # Sort indicator columns alphabetically\n",
        "        # Add back 'original_index' or 'sequential_index' if they exist and are not already in ordered_cols\n",
        "        if 'original_index' in processed_data.columns and 'original_index' not in ordered_cols:\n",
        "             ordered_cols.append('original_index')\n",
        "        if 'sequential_index' in processed_data.columns and 'sequential_index' not in ordered_cols:\n",
        "             ordered_cols.append('sequential_index')\n",
        "\n",
        "        # Select columns in the new order\n",
        "        processed_data = processed_data[ordered_cols]\n",
        "        print(\"‚úÖ Columns reordered.\")\n",
        "\n",
        "\n",
        "        # Add logging after indicator calculation and dropna\n",
        "        print(f\"‚ÑπÔ∏è –î–∞–Ω–Ω–∏ —Å–ª–µ–¥ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏, –ø–∏–≤–æ—Ç–∏ –∏ –ø–∞—Ç–µ—Ä–Ω–∏ –∏ –ø—Ä–µ–º–∞—Ö–≤–∞–Ω–µ –Ω–∞ NaNs. –ü—ä—Ä–≤–∏ 5 —Ä–µ–¥–∞:\")\n",
        "        print(processed_data.head())\n",
        "        print(f\"‚ÑπÔ∏è –ö–æ–ª–æ–Ω–∏ –≤ –¥–∞–Ω–Ω–∏—Ç–µ: {list(processed_data.columns)}\")\n",
        "        print(f\"‚ÑπÔ∏è –¢–∏–ø–æ–≤–µ –¥–∞–Ω–Ω–∏ –Ω–∞ –∫–æ–ª–æ–Ω–∏: {processed_data.dtypes}\")\n",
        "        print(f\"‚ÑπÔ∏è –ë—Ä–æ–π —Ä–µ–¥–æ–≤–µ —Å–ª–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∞: {len(processed_data)}\")\n",
        "\n",
        "        if processed_data.empty:\n",
        "             print(\"üö´ –û–±—Ä–∞–±–æ—Ç–µ–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏ —Å–∞ –ø—Ä–∞–∑–Ω–∏ —Å–ª–µ–¥ –≤—Å–∏—á–∫–∏ —Å—Ç—ä–ø–∫–∏. –í—Ä—ä—â–∞–Ω–µ –Ω–∞ None.\")\n",
        "             return None\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ An unexpected error occurred during technical indicator/pattern calculation: {e}\")\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "def split_data(df, split_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits a DataFrame into training and testing sets based on a ratio.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to split.\n",
        "        split_ratio (float): The ratio of data to use for the training set (0.0 to 1.0).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing (train_df, test_df), or (None, None) if the input df is invalid.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(\"üö´ Cannot split data: Input DataFrame is not available or is empty.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        split_index = int(len(df) * split_ratio)\n",
        "        train_data = df.iloc[:split_index].copy().reset_index(drop=True)\n",
        "        test_data = df.iloc[split_index:].copy().reset_index(drop=True)\n",
        "        print(f\"\\n‚û°Ô∏è Data split into Training ({len(train_data)} rows) and Testing ({len(test_data)} rows) sets using {split_ratio*100:.0f}/{ (1-split_ratio)*100:.0f} percentage split.\")\n",
        "        return train_data, test_data\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ An error occurred during data splitting: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NU6ZWsyfF9In"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === 1. –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏ ===\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "    df = df[['open', 'high', 'low', 'close', 'volume']].dropna()\n",
        "    return df\n",
        "\n",
        "# === 2. Classic Pivot Points ===\n",
        "def calculate_pivots(df):\n",
        "    daily = df.resample('1D').agg({'high': 'max', 'low': 'min', 'close': 'last'})\n",
        "    pivots = pd.DataFrame(index=daily.index)\n",
        "    pivots['P'] = (daily['high'] + daily['low'] + daily['close']) / 3\n",
        "    pivots['R1'] = 2 * pivots['P'] - daily['low']\n",
        "    pivots['S1'] = 2 * pivots['P'] - daily['high']\n",
        "    pivots['R2'] = pivots['P'] + (daily['high'] - daily['low'])\n",
        "    pivots['S2'] = pivots['P'] - (daily['high'] - daily['low'])\n",
        "    return pivots\n",
        "\n",
        "# === 3. –Ø–ø–æ–Ω—Å–∫–∏ —Å–≤–µ—â–Ω–∏ –ø–∞—Ç–µ—Ä–Ω–∏ ===\n",
        "def detect_patterns(df):\n",
        "    signals = []\n",
        "\n",
        "    for i in range(2, len(df)):\n",
        "        o1, h1, l1, c1 = df.iloc[i-2][['open', 'high', 'low', 'close']]\n",
        "        o2, h2, l2, c2 = df.iloc[i-1][['open', 'high', 'low', 'close']]\n",
        "        o3, h3, l3, c3 = df.iloc[i][['open', 'high', 'low', 'close']]\n",
        "\n",
        "        # Bullish Engulfing\n",
        "        if c2 < o2 and c3 > o3 and c3 > o2 and o3 < c2:\n",
        "            signals.append((df.index[i], 'Bullish Engulfing'))\n",
        "\n",
        "        # Bearish Engulfing\n",
        "        elif c2 > o2 and c3 < o3 and c3 < o2 and o3 > c2:\n",
        "            signals.append((df.index[i], 'Bearish Engulfing'))\n",
        "\n",
        "        # Hammer\n",
        "        elif (h3 - l3) > 3 * (o3 - c3) and (c3 - l3) / (h3 - l3) > 0.6:\n",
        "            signals.append((df.index[i], 'Hammer'))\n",
        "\n",
        "        # Hanging Man\n",
        "        elif (h3 - l3) > 3 * (c3 - o3) and (h3 - c3) / (h3 - l3) > 0.6:\n",
        "            signals.append((df.index[i], 'Hanging Man'))\n",
        "\n",
        "        # Doji\n",
        "        elif abs(c3 - o3) < 0.05 * (h3 - l3):\n",
        "            signals.append((df.index[i], 'Doji'))\n",
        "\n",
        "        # Morning Star\n",
        "        if c1 < o1 and abs(c2 - o2) < 0.1 * (h2 - l2) and c3 > o3 and c3 > (c1 + o1)/2:\n",
        "            signals.append((df.index[i], 'Morning Star'))\n",
        "\n",
        "        # Evening Star\n",
        "        if c1 > o1 and abs(c2 - o2) < 0.1 * (h2 - l2) and c3 < o3 and c3 < (c1 + o1)/2:\n",
        "            signals.append((df.index[i], 'Evening Star'))\n",
        "\n",
        "        # Piercing Line\n",
        "        if c2 < o2 and c3 > o3 and c3 > (o2 + c2)/2 and o3 < c2:\n",
        "            signals.append((df.index[i], 'Piercing Line'))\n",
        "\n",
        "        # Dark Cloud Cover\n",
        "        if c2 > o2 and c3 < o3 and c3 < (o2 + c2)/2 and o3 > c2:\n",
        "            signals.append((df.index[i], 'Dark Cloud Cover'))\n",
        "\n",
        "        # Three White Soldiers\n",
        "        if all(df.iloc[j]['close'] > df.iloc[j]['open'] for j in range(i-2, i+1)):\n",
        "            signals.append((df.index[i], 'Three White Soldiers'))\n",
        "\n",
        "    return signals\n",
        "\n",
        "# === 4. –û–±–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –ª–æ–≥–≤–∞–Ω–µ ===\n",
        "def run_module(file_path):\n",
        "    df = load_data(file_path)\n",
        "    pivots = calculate_pivots(df)\n",
        "    patterns = detect_patterns(df)\n",
        "\n",
        "    print(\"üìä Pivot Points:\")\n",
        "    print(pivots.tail())\n",
        "\n",
        "    print(\"\\nüïØÔ∏è Detected Patterns:\")\n",
        "    for time, pattern in patterns[-10:]:\n",
        "        print(f\"{time} ‚Üí {pattern}\")\n",
        "\n",
        "# === –ü—Ä–∏–º–µ—Ä –∑–∞ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ ===\n",
        "# run_module(\"EURUSD5.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcC_HZZfaFGE",
        "outputId": "c398c28d-4241-4b8a-82e9-1b93a8bcdbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing agent_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile agent_utils.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
        "import gymnasium as gym\n",
        "import glob\n",
        "\n",
        "# Define a simple callback to update the progress bar in Streamlit\n",
        "class ProgressCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that updates a Streamlit progress bar and status text\n",
        "    during the training process.\n",
        "    \"\"\"\n",
        "    def __init__(self, progress_bar=None, status_text=None, verbose=0):\n",
        "        super(ProgressCallback, self).__init__(verbose)\n",
        "        self.progress_bar = progress_bar\n",
        "        self.status_text = status_text\n",
        "        self.total_timesteps_in_episode = 0\n",
        "        self.current_timesteps_in_episode = 0\n",
        "        self._prev_timesteps = 0\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        if hasattr(self.training_env, 'get_attr') and isinstance(self.training_env.get_attr('df', indices=0)[0], pd.DataFrame) and hasattr(self.training_env.get_attr('lookback_window', indices=0)[0], '__int__'):\n",
        "             df_len = len(self.training_env.get_attr('df', indices=0)[0])\n",
        "             lookback_window = self.training_env.get_attr('lookback_window', indices=0)[0]\n",
        "             self.total_timesteps_in_episode = max(0, df_len - lookback_window)\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.progress_bar is not None:\n",
        "            if self.locals.get('total_timesteps') is not None and self.locals['total_timesteps'] > 0:\n",
        "                 progress_value = min(1.0, self.num_timesteps / self.locals['total_timesteps'])\n",
        "                 self.progress_bar.progress(progress_value)\n",
        "\n",
        "\n",
        "        if self.status_text is not None:\n",
        "             if self.locals.get('total_timesteps') is not None:\n",
        "                  self.status_text.text(f\"–û–±—É—á–µ–Ω–∏–µ –≤ –ø—Ä–æ–≥—Ä–µ—Å: {self.num_timesteps}/{self.locals['total_timesteps']} —Å—Ç—ä–ø–∫–∏\")\n",
        "             else:\n",
        "                  self.status_text.text(f\"–û–±—É—á–µ–Ω–∏–µ –≤ –ø—Ä–æ–≥—Ä–µ—Å: {self.num_timesteps} —Å—Ç—ä–ø–∫–∏\")\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "# Function to create an agent instance\n",
        "# @st.cache_resource\n",
        "def create_agent(agent_type, env, agent_params=None):\n",
        "    \"\"\"\n",
        "    Creates an instance of a Stable-Baselines3 RL agent.\n",
        "\n",
        "    Args:\n",
        "        agent_type (str): The type of agent to create ('PPO', 'A2C', 'DQN').\n",
        "        env (gym.Env): The environment to train the agent on (should be a VecEnv).\n",
        "        agent_params (dict, optional): Dictionary of agent-specific parameters. Defaults to None.\n",
        "                                       Expected format: {param1: value1, ...}.\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm: The created agent instance, or None if creation fails.\n",
        "    \"\"\"\n",
        "    st.write(f\"‚öôÔ∏è –û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç —Ç–∏–ø: {agent_type}\")\n",
        "    model = None\n",
        "    try:\n",
        "        if agent_type == \"PPO\":\n",
        "            ppo_defaults = {\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"n_steps\": 2048,\n",
        "                \"batch_size\": 64,\n",
        "                \"n_epochs\": 10,\n",
        "                \"gamma\": 0.99,\n",
        "                \"gae_lambda\": 0.95,\n",
        "                \"clip_range\": 0.2,\n",
        "                \"verbose\": 0\n",
        "            }\n",
        "            final_ppo_params = {**ppo_defaults, **(agent_params if agent_params is not None else {})}\n",
        "            model = PPO(\"MlpPolicy\", env, **final_ppo_params)\n",
        "            st.write(\"‚úÖ PPO –∞–≥–µ–Ω—Ç —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "        elif agent_type == \"DQN\":\n",
        "            dqn_defaults = {\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"buffer_size\": 10000,\n",
        "                \"learning_starts\": 100,\n",
        "                \"batch_size\": 32,\n",
        "                \"gamma\": 0.99,\n",
        "                \"train_freq\": 1,\n",
        "                \"gradient_steps\": 1,\n",
        "                \"verbose\": 0\n",
        "            }\n",
        "            final_dqn_params = {**dqn_defaults, **(agent_params if agent_params is not None else {})}\n",
        "            if not isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                 st.error(f\"üö´ DQN requires a Discrete action space, but the environment has {type(env.action_space)}. Cannot create DQN agent.\")\n",
        "                 return None\n",
        "\n",
        "            model = DQN(\"MlpPolicy\", env, **final_dqn_params)\n",
        "            st.write(\"‚úÖ DQN –∞–≥–µ–Ω—Ç —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "        elif agent_type == \"A2C\":\n",
        "            a2c_defaults = {\n",
        "                \"learning_rate\": 7e-4,\n",
        "                \"n_steps\": 5,\n",
        "                \"gamma\": 0.99,\n",
        "                \"gae_lambda\": 0.95,\n",
        "                \"vf_coef\": 0.25,\n",
        "                \"ent_coef\": 0.01,\n",
        "                \"verbose\": 0\n",
        "            }\n",
        "            final_a2c_params = {**a2c_defaults, **(agent_params if agent_params is not None else {})}\n",
        "            model = A2C(\"MlpPolicy\", env, **final_a2c_params)\n",
        "            st.write(\"‚úÖ A2C –∞–≥–µ–Ω—Ç —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "\n",
        "        else:\n",
        "            st.error(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç: {agent_type}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ {agent_type}: {e}\")\n",
        "        st.exception(e)\n",
        "        model = None\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to train the agent\n",
        "# @st.cache_resource\n",
        "def train_agent(agent, total_timesteps=10000, progress_bar=None, status_text=None, save_dir=\"/content/drive/MyDrive/Colab_Models\", checkpoint_dir=\"/content/drive/MyDrive/Colab_Checkpoints\", save_freq=5000):\n",
        "    \"\"\"\n",
        "    Trains the provided Stable-Baselines3 agent with checkpointing.\n",
        "\n",
        "    Args:\n",
        "        agent (stable_baselines3.common.base.BaseAlgorithm): The agent instance to train.\n",
        "        total_timesteps (int): The total number of timesteps to train for.\n",
        "        progress_bar (streamlit.delta_generator.DeltaGenerator, optional): Streamlit progress bar object. Defaults to None.\n",
        "        status_text (streamlit.delta_generator.DeltaGenerator, optional): Streamlit text object for status updates. Defaults to None.\n",
        "        save_dir (str): Directory to save the final model. Defaults to \"/content/drive/MyDrive/Colab_Models\".\n",
        "        checkpoint_dir (str): Directory to save training checkpoints. Defaults to \"/content/drive/MyDrive/Colab_Checkpoints\".\n",
        "        save_freq (int): Frequency (in timesteps) of saving checkpoints. Defaults to 5000.\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm: The trained agent instance, or None if training fails.\n",
        "    \"\"\"\n",
        "    agent_type = type(agent).__name__\n",
        "    model_name = f\"valkyrie_{agent_type}_model\"\n",
        "\n",
        "    st.write(f\"üß† –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∑–∞ {agent_type} –∞–≥–µ–Ω—Ç –∑–∞ {total_timesteps} —Å—Ç—ä–ø–∫–∏...\")\n",
        "    trained_model = None\n",
        "\n",
        "    if agent is None:\n",
        "        st.error(\"üö´ train_agent: –ü–æ–ª—É—á–µ–Ω –µ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω (None) –∞–≥–µ–Ω—Ç –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.\")\n",
        "        return None\n",
        "    if not hasattr(agent, 'env') or agent.env is None:\n",
        "         st.error(\"üö´ train_agent: –ê–≥–µ–Ω—Ç—ä—Ç –Ω–µ –µ —Å–≤—ä—Ä–∑–∞–Ω —Å –≤–∞–ª–∏–¥–Ω–∞ —Å—Ä–µ–¥–∞.\")\n",
        "         return None\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Create the ProgressCallback instance\n",
        "        progress_callback_instance = ProgressCallback(progress_bar=progress_bar, status_text=status_text)\n",
        "\n",
        "        # Create the CheckpointCallback instance\n",
        "        # Ensure checkpoint directory exists\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            st.warning(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ç–∞ –∑–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {checkpoint_dir}. –û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ...\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            st.write(f\"–°—ä–∑–¥–∞–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏: {checkpoint_dir}\")\n",
        "\n",
        "        checkpoint_callback_instance = CheckpointCallback(\n",
        "            save_freq=save_freq,\n",
        "            save_path=checkpoint_dir,\n",
        "            name_prefix=f\"{agent_type}_checkpoint\",\n",
        "            save_replay_buffer=True, # Save replay buffer for DQN\n",
        "            save_vecnormalize=True # Save VecNormalize for VecEnvs\n",
        "        )\n",
        "\n",
        "        # Combine callbacks\n",
        "        callbacks = [progress_callback_instance, checkpoint_callback_instance]\n",
        "\n",
        "        st.write(\"üöÄ –ó–∞–ø–æ—á–≤–∞ –æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "        agent.learn(total_timesteps=total_timesteps, callback=callbacks)\n",
        "        st.write(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ—Ç–æ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏–∫–ª—é—á–∏.\")\n",
        "\n",
        "        trained_model = agent\n",
        "\n",
        "        # Save the final trained model with the dynamically generated name\n",
        "        final_save_path = os.path.join(save_dir, model_name)\n",
        "        save_agent(trained_model, final_save_path)\n",
        "\n",
        "        st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ –æ–±—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ –∏ –∑–∞–ø–∞–∑–µ–Ω –∫–∞—Ç–æ: {final_save_path}.zip\")\n",
        "        return trained_model\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ: {e}\")\n",
        "        st.exception(e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to save the agent\n",
        "def save_agent(agent, path):\n",
        "    \"\"\"\n",
        "    Saves the trained Stable-Baselines3 agent to a file.\n",
        "\n",
        "    Args:\n",
        "        agent (stable_baselines3.common.base.BaseAlgorithm): The agent instance to save.\n",
        "        path (str): The path (including filename, without .zip) to save the agent in Google Drive.\n",
        "    \"\"\"\n",
        "    if agent is not None:\n",
        "        try:\n",
        "            save_dir = os.path.dirname(path)\n",
        "            if not os.path.exists(save_dir):\n",
        "                 st.warning(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ç–∞ –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {save_dir}. –û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ...\")\n",
        "                 os.makedirs(save_dir, exist_ok=True)\n",
        "                 st.write(f\"–°—ä–∑–¥–∞–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ: {save_dir}\")\n",
        "\n",
        "            agent.save(path)\n",
        "            st.write(f\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ –∑–∞–ø–∞–∑–µ–Ω —É—Å–ø–µ—à–Ω–æ –≤: {path}.zip\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –≤ {path}.zip: {e}\")\n",
        "            st.exception(e)\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è –ù—è–º–∞ –∞–≥–µ–Ω—Ç –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ.\")\n",
        "\n",
        "\n",
        "# Function to load the agent by type, looking for checkpoints first\n",
        "# @st.cache_resource\n",
        "def load_agent_by_type(path, env, agent_type, checkpoint_dir=\"/content/drive/MyDrive/Colab_Checkpoints\"):\n",
        "    \"\"\"\n",
        "    Loads a Stable-Baselines3 agent from a file based on its type.\n",
        "    Prioritizes loading from the latest checkpoint in checkpoint_dir if available,\n",
        "    otherwise loads from the initial model file at the specified path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The path to the initial saved agent file (.zip) or a directory containing checkpoints.\n",
        "                    If checkpoint_dir is provided, this path is used as a fallback if no checkpoints are found.\n",
        "        env (gym.Env): The environment compatible with the agent (should be a VecEnv).\n",
        "        agent_type (str): The type of agent to load ('PPO', 'A2C', 'DQN').\n",
        "        checkpoint_dir (str): Directory where training checkpoints are saved. Defaults to \"/content/drive/MyDrive/Colab_Checkpoints\".\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm: The loaded agent instance, or None if loading fails.\n",
        "    \"\"\"\n",
        "    st.write(f\"‚öôÔ∏è –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç —Ç–∏–ø {agent_type}...\")\n",
        "    model = None\n",
        "\n",
        "    # Try loading from the latest checkpoint first\n",
        "    if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "        checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f\"{agent_type}_checkpoint_*.zip\"))\n",
        "        if checkpoint_files:\n",
        "            # Find the latest checkpoint based on the timestep in the filename\n",
        "            latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
        "            st.info(f\"‚úÖ –ù–∞–º–µ—Ä–µ–Ω –ø–æ—Å–ª–µ–¥–µ–Ω —á–µ–∫–ø–æ–π–Ω—Ç: {latest_checkpoint}. –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç—Ç—É–∫.\")\n",
        "            try:\n",
        "                if agent_type == \"PPO\":\n",
        "                    model = PPO.load(latest_checkpoint, env=env)\n",
        "                elif agent_type == \"DQN\":\n",
        "                    if not isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                         st.error(f\"üö´ DQN requires a Discrete action space, but the environment has {type(env.action_space)}. Cannot load DQN agent from checkpoint.\")\n",
        "                         return None\n",
        "                    model = DQN.load(latest_checkpoint, env=env)\n",
        "                elif agent_type == \"A2C\":\n",
        "                    model = A2C.load(latest_checkpoint, env=env)\n",
        "                else:\n",
        "                    st.error(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç —Ç–∏–ø –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç: {agent_type}\")\n",
        "                    return None\n",
        "                st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç —Ç–∏–ø {agent_type} —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç.\")\n",
        "                return model\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç {latest_checkpoint}: {e}\")\n",
        "                st.exception(e)\n",
        "                # Fallback to loading the initial model if checkpoint loading fails\n",
        "\n",
        "\n",
        "    # If no checkpoints found or checkpoint loading failed, try loading the initial model\n",
        "    st.info(f\"‚ö†Ô∏è –ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –≤ {checkpoint_dir} –∏–ª–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ—Ç–æ —Å–µ –ø—Ä–æ–≤–∞–ª–∏. –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç: {path}\")\n",
        "    try:\n",
        "        # Construct the full path, ensuring we don't add .zip if it's already there\n",
        "        full_path = path\n",
        "        if not full_path.lower().endswith('.zip'):\n",
        "             full_path = f\"{path}.zip\"\n",
        "\n",
        "        if os.path.exists(full_path): # Check if the initial model file exists\n",
        "            if agent_type == \"PPO\":\n",
        "                model = PPO.load(full_path, env=env)\n",
        "                st.write(\"‚úÖ PPO –∞–≥–µ–Ω—Ç –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            elif agent_type == \"DQN\":\n",
        "                if not isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                     st.error(f\"üö´ DQN requires a Discrete action space, but the environment has {type(env.action_space)}. Cannot load DQN agent from initial path.\")\n",
        "                     return None\n",
        "                model = DQN.load(full_path, env=env)\n",
        "                st.write(\"‚úÖ DQN –∞–≥–µ–Ω—Ç –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            elif agent_type == \"A2C\":\n",
        "                model = A2C.load(full_path, env=env)\n",
        "                st.write(\"‚úÖ A2C –∞–≥–µ–Ω—Ç –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            else:\n",
        "                st.error(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç —Ç–∏–ø –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç: {agent_type}\")\n",
        "                return None\n",
        "            st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç —Ç–∏–ø {agent_type} —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            return model\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"‚ö†Ô∏è –ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω —Ñ–∞–π–ª –Ω–∞ –∞–≥–µ–Ω—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç: {full_path}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç {full_path}: {e}\")\n",
        "        st.exception(e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkhLYZGtrJEd",
        "outputId": "e7d25737-d0be-4064-bbed-3f234c7d3fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing forex_dashboard.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile forex_dashboard.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import plotly.graph_objects as go # Import Plotly for interactive charts\n",
        "from plotly.subplots import make_subplots # For multi-panel charts\n",
        "import json # Import json for saving/loading config\n",
        "\n",
        "# –ò–º–ø–æ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –º–æ–¥—É–ª–∏—Ç–µ –Ω–∞ —è–¥—Ä–æ—Ç–æ\n",
        "from data_utils import download_forex_data, add_technical_indicators, split_data\n",
        "# Corrected import path and class name: Import ForexTradingEnv from forex_env_utils\n",
        "# Corrected import path\n",
        "from env.forex_env import ForexTradingEnv, calculate_metrics # Assuming calculate_metrics is in forex_env.py\n",
        "from agent_utils import create_agent, train_agent, save_agent, load_agent_by_type\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import glob # Import glob to find latest checkpoint\n",
        "import yfinance as yf  # üì• –ò–º–ø–æ—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Yahoo Finance API\n",
        "\n",
        "\n",
        "# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞ Streamlit —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞ ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"ValkyrieFX Dashboard\", page_icon=\"üìä\")\n",
        "\n",
        "st.title(\"ValkyrieFX Trading Dashboard\")\n",
        "\n",
        "# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Å—ä—Å—Ç–æ—è–Ω–∏–µ—Ç–æ –Ω–∞ —Å–µ—Å–∏—è—Ç–∞ (Session State) ---\n",
        "# Ensure all necessary keys are initialized\n",
        "if 'processed_data' not in st.session_state:\n",
        "    st.session_state['processed_data'] = None\n",
        "if 'raw_uploaded_data' not in st.session_state:\n",
        "    st.session_state['raw_uploaded_data'] = None\n",
        "if 'train_data' not in st.session_state:\n",
        "    st.session_state['train_data'] = None\n",
        "if 'test_data' not in st.session_state:\n",
        "    st.session_state['test_data'] = None\n",
        "if 'trained_agent' not in st.session_state:\n",
        "    st.session_state['trained_agent'] = None\n",
        "if 'trained_agent_name' not in st.session_state:\n",
        "     st.session_state['trained_agent_name'] = None\n",
        "if 'loaded_agent' not in st.session_state:\n",
        "    st.session_state['loaded_agent'] = None\n",
        "if 'loaded_agent_name' not in st.session_state:\n",
        "     st.session_state['loaded_agent_name'] = None\n",
        "if 'backtesting_results' not in st.session_state:\n",
        "    st.session_state['backtesting_results'] = None\n",
        "if 'performance_metrics' not in st.session_state:\n",
        "    st.session_state['performance_metrics'] = None\n",
        "if 'trades_log' not in st.session_state:\n",
        "    st.session_state['trades_log'] = None\n",
        "if 'env_config' not in st.session_state:\n",
        "     st.session_state['env_config'] = {}\n",
        "if 'agent_config' not in st.session_state:\n",
        "     st.session_state['agent_config'] = {}\n",
        "if 'last_checkpoint_path' not in st.session_state: # To store the path of the last loaded checkpoint\n",
        "     st.session_state['last_checkpoint_path'] = None\n",
        "if 'data_timeframe' not in st.session_state: # Store the selected timeframe\n",
        "     st.session_state['data_timeframe'] = \"1d\"\n",
        "\n",
        "\n",
        "# --- Sidebar Configuration ---\n",
        "st.sidebar.header(\"‚öôÔ∏è –ì–ª–æ–±–∞–ª–Ω–∏ –ù–∞—Å—Ç—Ä–æ–π–∫–∏\")\n",
        "\n",
        "# Data Loading Section (Moved to sidebar as it affects all tabs)\n",
        "st.sidebar.subheader(\"üìà –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –¥–∞–Ω–Ω–∏\")\n",
        "\n",
        "# Add a radio button to choose data source\n",
        "data_source = st.sidebar.radio(\n",
        "    \"–ò–∑–±–µ—Ä–∏ –∏–∑—Ç–æ—á–Ω–∏–∫ –Ω–∞ –¥–∞–Ω–Ω–∏\",\n",
        "    (\"–ò–∑—Ç–µ–≥–ª–∏ –æ—Ç Yahoo Finance\", \"–ö–∞—á–∏ –æ—Ç CSV\"),\n",
        "    key='data_source_radio_sidebar'\n",
        ")\n",
        "\n",
        "uploaded_file = st.sidebar.file_uploader(\"üìÇ –ö–∞—á–∏ –¥–∞–Ω–Ω–∏ –æ—Ç CSV —Ñ–∞–π–ª\", type=[\"csv\"], key='upload_csv_file_sidebar') # Moved file uploader\n",
        "\n",
        "raw_data_to_process = None # Initialize variable to hold raw data\n",
        "\n",
        "# Global timeframe selector\n",
        "# This will be visible regardless of the data source, but its value might be overridden by Yahoo Finance download\n",
        "if data_source in [\"–ò–∑—Ç–µ–≥–ª–∏ –æ—Ç Yahoo Finance\", \"–ö–∞—á–∏ –æ—Ç CSV\"]:\n",
        "     st.session_state['data_timeframe'] = st.sidebar.selectbox(\"‚è±Ô∏è –¢–∞–π–º—Ñ—Ä–µ–π–º\", [\"1d\", \"1h\", \"30m\", \"15m\", \"5m\", \"1m\"], key=\"global_timeframe_selector\")\n",
        "\n",
        "\n",
        "# Display Yahoo Finance controls only if selected\n",
        "if data_source == \"–ò–∑—Ç–µ–≥–ª–∏ –æ—Ç Yahoo Finance\":\n",
        "    data_ticker = st.sidebar.text_input(\"–¢–∏–∫–µ—Ä (–Ω–∞–ø—Ä. EURUSD=X)\", value=\"EURUSD=X\", key='data_ticker_input_sidebar') # Changed key name\n",
        "    data_start_date = st.sidebar.date_input(\"–ù–∞—á–∞–ª–Ω–∞ –¥–∞—Ç–∞\", value=pd.to_datetime(\"2015-01-01\"), key='data_start_date_input_sidebar') # Changed key name\n",
        "    data_end_date = st.sidebar.date_input(\"–ö—Ä–∞–π–Ω–∞ –¥–∞—Ç–∞\", value=pd.to_datetime(\"2023-12-31\"), key='data_end_date_input_sidebar') # Changed key name\n",
        "    # Removed the timeframe selectbox from here, using the global one\n",
        "\n",
        "    if st.sidebar.button(\"‚¨áÔ∏è –ò–∑—Ç–µ–≥–ª–∏ –∏ –û–±—Ä–∞–±–æ—Ç–∏ –î–∞–Ω–Ω–∏\", key='download_and_process_button_sidebar'): # Changed key name\n",
        "         with st.spinner(f\"–ò–∑—Ç–µ–≥–ª—è–Ω–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –¥–∞–Ω–Ω–∏ –∑–∞ {data_ticker} ({st.session_state['data_timeframe']})...\"):\n",
        "             # Ensure dates are in YYYY-MM-DD format string for yfinance\n",
        "             start_date_str = data_start_date.strftime(\"%Y-%m-%d\")\n",
        "             end_date_str = data_end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "             # Use the value from the global timeframe selector\n",
        "             raw_data = download_forex_data(data_ticker, start_date_str, end_date_str, st.session_state['data_timeframe']) # Use raw_data\n",
        "             if raw_data is not None and not raw_data.empty:\n",
        "                 # Move processing steps inside this if block\n",
        "                 with st.spinner(\"–î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏...\"):\n",
        "                     # Pass atr_window from sidebar to add_technical_indicators\n",
        "                     processed_data = add_technical_indicators(raw_data, atr_window=st.session_state.get('env_atr_window_sidebar', 14)) # Use raw_data\n",
        "\n",
        "                 if processed_data is not None and not processed_data.empty:\n",
        "                      split_ratio = st.sidebar.slider(\"–°—ä–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏/—Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏ (%)\", min_value=50, max_value=90, value=80, step=5, key='data_split_ratio_slider_sidebar') / 100.0 # Changed key name\n",
        "                      with st.spinner(\"–†–∞–∑–¥–µ–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏...\"):\n",
        "                           train_data, test_data = split_data(processed_data, split_ratio)\n",
        "\n",
        "                      st.session_state['processed_data'] = processed_data\n",
        "                      st.session_state['train_data'] = train_data\n",
        "                      st.session_state['test_data'] = test_data\n",
        "                      st.sidebar.success(\"‚úÖ –î–∞–Ω–Ω–∏—Ç–µ —Å–∞ –∏–∑—Ç–µ–≥–ª–µ–Ω–∏, –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ.\")\n",
        "                 else:\n",
        "                      st.session_state['processed_data'] = None\n",
        "                      st.session_state['train_data'] = None\n",
        "                      st.session_state['test_data'] = None\n",
        "                      st.sidebar.error(\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∏–ª–∏ –¥–∞–Ω–Ω–∏—Ç–µ —Å–∞ –ø—Ä–∞–∑–Ω–∏ —Å–ª–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∞.\")\n",
        "             else:\n",
        "                 st.session_state['processed_data'] = None\n",
        "                 st.session_state['train_data'] = None\n",
        "                 st.session_state['test_data'] = None\n",
        "                 st.sidebar.error(\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑—Ç–µ–≥–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏. –ú–æ–ª—è, –ø—Ä–æ–≤–µ—Ä–µ—Ç–µ —Ç–∏–∫–µ—Ä–∞, –¥–∞—Ç–∏—Ç–µ –∏–ª–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –≤—Ä—ä–∑–∫–∞—Ç–∞.\")\n",
        "\n",
        "\n",
        "elif data_source == \"–ö–∞—á–∏ –æ—Ç CSV\":\n",
        "    # Use the uploaded_file from the file_uploader above\n",
        "    if uploaded_file is not None:\n",
        "        if st.sidebar.button(\"üõ†Ô∏è –û–±—Ä–∞–±–æ—Ç–∏ –ö–∞—á–µ–Ω–∏ –î–∞–Ω–Ω–∏\", key='process_uploaded_button_sidebar'): # Changed key name\n",
        "            # Read the CSV file into a pandas DataFrame\n",
        "            try:\n",
        "                raw_data = pd.read_csv(uploaded_file) # Use raw_data\n",
        "                st.session_state['raw_uploaded_data'] = raw_data.copy() # Store raw uploaded data in session state\n",
        "                st.sidebar.success(f\"‚úÖ –§–∞–π–ª—ä—Ç `{uploaded_file.name}` –µ –∑–∞—Ä–µ–¥–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞.\")\n",
        "\n",
        "                # Continue processing if raw_data is not None\n",
        "                if raw_data is not None and not raw_data.empty:\n",
        "                    with st.spinner(\"–î–æ–±–∞–≤—è–Ω–µ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏...\"):\n",
        "                         # Pass atr_window from sidebar to add_technical_indicators\n",
        "                        processed_data = add_technical_indicators(raw_data, atr_window=st.session_state.get('env_atr_window_sidebar', 14)) # Use raw_data\n",
        "\n",
        "                    if processed_data is not None and not processed_data.empty:\n",
        "                         split_ratio = st.sidebar.slider(\"–°—ä–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏/—Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏ (%)\", min_value=50, max_value=90, value=80, step=5, key='data_split_ratio_slider_sidebar') / 100.0 # Changed key name\n",
        "                         with st.spinner(\"–†–∞–∑–¥–µ–ª—è–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏...\"):\n",
        "                              train_data, test_data = split_data(processed_data, split_ratio)\n",
        "\n",
        "                         st.session_state['processed_data'] = processed_data\n",
        "                         st.session_state['train_data'] = train_data\n",
        "                         st.session_state['test_data'] = test_data\n",
        "                         st.sidebar.success(\"‚úÖ –î–∞–Ω–Ω–∏—Ç–µ —Å–∞ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏ —É—Å–ø–µ—à–Ω–æ.\")\n",
        "                    else:\n",
        "                         st.session_state['processed_data'] = None\n",
        "                         st.session_state['train_data'] = None\n",
        "                         st.session_state['test_data'] = None\n",
        "                         st.sidebar.error(\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∏–ª–∏ –¥–∞–Ω–Ω–∏—Ç–µ —Å–∞ –ø—Ä–∞–∑–Ω–∏ —Å–ª–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∞.\")\n",
        "                else:\n",
        "                     st.session_state['processed_data'] = None\n",
        "                     st.session_state['train_data'] = None\n",
        "                     st.session_state['test_data'] = None\n",
        "                     st.sidebar.error(\"üö´ –ö–∞—á–µ–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏ —Å–∞ –ø—Ä–∞–∑–Ω–∏ —Å–ª–µ–¥ —á–µ—Ç–µ–Ω–µ.\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                st.session_state['raw_uploaded_data'] = None\n",
        "                st.sidebar.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —á–µ—Ç–µ–Ω–µ –Ω–∞ CSV —Ñ–∞–π–ª–∞: {e}\")\n",
        "                raw_data = None # Ensure raw_data is None on error\n",
        "\n",
        "\n",
        "# Display loaded/processed data info\n",
        "if st.session_state['processed_data'] is not None:\n",
        "    st.sidebar.subheader(\"–°—Ç–∞—Ç—É—Å –Ω–∞ –¥–∞–Ω–Ω–∏—Ç–µ\")\n",
        "    st.sidebar.write(f\"–û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –¥–∞–Ω–Ω–∏: {len(st.session_state['processed_data'])} —Ä–µ–¥–∞\")\n",
        "    if st.session_state['train_data'] is not None:\n",
        "         st.sidebar.write(f\"–î–∞–Ω–Ω–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ: {len(st.session_state['train_data'])} —Ä–µ–¥–∞\")\n",
        "    if st.session_state['test_data'] is not None:\n",
        "         st.sidebar.write(f\"–¢–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏: {len(st.session_state['test_data'])} —Ä–µ–¥–∞\")\n",
        "else:\n",
        "    # This block will execute if no data was downloaded or uploaded/processed successfully\n",
        "    pass\n",
        "\n",
        "\n",
        "# Environment Parameters (Moved to sidebar as they are used in multiple tabs)\n",
        "st.sidebar.header(\"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞\")\n",
        "initial_amount = st.sidebar.number_input(\n",
        "    \"–ù–∞—á–∞–ª–µ–Ω –∫–∞–ø–∏—Ç–∞–ª ($)\", min_value=1000.0, max_value=10000000.0, value=100000.0, step=1000.0, format=\"%.2f\", key='env_initial_amount_sidebar'\n",
        ")\n",
        "lookback_window = st.sidebar.number_input(\n",
        "    \"Lookback Window\", min_value=1, max_value=200, value=20, step=1, key='env_lookback_window_sidebar'\n",
        ")\n",
        "\n",
        "st.sidebar.markdown(\"##### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ —Ä–∏—Å–∫ –∏ –ø–æ–∑–∏—Ü–∏—è\")\n",
        "stop_loss_pct = st.sidebar.number_input(\n",
        "    \"–°—Ç–æ–ø-–ª–æ—Å (%)\", min_value=0.0, max_value=100.0, value=2.0, step=0.1, format=\"%.2f\", key='env_stop_loss_pct_sidebar'\n",
        ") / 100.0\n",
        "take_profit_pct = st.sidebar.number_input(\n",
        "    \"–¢–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç (%)\", min_value=0.0, max_value=100.0, value=4.0, step=0.1, format=\"%.2f\", key='env_take_profit_pct_sidebar'\n",
        ") / 100.0\n",
        "max_drawdown_limit_pct = st.sidebar.number_input(\n",
        "    \"–ú–∞–∫—Å. –¥–æ–ø—É—Å—Ç–∏–º —Å–ø–∞–¥ –Ω–∞ –ø–æ—Ä—Ç—Ñ–µ–π–ª–∞ (%)\", min_value=0.01, max_value=100.0, value=10.0, step=0.1, format=\"%.2f\", key='env_max_drawdown_limit_pct_sidebar'\n",
        ") / 100.0\n",
        "\n",
        "st.sidebar.markdown(\"##### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ç–∞\")\n",
        "lot_model = st.sidebar.selectbox(\"–ú–æ–¥–µ–ª –∑–∞ —Ä–∞–∑–º–µ—Ä –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ç–∞\", [\"percent_of_capital\", \"volatility\"], key='env_lot_model_select_sidebar') # Changed key name\n",
        "position_size_pct = st.sidebar.number_input(\n",
        "    \"–†–∞–∑–º–µ—Ä –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ç–∞ (% –æ—Ç –∫–∞–ø–∏—Ç–∞–ª–∞)\", min_value=0.01, max_value=100.0, value=10.0, step=0.1, format=\"%.2f\", key='env_position_size_pct_sidebar'\n",
        ") / 100.0\n",
        "# ATR window is used in data_utils, not passed to env constructor directly\n",
        "# but its window size might be relevant for required data length.\n",
        "# Let's define it here as a parameter for consistency with environment logic,\n",
        "# even if it's primarily used in data_utils.\n",
        "# Ensure atr_window has a default value even if volatility is not selected\n",
        "atr_window = st.sidebar.number_input(\n",
        "    \"ATR Window (–∑–∞ Volatility Model)\", min_value=1, max_value=50, value=14, step=1, key='env_atr_window_sidebar', disabled=(lot_model != \"volatility\") # Changed key name\n",
        ")\n",
        "\n",
        "\n",
        "st.sidebar.markdown(\"##### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ —Ç–∞–∫—Å–∏\")\n",
        "buy_cost_pct = st.sidebar.number_input(\n",
        "    \"–¢–∞–∫—Å–∞ –ø—Ä–∏ –ø–æ–∫—É–ø–∫–∞ (%)\", min_value=0.0, max_value=1.0, value=0.1, step=0.001, format=\"%.3f\", key='env_buy_cost_pct_sidebar'\n",
        ") / 100.0\n",
        "sell_cost_pct = st.sidebar.number_input(\n",
        "    \"–¢–∞–∫—Å–∞ –ø—Ä–∏ –ø—Ä–æ–¥–∞–∂–±–∞ (%)\", min_value=0.0, max_value=1.0, value=0.1, step=0.001, format=\"%.3f\", key='env_sell_cost_pct_sidebar'\n",
        ") / 100.0\n",
        "\n",
        "st.sidebar.markdown(\"##### –ù–∞–≥—Ä–∞–¥–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è\")\n",
        "# –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏ –∑–∞ Reward Shaping –ø–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
        "tp_reward_bonus_pct = st.sidebar.number_input(\n",
        "    \"TP –ë–æ–Ω—É—Å (% –æ—Ç –Ω–∞—á–∞–ª–µ–Ω –∫–∞–ø–∏—Ç–∞–ª)\", min_value=0.0, max_value=100.0, value=1.0, step=0.1, format=\"%.2f\", key='env_tp_bonus_sidebar' # Changed key name\n",
        ") / 100.0\n",
        "sl_penalty_pct = st.sidebar.number_input(\n",
        "    \"SL –ù–∞–∫–∞–∑–∞–Ω–∏–µ (% –æ—Ç –Ω–∞—á–∞–ª–µ–Ω –∫–∞–ø–∏—Ç–∞–ª)\", min_value=0.0, max_value=100.0, value=1.0, step=0.1, format=\"%.2f\", key='env_sl_penalty_sidebar' # Changed key name\n",
        ") / 100.0\n",
        "# –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª –∑–∞ Trailing SL\n",
        "trailing_sl_pct = st.sidebar.number_input(\n",
        "    \"–¢—Ä–µ–π–ª–∏–Ω–≥ –°—Ç–æ–ø –õ–æ—Å (%)\", min_value=0.0, max_value=10.0, value=0.5, step=0.05, format=\"%.2f\", key='env_trailing_sl_pct_sidebar' # Changed key name\n",
        ") / 100.0\n",
        "\n",
        "# --- Agent Parameters (Moved to sidebar) ---\n",
        "st.sidebar.header(\"üß† –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –∞–≥–µ–Ω—Ç\")\n",
        "agent_type = st.sidebar.selectbox(\"–ò–∑–±–µ—Ä–∏ RL –∞–ª–≥–æ—Ä–∏—Ç—ä–º\", [\"PPO\", \"A2C\", \"DQN\"], key='agent_algo_select_sidebar') # Changed key name\n",
        "total_timesteps = st.sidebar.number_input(\"–û–±—â–æ —Å—Ç—ä–ø–∫–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ\", min_value=1000, max_value=1000000, value=50000, step=1000, key='agent_total_timesteps_input_sidebar') # Changed key name\n",
        "\n",
        "# Add PPO specific parameters in the sidebar, visible only if PPO is selected\n",
        "if agent_type == \"PPO\":\n",
        "    st.sidebar.subheader(\"PPO –ü–∞—Ä–∞–º–µ—Ç—Ä–∏\")\n",
        "    ppo_lr = st.sidebar.number_input(\"Learning Rate\", min_value=1e-7, max_value=1e-2, value=1e-4, format=\"%.7f\", key='ppo_lr_sidebar')\n",
        "    ppo_gamma = st.sidebar.number_input(\"Gamma\", min_value=0.0, max_value=1.0, value=0.99, step=0.01, format=\"%.2f\", key='ppo_gamma_sidebar')\n",
        "    ppo_n_steps = st.sidebar.number_input(\"N Steps\", min_value=1, max_value=8192, value=2048, step=1, key='ppo_n_steps_sidebar')\n",
        "    ppo_batch_size = st.sidebar.number_input(\"Batch Size\", min_value=1, max_value=512, value=64, step=1, key='ppo_batch_size_sidebar')\n",
        "    ppo_n_epochs = st.sidebar.number_input(\"N Epochs\", min_value=1, max_value=20, value=10, step=1, key='ppo_n_epochs_sidebar')\n",
        "    ppo_clip_range = st.sidebar.number_input(\"Clip Range\", min_value=0.0, max_value=1.0, value=0.2, step=0.01, format=\"%.2f\", key='ppo_clip_range_sidebar')\n",
        "    ppo_gae_lambda = st.sidebar.number_input(\"GAE Lambda\", min_value=0.0, max_value=1.0, value=0.95, step=0.01, format=\"%.2f\", key='ppo_gae_lambda_sidebar')\n",
        "\n",
        "# TODO: Add similar sections for A2C and DQN parameters if needed\n",
        "\n",
        "\n",
        "# Checkpointing Parameters (Moved to sidebar)\n",
        "st.sidebar.subheader(\"üíæ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏–Ω–≥\")\n",
        "checkpoint_dir = st.sidebar.text_input(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ –ß–µ–∫–ø–æ–π–Ω—Ç–∏–Ω–≥\", value=\"/content/drive/MyDrive/Colab_Checkpoints\", key='checkpoint_dir_input_sidebar') # Changed key name\n",
        "checkpoint_freq = st.sidebar.number_input(\"–ß–µ—Å—Ç–æ—Ç–∞ –Ω–∞ –ß–µ–∫–ø–æ–π–Ω—Ç–∏–Ω–≥ (—Å—Ç—ä–ø–∫–∏)\", min_value=1000, value=5000, step=1000, key='checkpoint_freq_input_sidebar') # Changed key name\n",
        "\n",
        "# Agent Save/Load Section (Moved to sidebar)\n",
        "st.sidebar.subheader(\"üì¶ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –ú–æ–¥–µ–ª–∏\")\n",
        "save_model_dir = st.sidebar.text_input(\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ –ó–∞–ø–∞–∑–µ–Ω–∏ –ê–≥–µ–Ω—Ç–∏\", value=\"/content/drive/MyDrive/Colab_Models\", key='save_model_dir_input_sidebar') # Changed key name\n",
        "\n",
        "# --- Check Google Drive Access (Global Check) ---\n",
        "if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    st.error(\"üö´ –ù–Ø–ú–ê –î–û–°–¢–™–ü –î–û GOOGLE DRIVE! –ú–æ–ª—è, —Å–≤—ä—Ä–∂–µ—Ç–µ Google Drive, –∑–∞ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏—Ç–µ –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ/–∑–∞—Ä–µ–∂–¥–∞–Ω–µ.\")\n",
        "    # Optionally disable relevant controls\n",
        "    drive_access_available = False\n",
        "else:\n",
        "    drive_access_available = True\n",
        "\n",
        "    # Ensure base save and checkpoint directories exist\n",
        "    os.makedirs(save_model_dir, exist_ok=True)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Main Content Area ---\n",
        "tab1, tab2, tab3 = st.tabs([\"–°—ä–∑–¥–∞–π –∏ –û–±—É—á–∏ –ù–æ–≤ –ê–≥–µ–Ω—Ç\", \"–ü—Ä–æ–¥—ä–ª–∂–∏ –û–±—É—á–µ–Ω–∏–µ –æ—Ç –ß–µ–∫–ø–æ–π–Ω—Ç\", \"–ë–µ–∫—Ç–µ—Å—Ç –∏ –ê–Ω–∞–ª–∏–∑\"])\n",
        "\n",
        "with tab1:\n",
        "    st.header(\"–°—ä–∑–¥–∞–π –∏ –û–±—É—á–∏ –ù–æ–≤ –ê–≥–µ–Ω—Ç\")\n",
        "    st.markdown(\"\"\"\n",
        "        –í —Ç–æ–∑–∏ —Ä–∞–∑–¥–µ–ª –º–æ–∂–µ—Ç–µ –¥–∞ —Å—ä–∑–¥–∞–¥–µ—Ç–µ –∏–∑—Ü—è–ª–æ –Ω–æ–≤ –∞–≥–µ–Ω—Ç —Å —Ç–µ–∫—É—â–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞ –∏ –∞–≥–µ–Ω—Ç–∞\n",
        "        –∏ –¥–∞ —Å—Ç–∞—Ä—Ç–∏—Ä–∞—Ç–µ –Ω–æ–≤–æ –æ–±—É—á–µ–Ω–∏–µ. **–í–Ω–∏–º–∞–Ω–∏–µ:** –¢–æ–≤–∞ —â–µ –∏–≥–Ω–æ—Ä–∏—Ä–∞ –≤—Å–∏—á–∫–∏ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏\n",
        "        –∏ –ø—Ä–µ–¥–∏—à–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞ —Ç–æ–∑–∏ —Ç–∏–ø –∞–≥–µ–Ω—Ç.\n",
        "    \"\"\")\n",
        "\n",
        "    # Button to Create and Train a NEW Agent\n",
        "    if st.button(\"üöÄ –°—ä–∑–¥–∞–π –∏ –û–±—É—á–∏ –ù–æ–≤ –ê–≥–µ–Ω—Ç\", key='create_and_train_new_agent_button'):\n",
        "        if not drive_access_available:\n",
        "             st.error(\"üö´ –ù–µ–æ–±—Ö–æ–¥–∏–º –µ –¥–æ—Å—Ç—ä–ø –¥–æ Google Drive –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∏ –∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏.\")\n",
        "        elif st.session_state.get(\"train_data\") is None or st.session_state[\"train_data\"].empty:\n",
        "            st.warning(\"‚ö†Ô∏è –ú–æ–ª—è, –∑–∞—Ä–µ–¥–µ—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–µ—Ç–µ –¥–∞–Ω–Ω–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ –ø—ä—Ä–≤–æ.\")\n",
        "        else:\n",
        "            st.info(f\"‚è≥ –°—ä–∑–¥–∞–≤–∞–Ω–µ –∏ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–æ–≤–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞ {agent_type} –∞–≥–µ–Ω—Ç...\")\n",
        "\n",
        "            # –°—ä–±–∏—Ä–∞–Ω–µ –Ω–∞ –≤—Å–∏—á–∫–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞ –æ—Ç UI\n",
        "            current_env_params = {\n",
        "                'initial_amount': initial_amount,\n",
        "                'lookback_window': lookback_window,\n",
        "                'buy_cost_pct': buy_cost_pct,\n",
        "                'sell_cost_pct': sell_cost_pct,\n",
        "                'max_drawdown_limit_pct': max_drawdown_limit_pct,\n",
        "                'position_size_pct': position_size_pct,\n",
        "                'stop_loss_pct': stop_loss_pct,\n",
        "                'take_profit_pct': take_profit_pct,\n",
        "                'trailing_sl_pct': trailing_sl_pct,\n",
        "                'lot_model': lot_model,\n",
        "                # atr_window is used in data_utils, not passed to env constructor\n",
        "                'tp_reward_bonus': tp_reward_bonus_pct,\n",
        "                'sl_penalty': sl_penalty_pct\n",
        "            }\n",
        "            st.session_state['env_config'] = current_env_params # Save env config to state\n",
        "\n",
        "            # –°—ä–±–∏—Ä–∞–Ω–µ –Ω–∞ –≤—Å–∏—á–∫–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç UI –∑–∞ –∏–∑–±—Ä–∞–Ω–∏—è —Ç–∏–ø\n",
        "            current_agent_params = {}\n",
        "            if agent_type == \"PPO\":\n",
        "                 current_agent_params = {\n",
        "                     \"learning_rate\": ppo_lr,\n",
        "                     \"gamma\": ppo_gamma,\n",
        "                     \"n_steps\": ppo_n_steps,\n",
        "                     \"batch_size\": ppo_batch_size,\n",
        "                     \"n_epochs\": ppo_n_epochs,\n",
        "                     \"clip_range\": ppo_clip_range,\n",
        "                     \"gae_lambda\": ppo_gae_lambda,\n",
        "                     \"verbose\": 0 # Ensure verbose is set to 0 in params\n",
        "                 }\n",
        "            # TODO: –î–æ–±–∞–≤–µ—Ç–µ —Å—ä–±–∏—Ä–∞–Ω–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∏ –∑–∞ A2C, DQN –∞–∫–æ –∏–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏ –∑–∞ —Ç—è—Ö\n",
        "            st.session_state['agent_config'] = current_agent_params # Save agent config to state\n",
        "\n",
        "\n",
        "            try:\n",
        "                # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∞ —Å—Ä–µ–¥–∞\n",
        "                train_data_for_env = st.session_state.get('train_data')\n",
        "                if train_data_for_env is None or train_data_for_env.empty:\n",
        "                     st.error(\"üö´ –í—ä—Ç—Ä–µ—à–Ω–∞ –≥—Ä–µ—à–∫–∞: –õ–∏–ø—Å–≤–∞—Ç –¥–∞–Ω–Ω–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ —Å–ª–µ–¥ –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞.\")\n",
        "                     st.stop()\n",
        "\n",
        "                # Corrected class name\n",
        "                train_env_instance = ForexTradingEnv(df=train_data_for_env.copy(), **current_env_params)\n",
        "                vec_train_env = DummyVecEnv([lambda: train_env_instance])\n",
        "\n",
        "                agent = None # Initialize agent to None\n",
        "                # Add try-except around agent creation\n",
        "                try:\n",
        "                    st.write(\"–û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞...\")\n",
        "                    agent = create_agent(agent_type, vec_train_env, agent_params=current_agent_params)\n",
        "\n",
        "                    if agent is not None:\n",
        "                         st.write(\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ —É—Å–ø–µ—à–Ω–æ —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "                    else:\n",
        "                         st.error(\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞: create_agent –≤—ä—Ä–Ω–∞ None.\")\n",
        "                         st.stop() # Stop execution if agent creation fails\n",
        "\n",
        "                except Exception as e:\n",
        "                     st.error(f\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞: {e}\")\n",
        "                     st.exception(e)\n",
        "                     agent = None\n",
        "                     st.stop() # Stop execution on creation error\n",
        "\n",
        "\n",
        "                if agent:\n",
        "                    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞\n",
        "                    progress_bar = st.progress(0)\n",
        "                    status_text = st.empty()\n",
        "\n",
        "                    # Pass progress_bar and status_text to train_agent\n",
        "                    trained_agent = train_agent(\n",
        "                        agent,\n",
        "                        total_timesteps=total_timesteps,\n",
        "                        progress_bar=progress_bar,\n",
        "                        status_text=status_text,\n",
        "                        save_dir=save_model_dir, # Use sidebar save dir\n",
        "                        checkpoint_dir=checkpoint_dir, # Use sidebar checkpoint dir\n",
        "                        save_freq=checkpoint_freq # Use sidebar checkpoint freq\n",
        "                    )\n",
        "\n",
        "                    progress_bar.empty()\n",
        "                    status_text.empty()\n",
        "\n",
        "                    if trained_agent:\n",
        "                        st.session_state['trained_agent'] = trained_agent\n",
        "                        st.session_state['trained_agent_name'] = agent_type # –ó–∞–ø–∞–∑–≤–∞–º–µ –∏–º–µ—Ç–æ –Ω–∞ –æ–±—É—á–µ–Ω –∞–≥–µ–Ω—Ç\n",
        "\n",
        "                        # Save config files\n",
        "                        agent_save_name = f\"valkyrie_{agent_type.lower()}_model\"\n",
        "                        config_save_path_base = os.path.join(save_model_dir, agent_save_name)\n",
        "                        with open(f\"{config_save_path_base}_env_config.json\", 'w') as f:\n",
        "                            json.dump(current_env_params, f, indent=4)\n",
        "                        with open(f\"{config_save_path_base}_agent_config.json\", 'w') as f:\n",
        "                            json.dump(current_agent_params, f, indent=4)\n",
        "\n",
        "                        st.success(f\"üéâ –ù–æ–≤–æ—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {agent_type} –ø—Ä–∏–∫–ª—é—á–∏ —É—Å–ø–µ—à–Ω–æ!\")\n",
        "                        st.info(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏—Ç–µ —Å–∞ –∑–∞–ø–∞–∑–µ–Ω–∏ –∫–∞—Ç–æ `{agent_save_name}_env_config.json` –∏ `{agent_save_name}_agent_config.json` –≤ `{save_model_dir}`.\")\n",
        "\n",
        "                    else:\n",
        "                        st.error(\"‚ùå –ù–æ–≤–æ—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –±–µ—à–µ –Ω–µ—É—Å–ø–µ—à–Ω–æ.\")\n",
        "                        st.session_state['trained_agent'] = None\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                # This outer try-except catches errors during environment creation or initial data checks\n",
        "                st.error(f\"üö´ –í—ä–∑–Ω–∏–∫–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–µ–¥–∏ –∏–ª–∏ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –∏ –æ–±—É—á–µ–Ω–∏–µ: {e}\")\n",
        "                st.exception(e)\n",
        "\n",
        "\n",
        "with tab2:\n",
        "    st.header(\"–ü—Ä–æ–¥—ä–ª–∂–∏ –û–±—É—á–µ–Ω–∏–µ –æ—Ç –ß–µ–∫–ø–æ–π–Ω—Ç\")\n",
        "    st.markdown(\"\"\"\n",
        "        –í —Ç–æ–∑–∏ —Ä–∞–∑–¥–µ–ª –º–æ–∂–µ—Ç–µ –¥–∞ –∑–∞—Ä–µ–¥–∏—Ç–µ –ø–æ—Å–ª–µ–¥–Ω–∏—è –Ω–∞–ª–∏—á–µ–Ω —á–µ–∫–ø–æ–π–Ω—Ç –∑–∞ –∏–∑–±—Ä–∞–Ω–∏—è —Ç–∏–ø –∞–≥–µ–Ω—Ç\n",
        "        –∏ –¥–∞ –ø—Ä–æ–¥—ä–ª–∂–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ –æ—Ç—Ç–∞–º.\n",
        "    \"\"\")\n",
        "\n",
        "    # Display available checkpoints for the selected agent type\n",
        "    st.subheader(f\"–ù–∞–ª–∏—á–Ω–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –∑–∞ {agent_type}:\")\n",
        "    if drive_access_available:\n",
        "         checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, f\"{agent_type}_checkpoint_*.zip\")))\n",
        "         if checkpoint_files:\n",
        "              st.write(f\"–ù–∞–º–µ—Ä–µ–Ω–∏ {len(checkpoint_files)} —á–µ–∫–ø–æ–π–Ω—Ç–∞ –≤ `{checkpoint_dir}`:\")\n",
        "              for cf in checkpoint_files:\n",
        "                   st.write(f\"- {os.path.basename(cf)}\")\n",
        "         else:\n",
        "              st.info(f\"‚ö†Ô∏è –ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –∑–∞ {agent_type} –≤ `{checkpoint_dir}`.\")\n",
        "    else:\n",
        "         st.warning(\"üö´ –ù–µ–æ–±—Ö–æ–¥–∏–º –µ –¥–æ—Å—Ç—ä–ø –¥–æ Google Drive –∑–∞ –ø–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏.\")\n",
        "\n",
        "\n",
        "    # Button to Load Latest Checkpoint and Continue Training\n",
        "    if st.button(\"üîÑ –ü—Ä–æ–¥—ä–ª–∂–∏ –û–±—É—á–µ–Ω–∏–µ –æ—Ç –ü–æ—Å–ª–µ–¥–µ–Ω –ß–µ–∫–ø–æ–π–Ω—Ç\", key='continue_training_button'):\n",
        "        if not drive_access_available:\n",
        "             st.error(\"üö´ –ù–µ–æ–±—Ö–æ–¥–∏–º –µ –¥–æ—Å—Ç—ä–ø –¥–æ Google Drive –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏.\")\n",
        "        elif st.session_state.get(\"train_data\") is None or st.session_state[\"train_data\"].empty:\n",
        "            st.warning(\"‚ö†Ô∏è –ú–æ–ª—è, –∑–∞—Ä–µ–¥–µ—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–µ—Ç–µ –¥–∞–Ω–Ω–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ –ø—ä—Ä–≤–æ.\")\n",
        "        else:\n",
        "             st.info(f\"‚è≥ –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–µ–Ω —á–µ–∫–ø–æ–π–Ω—Ç –∑–∞ {agent_type} –∏ –ø—Ä–æ–¥—ä–ª–∂–∞–≤–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ...\")\n",
        "\n",
        "             # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∞ —Å—Ä–µ–¥–∞ (–Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ)\n",
        "             # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ —Å—ä—â–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞ –∫–∞—Ç–æ –∑–∞ –Ω–æ–≤–æ –æ–±—É—á–µ–Ω–∏–µ\n",
        "             current_env_params = {\n",
        "                'initial_amount': initial_amount,\n",
        "                'lookback_window': lookback_window,\n",
        "                'buy_cost_pct': buy_cost_pct,\n",
        "                'sell_cost_pct': sell_cost_pct,\n",
        "                'max_drawdown_limit_pct': max_drawdown_limit_pct,\n",
        "                'position_size_pct': position_size_pct,\n",
        "                'stop_loss_pct': stop_loss_pct,\n",
        "                'take_profit_pct': take_profit_pct,\n",
        "                'trailing_sl_pct': trailing_sl_pct,\n",
        "                'lot_model': lot_model,\n",
        "                'tp_reward_bonus': tp_reward_bonus_pct,\n",
        "                'sl_penalty': sl_penalty_pct\n",
        "            }\n",
        "             st.session_state['env_config'] = current_env_params # Update env config in state\n",
        "\n",
        "\n",
        "             try:\n",
        "                 train_data_for_env = st.session_state.get('train_data')\n",
        "                 if train_data_for_env is None or train_data_for_env.empty:\n",
        "                      st.error(\"üö´ –í—ä—Ç—Ä–µ—à–Ω–∞ –≥—Ä–µ—à–∫–∞: –õ–∏–ø—Å–≤–∞—Ç –¥–∞–Ω–Ω–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ —Å–ª–µ–¥ –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞.\")\n",
        "                      st.stop()\n",
        "\n",
        "                 # Corrected class name\n",
        "                 train_env_instance = ForexTradingEnv(df=train_data_for_env.copy(), **current_env_params)\n",
        "                 vec_train_env = DummyVecEnv([lambda: train_env_instance])\n",
        "\n",
        "                 # --- –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—è —á–µ–∫–ø–æ–π–Ω—Ç ---\n",
        "                 # load_agent_by_type already handles finding the latest checkpoint\n",
        "                 loaded_agent = load_agent_by_type(\n",
        "                     path=save_model_dir, # This path is now a fallback, not the primary source\n",
        "                     env=vec_train_env,\n",
        "                     agent_type=agent_type,\n",
        "                     checkpoint_dir=checkpoint_dir # load_agent_by_type will look here first\n",
        "                 )\n",
        "\n",
        "                 if loaded_agent is not None:\n",
        "                     st.session_state['trained_agent'] = loaded_agent # Store the loaded agent for training\n",
        "                     st.session_state['trained_agent_name'] = agent_type\n",
        "                     st.session_state['loaded_agent'] = None # Clear loaded_agent if continuing training\n",
        "                     st.session_state['loaded_agent_name'] = None\n",
        "                     st.success(f\"‚úÖ {agent_type} –∞–≥–µ–Ω—Ç—ä—Ç –µ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—è —á–µ–∫–ø–æ–π–Ω—Ç. –û–±—É—á–µ–Ω–∏–µ—Ç–æ —â–µ –ø—Ä–æ–¥—ä–ª–∂–∏.\")\n",
        "\n",
        "                     # --- –ü—Ä–æ–¥—ä–ª–∂–∞–≤–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ ---\n",
        "                     progress_bar = st.progress(0)\n",
        "                     status_text = st.empty()\n",
        "\n",
        "                     trained_agent = train_agent(\n",
        "                         st.session_state['trained_agent'], # Use the loaded agent\n",
        "                         total_timesteps=total_timesteps,\n",
        "                         progress_bar=progress_bar,\n",
        "                         status_text=status_text,\n",
        "                         save_dir=save_model_dir, # Use sidebar save dir\n",
        "                         checkpoint_dir=checkpoint_dir, # Use sidebar checkpoint dir\n",
        "                         save_freq=checkpoint_freq # Use sidebar checkpoint freq\n",
        "                     )\n",
        "\n",
        "                     progress_bar.empty()\n",
        "                     status_text.empty()\n",
        "\n",
        "                     if trained_agent:\n",
        "                         st.session_state['trained_agent'] = trained_agent\n",
        "                         st.success(f\"üéâ –û–±—É—á–µ–Ω–∏–µ—Ç–æ –Ω–∞ {agent_type} –ø—Ä–∏–∫–ª—é—á–∏ —É—Å–ø–µ—à–Ω–æ (–ø—Ä–æ–¥—ä–ª–∂–µ–Ω–æ –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç)!\")\n",
        "                     else:\n",
        "                         st.error(\"‚ùå –ü—Ä–æ–¥—ä–ª–∂–∞–≤–∞–Ω–µ—Ç–æ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ –±–µ—à–µ –Ω–µ—É—Å–ø–µ—à–Ω–æ.\")\n",
        "                         st.session_state['trained_agent'] = None\n",
        "\n",
        "                 else:\n",
        "                     st.warning(f\"‚ö†Ô∏è –ù–µ –±—è—Ö–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –∑–∞ {agent_type} –≤ `{checkpoint_dir}` –∏–ª–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ—Ç–æ —Å–µ –ø—Ä–æ–≤–∞–ª–∏. –ú–æ–ª—è, —Å—ä–∑–¥–∞–π—Ç–µ –Ω–æ–≤ –∞–≥–µ–Ω—Ç –≤ –ø—ä—Ä–≤–∏—è —Ç–∞–±.\")\n",
        "                     st.session_state['trained_agent'] = None # Ensure state is clean\n",
        "\n",
        "             except Exception as e:\n",
        "                 st.error(f\"üö´ –í—ä–∑–Ω–∏–∫–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç –∏–ª–∏ –ø—Ä–æ–¥—ä–ª–∂–∞–≤–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ: {e}\")\n",
        "                 st.exception(e)\n",
        "                 st.session_state['trained_agent'] = None # Ensure state is clean\n",
        "\n",
        "\n",
        "with tab3:\n",
        "    st.header(\"–ë–µ–∫—Ç–µ—Å—Ç –∏ –ê–Ω–∞–ª–∏–∑\")\n",
        "    st.markdown(\"\"\"\n",
        "        –í —Ç–æ–∑–∏ —Ä–∞–∑–¥–µ–ª –º–æ–∂–µ—Ç–µ –¥–∞ —Å—Ç–∞—Ä—Ç–∏—Ä–∞—Ç–µ –±–µ–∫—Ç–µ—Å—Ç —Å –æ–±—É—á–µ–Ω –∏–ª–∏ —Ä—ä—á–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –∞–≥–µ–Ω—Ç\n",
        "        –∏ –¥–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–∞—Ç–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ.\n",
        "    \"\"\")\n",
        "\n",
        "    # --- Agent Selection for Backtesting ---\n",
        "    st.subheader(\"–ò–∑–±–æ—Ä –Ω–∞ –ê–≥–µ–Ω—Ç –∑–∞ –ë–µ–∫—Ç–µ—Å—Ç\")\n",
        "\n",
        "    agent_for_backtesting = None\n",
        "    agent_name_for_backtesting = \"–ù—è–º–∞\"\n",
        "\n",
        "    # Option to use the currently trained agent\n",
        "    use_trained_agent = st.checkbox(\"–ò–∑–ø–æ–ª–∑–≤–∞–π —Ç–µ–∫—É—â–æ –æ–±—É—á–µ–Ω –∞–≥–µ–Ω—Ç\", value=('trained_agent' in st.session_state and st.session_state['trained_agent'] is not None), key='use_trained_agent_checkbox', disabled=('trained_agent' not in st.session_state or st.session_state['trained_agent'] is None))\n",
        "\n",
        "    if use_trained_agent and st.session_state.get('trained_agent') is not None:\n",
        "        agent_for_backtesting = st.session_state['trained_agent']\n",
        "        agent_name_for_backtesting = f\"–¢–µ–∫—É—â–æ –æ–±—É—á–µ–Ω ({st.session_state.get('trained_agent_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')})\"\n",
        "        st.info(f\"–ò–∑–±—Ä–∞–Ω –∞–≥–µ–Ω—Ç –∑–∞ –±–µ–∫—Ç–µ—Å—Ç: **{agent_name_for_backtesting}**\") # Typo fixed here\n",
        "\n",
        "    else:\n",
        "         st.info(\"–ù—è–º–∞ –∏–∑–±—Ä–∞–Ω —Ç–µ–∫—É—â–æ –æ–±—É—á–µ–Ω –∞–≥–µ–Ω—Ç. –ú–æ–∂–µ—Ç–µ –¥–∞ –∑–∞—Ä–µ–¥–∏—Ç–µ –∞–≥–µ–Ω—Ç –æ—Ç —Ñ–∞–π–ª –ø–æ-–¥–æ–ª—É.\")\n",
        "         st.session_state['loaded_agent'] = None # Clear loaded agent if using trained one\n",
        "\n",
        "         # Option to load an agent from a specific file\n",
        "         st.subheader(\"–ó–∞—Ä–µ–¥–∏ –ê–≥–µ–Ω—Ç –æ—Ç –§–∞–π–ª –∑–∞ –ë–µ–∫—Ç–µ—Å—Ç\")\n",
        "         # List available saved models\n",
        "         available_saved_models = []\n",
        "         if drive_access_available and os.path.exists(save_model_dir):\n",
        "              model_files = glob.glob(os.path.join(save_model_dir, \"*.zip\"))\n",
        "              # Filter for agent model files, not checkpoints or config\n",
        "              available_saved_models = [os.path.basename(f) for f in model_files if not os.path.basename(f).startswith('._') and '_checkpoint_' not in os.path.basename(f) and not os.path.basename(f).endswith('_env_config.json') and not os.path.basename(f).endswith('_agent_config.json')]\n",
        "\n",
        "         selected_model_file = st.selectbox(\"–ò–∑–±–µ—Ä–∏ –∑–∞–ø–∞–∑–µ–Ω –∞–≥–µ–Ω—Ç (.zip —Ñ–∞–π–ª)\", [\"-- –ò–∑–±–µ—Ä–∏ —Ñ–∞–π–ª --\"] + available_saved_models, key='select_saved_model_file')\n",
        "\n",
        "         if selected_model_file != \"-- –ò–∑–±–µ—Ä–∏ —Ñ–∞–π–ª --\":\n",
        "              # Attempt to determine agent type from filename (basic guess)\n",
        "              guessed_agent_type = \"PPO\" # Default guess\n",
        "              if \"dqn\" in selected_model_file.lower():\n",
        "                   guessed_agent_type = \"DQN\"\n",
        "              elif \"a2c\" in selected_model_file.lower():\n",
        "                   guessed_agent_type = \"A2C\"\n",
        "              # Allow user to confirm/correct agent type\n",
        "              loaded_agent_type_override = st.selectbox(f\"–ü–æ—Ç–≤—ä—Ä–¥–∏/–ö–æ—Ä–∏–≥–∏—Ä–∞–π —Ç–∏–ø –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –∑–∞ `{selected_model_file}`:\", [\"PPO\", \"A2C\", \"DQN\"], index=[\"PPO\", \"A2C\", \"DQN\"].index(guessed_agent_type) if guessed_agent_type in [\"PPO\", \"A2C\", \"DQN\"] else 0, key='loaded_agent_type_override') # Added safety check for index\n",
        "\n",
        "\n",
        "              if st.button(f\"–ó–∞—Ä–µ–¥–∏ `{selected_model_file}`\", key='load_agent_for_backtest_button'):\n",
        "                   if not drive_access_available:\n",
        "                       st.error(\"üö´ –ù–µ–æ–±—Ö–æ–¥–∏–º –µ –¥–æ—Å—Ç—ä–ø –¥–æ Google Drive –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∏.\")\n",
        "                   elif st.session_state.get(\"test_data\") is None or st.session_state[\"test_data\"].empty:\n",
        "                       st.warning(\"‚ö†Ô∏è –ù–µ–æ–±—Ö–æ–¥–∏–º–∏ —Å–∞ —Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏ –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Å—Ä–µ–¥–∞, —Å—ä–≤–º–µ—Å—Ç–∏–º–∞ —Å –∞–≥–µ–Ω—Ç–∞.\")\n",
        "                   else:\n",
        "                        st.info(f\"‚öôÔ∏è –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç —Ç–∏–ø {loaded_agent_type_override} –æ—Ç: `{selected_model_file}`...\")\n",
        "                        full_model_path = os.path.join(save_model_dir, selected_model_file)\n",
        "\n",
        "                        # Create a temporary environment for loading (using test data)\n",
        "                        # Use the current environment parameters from the sidebar\n",
        "                        current_env_params_for_load = {\n",
        "                             'initial_amount': initial_amount,\n",
        "                             'lookback_window': lookback_window,\n",
        "                             'buy_cost_pct': buy_cost_pct,\n",
        "                             'sell_cost_pct': sell_cost_pct,\n",
        "                             'max_drawdown_limit_pct': max_drawdown_limit_pct,\n",
        "                             'position_size_pct': position_size_pct,\n",
        "                             'stop_loss_pct': stop_loss_pct,\n",
        "                             'take_profit_pct': take_profit_pct,\n",
        "                             'trailing_sl_pct': trailing_sl_pct,\n",
        "                             'lot_model': lot_model,\n",
        "                             'tp_reward_bonus': tp_reward_bonus_pct,\n",
        "                             'sl_penalty': sl_penalty_pct\n",
        "                         }\n",
        "\n",
        "                        try:\n",
        "                             test_data_for_load_env = st.session_state.get('test_data')\n",
        "                             if test_data_for_load_env is None or test_data_for_load_env.empty:\n",
        "                                  st.error(\"üö´ –í—ä—Ç—Ä–µ—à–Ω–∞ –≥—Ä–µ—à–∫–∞: –õ–∏–ø—Å–≤–∞—Ç —Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏ –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Å—Ä–µ–¥–∞ –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ.\")\n",
        "                                  st.stop()\n",
        "\n",
        "                             # Corrected class name\n",
        "                             temp_load_env_instance = ForexTradingEnv(df=test_data_for_load_env.copy(), **current_env_params_for_load)\n",
        "                             temp_vec_env = DummyVecEnv([lambda: temp_load_env_instance])\n",
        "\n",
        "                             loaded_agent_obj = load_agent_by_type(\n",
        "                                 path=full_model_path, # Load directly from the specified path\n",
        "                                 env=temp_vec_env,\n",
        "                                 agent_type=loaded_agent_type_override,\n",
        "                                 checkpoint_dir=None # Do not look for checkpoints when loading a specific saved model file\n",
        "                             )\n",
        "\n",
        "                             if loaded_agent_obj:\n",
        "                                 st.session_state['loaded_agent'] = loaded_agent_obj\n",
        "                                 st.session_state['loaded_agent_name'] = loaded_agent_type_override\n",
        "                                 st.session_state['trained_agent'] = None # Clear trained agent if loading a specific one\n",
        "                                 st.session_state['trained_agent_name'] = None\n",
        "                                 st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç —Ç–∏–ø {loaded_agent_type_override} –µ –∑–∞—Ä–µ–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ –±–µ–∫—Ç–µ—Å—Ç.\")\n",
        "                                 # Update the agent_for_backtesting variable\n",
        "                                 agent_for_backtesting = st.session_state['loaded_agent']\n",
        "                                 agent_name_for_backtesting = f\"–ó–∞—Ä–µ–¥–µ–Ω ({st.session_state.get('loaded_agent_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')})\"\n",
        "                                 st.info(f\"–ò–∑–±—Ä–∞–Ω –∞–≥–µ–Ω—Ç –∑–∞ –±–µ–∫—Ç–µ—Å—Ç: **{agent_name_for_backtesting}**\")\n",
        "\n",
        "                             else:\n",
        "                                 st.error(\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç —Ñ–∞–π–ª.\")\n",
        "                                 st.session_state['loaded_agent'] = None\n",
        "                                 st.session_state['loaded_agent_name'] = None\n",
        "\n",
        "                        except Exception as e:\n",
        "                            st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç —Ñ–∞–π–ª: {e}\")\n",
        "                            st.exception(e)\n",
        "                            st.session_state['loaded_agent'] = None\n",
        "                            st.session_state['loaded_agent_name'] = None\n",
        "\n",
        "    # Ensure agent_for_backtesting is set correctly after load attempts\n",
        "    if use_trained_agent and st.session_state.get('trained_agent') is not None:\n",
        "         agent_for_backtesting = st.session_state['trained_agent']\n",
        "         agent_name_for_backtesting = f\"–¢–µ–∫—É—â–æ –æ–±—É—á–µ–Ω ({st.session_state.get('trained_agent_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')})\"\n",
        "    elif st.session_state.get('loaded_agent') is not None:\n",
        "         agent_for_backtesting = st.session_state['loaded_agent']\n",
        "         agent_name_for_backtesting = f\"–ó–∞—Ä–µ–¥–µ–Ω ({st.session_state.get('loaded_agent_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')})\"\n",
        "    else:\n",
        "        agent_for_backtesting = None\n",
        "        agent_name_for_backtesting = \"–ù—è–º–∞\"\n",
        "\n",
        "    st.write(f\"–ê–≥–µ–Ω—Ç –∑–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥: **{agent_name_for_backtesting}**\") # Display final selected agent\n",
        "\n",
        "\n",
        "    # --- Start Backtesting Button ---\n",
        "    if st.button(\"üî¨ –°—Ç–∞—Ä—Ç–∏—Ä–∞–π –ë–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥\", key='start_backtesting_button_tab3'): # Changed key name\n",
        "        if agent_for_backtesting is None:\n",
        "            st.warning(\"‚ö†Ô∏è –ù—è–º–∞ –Ω–∞–ª–∏—á–µ–Ω –æ–±—É—á–µ–Ω –∏–ª–∏ –∑–∞—Ä–µ–¥–µ–Ω –∞–≥–µ–Ω—Ç –∑–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥.\")\n",
        "        elif st.session_state['test_data'] is None or st.session_state['test_data'].empty:\n",
        "            st.warning(\"‚ö†Ô∏è –õ–∏–ø—Å–≤–∞—Ç —Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏ –∑–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥. –ú–æ–ª—è, –∑–∞—Ä–µ–¥–µ—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–µ—Ç–µ –¥–∞–Ω–Ω–∏ –ø—ä—Ä–≤–æ.\")\n",
        "        else:\n",
        "            st.info(\"‚è≥ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥...\")\n",
        "\n",
        "            # –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Å—Ä–µ–¥–∞ –∑–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥ (–∏–∑–ø–æ–ª–∑–≤–∞–º–µ —Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏)\n",
        "            # –°—ä–±–∏—Ä–∞–Ω–µ –Ω–∞ –≤—Å–∏—á–∫–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ —Å—Ä–µ–¥–∞—Ç–∞ –æ—Ç UI (—Å—ä—â–∏—Ç–µ –∫–∞—Ç–æ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ)\n",
        "            current_env_params_for_backtest = {\n",
        "                 'initial_amount': initial_amount,\n",
        "                 'lookback_window': lookback_window,\n",
        "                 'buy_cost_pct': buy_cost_pct,\n",
        "                 'sell_cost_pct': sell_cost_pct,\n",
        "                 'max_drawdown_limit_pct': max_drawdown_limit_pct,\n",
        "                 'position_size_pct': position_size_pct,\n",
        "                 'stop_loss_pct': stop_loss_pct,\n",
        "                 'take_profit_pct': take_profit_pct,\n",
        "                 'trailing_sl_pct': trailing_sl_pct,\n",
        "                 'lot_model': lot_model,\n",
        "                 'tp_reward_bonus': tp_reward_bonus_pct,\n",
        "                 'sl_penalty': sl_penalty_pct\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                 # Corrected class name\n",
        "                 test_env_instance = ForexTradingEnv(df=st.session_state['test_data'].copy(), **current_env_params_for_backtest)\n",
        "\n",
        "                 # --- –ò–∑–ø—ä–ª–Ω–µ–Ω–∏–µ –Ω–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ (—Ä—ä—á–µ–Ω —Ü–∏–∫—ä–ª) ---\n",
        "                 st.write(\"üîÑ –ó–∞–ø–æ—á–≤–∞ —Å–∏–º—É–ª–∞—Ü–∏—è –Ω–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥...\")\n",
        "                 obs, info = test_env_instance.reset()\n",
        "                 done = False\n",
        "                 truncated = False\n",
        "                 backtesting_results = []\n",
        "                 step_count = 0\n",
        "\n",
        "                 # Correct total steps is total rows minus the start offset (max of lookback and atr window)\n",
        "                 # Ensure atr_window is correctly retrieved, fallback to default if needed\n",
        "                 env_atr_window = current_env_params_for_backtest.get('atr_window', 14) # Get atr_window from env params\n",
        "                 # Access lot_model from the correct environment instance\n",
        "                 start_offset = max(test_env_instance.lookback_window, env_atr_window if test_env_instance.lot_model == 'volatility' else 0) # Use correct atr_window\n",
        "                 total_steps_backtest = len(test_env_instance.df) - start_offset # Total simulation steps available\n",
        "\n",
        "                 # Add progress bar and status text for backtesting\n",
        "                 progress_bar_backtest = st.progress(0)\n",
        "                 status_text_backtest = st.empty()\n",
        "\n",
        "                 st.write(f\"–û–±—â –±—Ä–æ–π –≤—ä–∑–º–æ–∂–Ω–∏ —Å—Ç—ä–ø–∫–∏ –∑–∞ —Å–∏–º—É–ª–∞—Ü–∏—è: {total_steps_backtest}\")\n",
        "\n",
        "\n",
        "                 while not done and not truncated and test_env_instance.current_step < len(test_env_instance.df): # Loop while not done, not truncated, and within data bounds\n",
        "                     try:\n",
        "                          st.write(f\"–°—Ç—ä–ø–∫–∞: {test_env_instance.current_step}\")\n",
        "                          # Predict the action using the trained agent (deterministic=True for evaluation)\n",
        "                          # Add logging before prediction\n",
        "                          st.write(f\"–ü—Ä–µ–¥–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞–Ω–µ - obs shape: {obs.shape if isinstance(obs, np.ndarray) else 'N/A'}, obs type: {type(obs)}\")\n",
        "                          action, _states = agent_for_backtesting.predict(obs, deterministic=True)\n",
        "                          action_scalar = action.item() if isinstance(action, np.ndarray) else action\n",
        "                          st.write(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ: {action_scalar}\")\n",
        "\n",
        "\n",
        "                          # Basic check for valid observation before stepping\n",
        "                          if obs is None or not isinstance(obs, np.ndarray) or obs.shape != test_env_instance.observation_space.shape:\n",
        "                               st.error(f\"üö´ Error at step {step_count}: Invalid observation received from environment. Shape: {obs.shape if isinstance(obs, np.ndarray) else 'N/A'}, Expected: {test_env_instance.observation_space.shape}\")\n",
        "                               break # Exit loop on invalid observation\n",
        "\n",
        "                          # Take a step in the environment\n",
        "                          st.write(f\"–ü—Ä–µ–¥–∏ —Å—Ç—ä–ø–∫–∞ –≤ —Å—Ä–µ–¥–∞—Ç–∞ —Å –¥–µ–π—Å—Ç–≤–∏–µ: {action_scalar}\")\n",
        "                          # Add try-except around env.step\n",
        "                          try:\n",
        "                             obs, reward, done, truncated, info = test_env_instance.step(action_scalar)\n",
        "                             st.write(f\"–°–ª–µ–¥ —Å—Ç—ä–ø–∫–∞ –≤ —Å—Ä–µ–¥–∞—Ç–∞ - reward: {reward}, done: {done}, truncated: {truncated}, info keys: {info.keys()}\")\n",
        "                          except Exception as e:\n",
        "                             st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ —Å—Ç—ä–ø–∫–∞ –≤ —Å—Ä–µ–¥–∞—Ç–∞ (env.step) –Ω–∞ —Å—Ç—ä–ø–∫–∞ {step_count}: {e}\")\n",
        "                             st.exception(e)\n",
        "                             done = True # Stop simulation on environment step error\n",
        "                             break # Exit loop\n",
        "\n",
        "\n",
        "                          # Add action to info for logging/analysis\n",
        "                          info['action'] = action_scalar\n",
        "                          info['reward'] = reward # Store reward in info for analysis/plotting\n",
        "\n",
        "                          # Append info dictionary\n",
        "                          # Ensure 'date' is included in the info dictionary from the environment step\n",
        "                          # Assuming your ForexTradingEnv step method adds 'date' to the info dict\n",
        "                          # Add a check for required keys in info dictionary before appending\n",
        "                          required_info_keys = ['portfolio_value', 'date'] # Add other keys if necessary\n",
        "                          if all(key in info for key in required_info_keys):\n",
        "                               backtesting_results.append(info)\n",
        "                          else:\n",
        "                               st.warning(f\"‚ö†Ô∏è –°—Ç—ä–ø–∫–∞ {step_count}: Info dictionary –ª–∏–ø—Å–≤–∞—Ç –∫–ª—é—á–æ–≤–µ ({[key for key in required_info_keys if key not in info]}). –ü—Ä–æ–ø—É—Å–∫–∞–Ω–µ –Ω–∞ –∑–∞–ø–∏—Å –∑–∞ —Ç–∞–∑–∏ —Å—Ç—ä–ø–∫–∞.\")\n",
        "\n",
        "\n",
        "                          step_count += 1\n",
        "                          # Update progress bar based on the actual steps taken relative to total possible steps\n",
        "                          if total_steps_backtest > 0:\n",
        "                               progress_value = min(1.0, step_count / total_steps_backtest) # Ensure progress doesn't exceed 1\n",
        "                               progress_bar_backtest.progress(progress_value)\n",
        "                               status_text_backtest.text(f\"–ë–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥ –≤ –ø—Ä–æ–≥—Ä–µ—Å: {step_count}/{total_steps_backtest} —Å—Ç—ä–ø–∫–∏\")\n",
        "                          else:\n",
        "                               status_text_backtest.text(\"–ë–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥: –ù—è–º–∞ —Å—Ç—ä–ø–∫–∏ –∑–∞ —Å–∏–º—É–ª–∞—Ü–∏—è.\")\n",
        "                               break # Exit if no steps possible\n",
        "\n",
        "\n",
        "                     except Exception as e:\n",
        "                          st.error(f\"üö´ –í—ä–∑–Ω–∏–∫–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ —Å—Ç—ä–ø–∫–∞ –≤ —Ü–∏–∫—ä–ª–∞ –Ω–∞ –±–µ–∫—Ç–µ—Å—Ç (–∏–∑–≤—ä–Ω env.step) –Ω–∞ —Å—Ç—ä–ø–∫–∞ {step_count}: {e}\")\n",
        "                          st.exception(e)\n",
        "                          done = True # –ü—Ä–µ–∫—Ä–∞—Ç—è–≤–∞–º–µ —Å–∏–º—É–ª–∞—Ü–∏—è—Ç–∞ –ø—Ä–∏ –≥—Ä–µ—à–∫–∞\n",
        "\n",
        "\n",
        "                 progress_bar_backtest.empty()\n",
        "                 status_text_backtest.empty()\n",
        "                 st.write(\"‚úÖ –ë–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Å–∏–º—É–ª–∞—Ü–∏—è—Ç–∞ –ø—Ä–∏–∫–ª—é—á–∏.\")\n",
        "\n",
        "\n",
        "                 # Store results in session state\n",
        "                 if backtesting_results:\n",
        "                     st.write(f\"–°—ä–±—Ä–∞–Ω–∏ {len(backtesting_results)} —Ä–µ–∑—É–ª—Ç–∞—Ç–∞ –æ—Ç –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞.\")\n",
        "                     st.session_state['backtesting_results'] = pd.DataFrame(backtesting_results)\n",
        "                     st.session_state['trades_log'] = test_env_instance.trades # –í–∑–µ–º–∞–º–µ –¥–Ω–µ–≤–Ω–∏–∫–∞ –Ω–∞ —Å–¥–µ–ª–∫–∏—Ç–µ –æ—Ç —Å—Ä–µ–¥–∞—Ç–∞\n",
        "\n",
        "                     # --- –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –∏ –ø–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ ---\n",
        "                     if not st.session_state['backtesting_results'].empty and 'portfolio_value' in st.session_state['backtesting_results'].columns:\n",
        "                         st.write(\"–ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ...\")\n",
        "                         performance_metrics = calculate_metrics(\n",
        "                             st.session_state['backtesting_results']['portfolio_value'],\n",
        "                             st.session_state['trades_log'],\n",
        "                             current_env_params_for_backtest['initial_amount']\n",
        "                         )\n",
        "                         st.session_state['performance_metrics'] = performance_metrics\n",
        "                         st.success(\"‚úÖ –ë–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥—ä—Ç –ø—Ä–∏–∫–ª—é—á–∏. –ú–µ—Ç—Ä–∏–∫–∏—Ç–µ —Å–∞ –∏–∑—á–∏—Å–ª–µ–Ω–∏.\")\n",
        "\n",
        "                     elif not st.session_state['backtesting_results'].empty:\n",
        "                          st.warning(\"‚ö†Ô∏è –†–µ–∑—É–ª—Ç–∞—Ç–∏ –æ—Ç –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ —Å–∞ —Å—ä–±—Ä–∞–Ω–∏, –Ω–æ –ª–∏–ø—Å–≤–∞ –∫–æ–ª–æ–Ω–∞—Ç–∞ 'portfolio_value'. –ù–µ –º–æ–≥–∞—Ç –¥–∞ —Å–µ –∏–∑—á–∏—Å–ª—è—Ç –º–µ—Ç—Ä–∏–∫–∏.\")\n",
        "                          st.session_state['performance_metrics'] = None\n",
        "                     else:\n",
        "                          st.warning(\"‚ö†Ô∏è –ù–µ –±—è—Ö–∞ —Å—ä–±—Ä–∞–Ω–∏ –¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –¥–∞–Ω–Ω–∏ —Å 'portfolio_value' –∑–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏.\")\n",
        "                          st.session_state['performance_metrics'] = None\n",
        "                          st.session_state['trades_log'] = None\n",
        "\n",
        "                 else:\n",
        "                      st.warning(\"‚ö†Ô∏è –ù–µ –±—è—Ö–∞ —Å—ä–±—Ä–∞–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –æ—Ç –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞.\")\n",
        "                      st.session_state['backtesting_results'] = None\n",
        "                      st.session_state['performance_metrics'] = None\n",
        "                      st.session_state['trades_log'] = None\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Å—Ä–µ–¥–∞ –∏–ª–∏ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ –Ω–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥: {e}\")\n",
        "                st.exception(e)\n",
        "                st.session_state['backtesting_results'] = None\n",
        "                st.session_state['performance_metrics'] = None\n",
        "                st.session_state['trades_log'] = None\n",
        "\n",
        "\n",
        "    # --- –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ –†–µ–∑—É–ª—Ç–∞—Ç–∏ (within Tab 3) ---\n",
        "    st.subheader(\"–†–µ–∑—É–ª—Ç–∞—Ç–∏ –æ—Ç –ë–µ–∫—Ç–µ—Å—Ç\")\n",
        "\n",
        "    # –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏\n",
        "    if st.session_state['performance_metrics'] is not None:\n",
        "        st.markdown(\"##### –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ\")\n",
        "        metrics_df = pd.DataFrame.from_dict(st.session_state['performance_metrics'], orient='index', columns=['Value'])\n",
        "        st.dataframe(metrics_df)\n",
        "\n",
        "    # –ü–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ –¥–Ω–µ–≤–Ω–∏–∫ –Ω–∞ —Å–¥–µ–ª–∫–∏—Ç–µ\n",
        "    if st.session_state['trades_log'] is not None and st.session_state['trades_log']:\n",
        "        st.markdown(\"##### –î–Ω–µ–≤–Ω–∏–∫ –Ω–∞ —Å–¥–µ–ª–∫–∏—Ç–µ\")\n",
        "        trades_df = pd.DataFrame(st.session_state['trades_log'])\n",
        "        st.dataframe(trades_df)\n",
        "        st.info(f\"–û–±—â –±—Ä–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–∞–Ω–∏ —Å–¥–µ–ª–∫–∏: {len(st.session_state['trades_log'])}\")\n",
        "        # Option to save trades log\n",
        "        csv_trades = trades_df.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\n",
        "             label=\"–ò–∑—Ç–µ–≥–ª–∏ –¥–Ω–µ–≤–Ω–∏–∫ –Ω–∞ —Å–¥–µ–ª–∫–∏—Ç–µ –∫–∞—Ç–æ CSV\",\n",
        "             data=csv_trades,\n",
        "             file_name=f'trades_log_{agent_name_for_backtesting.replace(\" \", \"_\").lower()}.csv',\n",
        "             mime='text/csv',\n",
        "             key='download_trades_log'\n",
        "        )\n",
        "\n",
        "    elif st.session_state['trades_log'] is not None and not st.session_state['trades_log']:\n",
        "         st.info(\"‚ÑπÔ∏è –ü–æ –≤—Ä–µ–º–µ –Ω–∞ –±–µ–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ –Ω–µ —Å–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–∞–Ω–∏ —Å–¥–µ–ª–∫–∏.\")\n",
        "    elif st.session_state['backtesting_results'] is not None:\n",
        "        st.info(\"‚ÑπÔ∏è –ë–µ–∫te—Å—Ç–∏–Ω–≥—ä—Ç –ø—Ä–∏–∫–ª—é—á–∏, –Ω–æ –¥–Ω–µ–≤–Ω–∏–∫—ä—Ç –Ω–∞ —Å–¥–µ–ª–∫–∏—Ç–µ –Ω–µ –µ –Ω–∞–ª–∏—á–µ–Ω.\")\n",
        "\n",
        "\n",
        "    # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –î–∞–Ω–Ω–∏ –∏ –°–¥–µ–ª–∫–∏ (Plotly Chart - within Tab 3) ---\n",
        "    st.subheader(\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –¥–∞–Ω–Ω–∏ –∏ —Å–¥–µ–ª–∫–∏\")\n",
        "\n",
        "    if st.session_state['processed_data'] is not None and not st.session_state['processed_data'].empty:\n",
        "        # Use processed_data for the main plot, as it contains indicators\n",
        "        plot_df = st.session_state['processed_data'].copy()\n",
        "\n",
        "        # Ensure the date column is datetime and is the index\n",
        "        if 'date' in plot_df.columns:\n",
        "             plot_df['date'] = pd.to_datetime(plot_df['date'], errors='coerce')\n",
        "             plot_df.dropna(subset=['date'], inplace=True) # Remove rows with invalid dates\n",
        "             if not plot_df.empty:\n",
        "                  plot_df = plot_df.set_index('date')\n",
        "             else:\n",
        "                  st.warning(\"–ù–µ–≤–∞–ª–∏–¥–Ω–∏ –¥–∞—Ç–∏ –≤ –¥–∞–Ω–Ω–∏—Ç–µ —Å–ª–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∞. –ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –ø–æ—Å—Ç—Ä–æ–∏ –≥—Ä–∞—Ñ–∏–∫–∞.\")\n",
        "                  plot_df = None # To avoid errors below\n",
        "        elif not isinstance(plot_df.index, pd.DatetimeIndex):\n",
        "             st.warning(\"–ù–µ–≤–∞–ª–∏–¥–µ–Ω —Ñ–æ—Ä–º–∞—Ç –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞. –ù—É–∂–µ–Ω –µ DatetimeIndex –∏–ª–∏ 'date' –∫–æ–ª–æ–Ω–∞ –∑–∞ –≥—Ä–∞—Ñ–∏–∫–∞—Ç–∞.\")\n",
        "             plot_df = None\n",
        "\n",
        "\n",
        "        if plot_df is not None and not plot_df.empty:\n",
        "\n",
        "            # --- Controls for selecting indicators for visualization ---\n",
        "            available_indicators = [col for col in plot_df.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'original_index', 'sequential_index', 'date']] # Exclude non-indicator columns and 'date'\n",
        "            selected_indicators = st.multiselect(\n",
        "                \"–ò–∑–±–µ—Ä–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∑–∞ –ø–æ–∫–∞–∑–≤–∞–Ω–µ\",\n",
        "                available_indicators,\n",
        "                default=[ind for ind in ['sma', 'rsi', 'macd', 'bb_upper', 'bb_lower', 'bb_mavg', 'atr'] if ind in available_indicators], # Example default selected\n",
        "                key='selected_indicators_for_plot_tab3' # Changed key name\n",
        "            )\n",
        "\n",
        "            # --- Create Plotly figure ---\n",
        "            # Determine how many subplots we need (main + one for each indicator not on the main chart)\n",
        "            # Indicators usually shown on the same chart as price: SMA, EMA, Bollinger Bands (upper, lower, mavg)\n",
        "            # Indicators usually shown in separate subplots: RSI, MACD (macd, signal, diff), CCI, ADX (adx, pos, neg), AO, Stochastic (k, d), ATR\n",
        "            indicators_on_price_chart = ['sma', 'ema_12', 'ema_26', 'bb_upper', 'bb_lower', 'bb_mavg'] # Added specific EMAs\n",
        "            indicators_in_subcharts = ['rsi', 'macd', 'macd_signal', 'macd_diff', 'cci', 'adx', 'adx_pos', 'adx_neg', 'ao', 'stoch_k', 'stoch_d', 'atr']\n",
        "\n",
        "            # Filter selected indicators based on where they are shown\n",
        "            selected_on_price = [ind for ind in selected_indicators if ind in indicators_on_price_chart and ind in plot_df.columns]\n",
        "            selected_in_subcharts = [ind for ind in selected_indicators if ind in indicators_in_subcharts and ind in plot_df.columns]\n",
        "\n",
        "            # Number of subplots = 1 (for price) + number of selected indicators in separate subplots\n",
        "            # We should count MACD as one group if at least one of its components is selected\n",
        "            has_macd_subchart = any(ind in selected_in_subcharts for ind in ['macd', 'macd_signal', 'macd_diff'])\n",
        "            num_subcharts = 1 + len([ind for ind in selected_in_subcharts if ind not in ['macd', 'macd_signal', 'macd_diff']]) + (1 if has_macd_subchart else 0)\n",
        "\n",
        "            # Determine subplot heights (main is larger)\n",
        "            base_height = 0.6 # Height for the price chart\n",
        "            subchart_height = (1.0 - base_height) / max(1, (num_subcharts - 1)) if num_subcharts > 1 else 0 # Remaining height divided among subcharts\n",
        "            row_heights = [base_height] + [subchart_height] * (num_subcharts - 1)\n",
        "\n",
        "\n",
        "            # Create subplots\n",
        "            fig = make_subplots(\n",
        "                rows=num_subcharts,\n",
        "                cols=1,\n",
        "                shared_xaxes=True, # Share x-axis between all charts\n",
        "                vertical_spacing=0.05, # Space between charts\n",
        "                row_heights=row_heights, # Row heights\n",
        "                # Add titles for Y-axes\n",
        "                subplot_titles=['–¶–µ–Ω–∞'] + [ind.upper() for ind in selected_in_subcharts if ind not in ['macd', 'macd_signal', 'macd_diff']] + (['MACD'] if has_macd_subchart else [])\n",
        "\n",
        "            )\n",
        "\n",
        "            # --- Add Price (Candlestick) to the main chart ---\n",
        "            fig.add_trace(go.Candlestick(\n",
        "                x=plot_df.index,\n",
        "                open=plot_df['open'],\n",
        "                high=plot_df['high'],\n",
        "                low=plot_df['low'],\n",
        "                close=plot_df['close'],\n",
        "                name='–¶–µ–Ω–∞',\n",
        "                increasing_line_color='green', # Green for increasing candles\n",
        "                decreasing_line_color='red' # Red for decreasing candles\n",
        "            ), row=1, col=1)\n",
        "\n",
        "            # --- Add selected indicators to the main chart ---\n",
        "            for indicator_name in selected_on_price:\n",
        "                if indicator_name in plot_df.columns:\n",
        "                     fig.add_trace(go.Scatter(\n",
        "                         x=plot_df.index,\n",
        "                         y=plot_df[indicator_name],\n",
        "                         mode='lines',\n",
        "                         name=indicator_name.upper(), # Display in uppercase\n",
        "                         line=dict(width=1) # Thin line for indicators\n",
        "                     ), row=1, col=1)\n",
        "\n",
        "            # --- Add selected indicators to separate subplots ---\n",
        "            row_index = 2 # Start from the second row for subplots\n",
        "            added_macd_group = False # Flag whether the MACD group has been added\n",
        "\n",
        "            for indicator_name in selected_in_subcharts:\n",
        "                if indicator_name in plot_df.columns:\n",
        "                     # We might want MACD components on one chart\n",
        "                     if indicator_name in ['macd', 'macd_signal', 'macd_diff']:\n",
        "                          if not added_macd_group: # Add the MACD group only once\n",
        "                               # Add all MACD components at once in a new subplot\n",
        "                               macd_components_present = [ind for ind in ['macd', 'macd_signal', 'macd_diff'] if ind in selected_in_subcharts and ind in plot_df.columns]\n",
        "                               if macd_components_present:\n",
        "                                    # MACD Line\n",
        "                                    if 'macd' in macd_components_present:\n",
        "                                        fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['macd'], mode='lines', name='MACD Line', line=dict(color='blue', width=1)), row=row_index, col=1)\n",
        "                                    # MACD Signal Line\n",
        "                                    if 'macd_signal' in macd_components_present:\n",
        "                                         fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['macd_signal'], mode='lines', name='MACD Signal', line=dict(color='red', width=1)), row=row_index, col=1)\n",
        "                                    # MACD Histogram (as Bar chart)\n",
        "                                    if 'macd_diff' in macd_components_present:\n",
        "                                         # Determine histogram color based on value\n",
        "                                         colors = ['green' if val >= 0 else 'red' for val in plot_df['macd_diff']]\n",
        "                                         fig.add_trace(go.Bar(x=plot_df.index, y=plot_df['macd_diff'], name='MACD Hist', marker_color=colors, opacity=0.7), row=row_index, col=1)\n",
        "\n",
        "                                    # fig.update_yaxes(title_text=\"MACD\", row=row_index, col=1) # Title set by subplot_titles\n",
        "                                    row_index += 1 # Move to the next row after adding the MACD group\n",
        "                                    added_macd_group = True # Mark that the MACD group has been added\n",
        "\n",
        "                     # Indicators in separate subplots (excluding MACD, as we handled it as a group)\n",
        "                     elif indicator_name not in ['macd', 'macd_signal', 'macd_diff']:\n",
        "                         fig.add_trace(go.Scatter(\n",
        "                             x=plot_df.index,\n",
        "                             y=plot_df[indicator_name],\n",
        "                             mode='lines',\n",
        "                             name=indicator_name.upper(),\n",
        "                             line=dict(width=1)\n",
        "                         ), row=row_index, col=1)\n",
        "                         # fig.update_yaxes(title_text=indicator_name.upper(), row=row_index, col=1) # Title set by subplot_titles\n",
        "                         row_index += 1 # Move to the next row\n",
        "\n",
        "            # --- Add markers for trades from backtest ---\n",
        "            if st.session_state['trades_log'] is not None and st.session_state['trades_log']:\n",
        "                 trades_df = pd.DataFrame(st.session_state['trades_log'])\n",
        "                 # Ensure dates in the trades log match the index of plot_df\n",
        "                 if 'date' in trades_df.columns:\n",
        "                      trades_df['date'] = pd.to_datetime(trades_df['date'], errors='coerce')\n",
        "                      trades_df.dropna(subset=['date'], inplace=True)\n",
        "                      trades_df = trades_df.set_index('date')\n",
        "\n",
        "                      # Filter trades that are within the range of plot_df\n",
        "                      trades_df = trades_df[(trades_df.index >= plot_df.index.min()) & (trades_df.index <= plot_df.index.max())]\n",
        "\n",
        "\n",
        "                      # Add markers for Buy\n",
        "                      buy_trades = trades_df[trades_df['action'] == 'buy']\n",
        "                      if not buy_trades.empty:\n",
        "                           fig.add_trace(go.Scatter(\n",
        "                               x=buy_trades.index,\n",
        "                               y=buy_trades['price'], # Use execution price\n",
        "                               mode='markers',\n",
        "                               marker=dict(color='green', size=10, symbol='triangle-up'), # Green upward triangle\n",
        "                               name='Buy Signal',\n",
        "                               hoverinfo='text',\n",
        "                               text=[f\"Buy<br>Price: {p:.5f}<br>Units: {u:.2f}<br>Step: {s}\" for p, u, s in zip(buy_trades['price'], buy_trades['units'], buy_trades['step'])]\n",
        "                           ), row=1, col=1) # Add markers to the main chart\n",
        "\n",
        "                      # Add markers for Sell (exits - sell, SL, TP)\n",
        "                      sell_trades = trades_df[trades_df['type'] == 'exit'] # Exits are of type 'exit'\n",
        "                      if not sell_trades.empty:\n",
        "                           fig.add_trace(go.Scatter(\n",
        "                               x=sell_trades.index,\n",
        "                               y=sell_trades['price'], # Use execution price\n",
        "                               mode='markers',\n",
        "                               marker=dict(color='red', size=10, symbol='triangle-down'), # Red downward triangle\n",
        "                               name='Sell/Exit Signal',\n",
        "                               hoverinfo='text',\n",
        "                               text=[f\"{act}<br>Price: {p:.5f}<br>PnL: {pnl:.2f}<br>Step: {s}\" for act, p, pnl, s in zip(sell_trades['action'], sell_trades['price'], sell_trades['pnl'], sell_trades['step'])]\n",
        "                           ), row=1, col=1) # Add markers to the main chart\n",
        "\n",
        "\n",
        "            # --- Configure chart layout ---\n",
        "            fig.update_layout(\n",
        "                # title='–¶–µ–Ω–∞, –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∏ –°–¥–µ–ª–∫–∏', # Title is now set implicitly by make_subplots titles\n",
        "                xaxis_title='–î–∞—Ç–∞',\n",
        "                # yaxis_title='–¶–µ–Ω–∞', # Y-axis title set by subplot_titles\n",
        "                xaxis_rangeslider_visible=False, # Hide the lower range slider\n",
        "                hovermode='x unified', # Show info for all traces on one date\n",
        "                height=800 # Total chart height\n",
        "            )\n",
        "\n",
        "            # Update layout for shared x-axis and other settings\n",
        "            # Ensure only the bottom chart shows x-axis tick labels\n",
        "            for i in range(1, num_subcharts + 1):\n",
        "                 if i < num_subcharts:\n",
        "                      fig.update_xaxes(showticklabels=False, row=i, col=1)\n",
        "                 else:\n",
        "                      fig.update_xaxes(showticklabels=True, row=i, col=1)\n",
        "\n",
        "            # Show the chart in Streamlit\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        else:\n",
        "            st.info(\"–ó–∞—Ä–µ–¥–µ—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–µ—Ç–µ –¥–∞–Ω–Ω–∏, –∑–∞ –¥–∞ –≤–∏–¥–∏—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∞—Ç–∞.\")\n",
        "    else:\n",
        "        st.info(\"–ó–∞—Ä–µ–¥–µ—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–µ—Ç–µ –¥–∞–Ω–Ω–∏, –∑–∞ –¥–∞ –≤–∏–¥–∏—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∞—Ç–∞.\")\n",
        "\n",
        "\n",
        "# --- Instructions for running the Streamlit dashboard locally (duplicated for convenience) ---\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.subheader(\"–ö–∞–∫ –¥–∞ —Å—Ç–∞—Ä—Ç–∏—Ä–∞—Ç–µ –¥–∞—à–±–æ—Ä–¥–∞\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "1. –ó–∞–ø–∞–∑–µ—Ç–µ –∫–æ–¥–∞ –∫–∞—Ç–æ `forex_dashboard.py`.\n",
        "2. –£–≤–µ—Ä–µ—Ç–µ —Å–µ, —á–µ —Ñ–∞–π–ª–æ–≤–µ—Ç–µ `data_utils.py`, `forex_env_utils.py`, `agent_utils.py` —Å–∞ –≤ —Å—ä—â–∞—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è.\n",
        "3. –û—Ç–≤–æ—Ä–µ—Ç–µ —Ç–µ—Ä–º–∏–Ω–∞–ª –≤ —Ç–∞–∑–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è.\n",
        "4. –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–π—Ç–µ Streamlit, yfinance, ta, plotly:\n",
        "   `pip install streamlit yfinance ta plotly`\n",
        "5. –°—Ç–∞—Ä—Ç–∏—Ä–∞–π—Ç–µ –¥–∞—à–±–æ—Ä–¥–∞:\n",
        "   `streamlit run forex_dashboard.py`\n",
        "6. –û—Ç–≤–æ—Ä–µ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–∏—è URL –∞–¥—Ä–µ—Å –≤ –±—Ä–∞—É–∑—ä—Ä–∞.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5k4CBtNbnJc",
        "outputId": "ff3f861f-d82c-4ae5-cb86-1d2b6b61e0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "üîÑ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Cloudflare Tunnel...\n",
            "–ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—É–±–ª–∏—á–µ–Ω URL...\n",
            "2025-10-07T03:56:16Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-07T03:56:16Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-07T03:56:19Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-07T03:56:19Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-07T03:56:19Z INF |  https://suffering-outreach-caps-sponsorship.trycloudflare.com                             |\n",
            "\n",
            "üîó –ü—É–±–ª–∏—á–µ–Ω URL –∫—ä–º Streamlit —Ç–∞–±–ª–æ—Ç–æ: https://suffering-outreach-caps-sponsorship.trycloudflare.com\n"
          ]
        }
      ],
      "source": [
        "# üßπ 1. –°–ø–∏—Ä–∞–Ω–µ –Ω–∞ –≤—Å–∏—á–∫–∏ –ø—Ä–µ–¥–∏—à–Ω–∏ –ø—Ä–æ—Ü–µ—Å–∏ –Ω–∞ Streamlit\n",
        "!pkill -f streamlit || echo \"–ù—è–º–∞ –∞–∫—Ç–∏–≤–Ω–∏ –ø—Ä–æ—Ü–µ—Å–∏ –Ω–∞ Streamlit\"\n",
        "\n",
        "# üöÄ 2. –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Streamlit —Ç–∞–±–ª–æ—Ç–æ –≤—ä–≤ —Ñ–æ–Ω–æ–≤ —Ä–µ–∂–∏–º\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "import re\n",
        "\n",
        "def run_streamlit():\n",
        "    # Use the correct port 8501 as used by cloudflared tunnel\n",
        "    os.system(\"streamlit run forex_dashboard.py --server.port 8501\")\n",
        "\n",
        "# Start Streamlit in a separate thread\n",
        "threading.Thread(target=run_streamlit).start()\n",
        "\n",
        "# ‚è≥ 3. –ò–∑—á–∞–∫–≤–∞–Ω–µ Streamlit –¥–∞ —Å–µ —Å—Ç–∞—Ä—Ç–∏—Ä–∞\n",
        "time.sleep(5)\n",
        "\n",
        "# üåê 4. –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Cloudflare Tunnel –∏ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—É–±–ª–∏—á–µ–Ω URL\n",
        "print(\"üîÑ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Cloudflare Tunnel...\")\n",
        "# Ensure cloudflared executable is in the current directory\n",
        "process = subprocess.Popen([\"./cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "\n",
        "# üì° 5. –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—É–±–ª–∏—á–Ω–∏—è URL –æ—Ç –∏–∑—Ö–æ–¥–∞\n",
        "print(\"–ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—É–±–ª–∏—á–µ–Ω URL...\")\n",
        "tunnel_url = None\n",
        "while True:\n",
        "    line = process.stdout.readline().decode()\n",
        "    if not line:\n",
        "        break # Exit loop if there's no output\n",
        "    print(line.strip())\n",
        "    match = re.search(r\"https://.*\\.trycloudflare\\.com\", line)\n",
        "    if match:\n",
        "        tunnel_url = match.group(0)\n",
        "        print(f\"\\nüîó –ü—É–±–ª–∏—á–µ–Ω URL –∫—ä–º Streamlit —Ç–∞–±–ª–æ—Ç–æ: {tunnel_url}\")\n",
        "        break\n",
        "    # Add a small delay to avoid busy-waiting\n",
        "    time.sleep(0.1)\n",
        "\n",
        "if tunnel_url is None:\n",
        "    print(\"üö´ –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ø—É–±–ª–∏—á–µ–Ω URL. –ú–æ–ª—è, –ø—Ä–æ–≤–µ—Ä–µ—Ç–µ –∏–∑—Ö–æ–¥–∞ –∑–∞ –≥—Ä–µ—à–∫–∏.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai\n"
      ],
      "metadata": {
        "id": "Ztka8i4XAFCD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "VGDrMEYjANob"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "def explain_agent_action(obs, action, current_price, account_balance, open_position_units):\n",
        "    prompt = f\"\"\"\n",
        "    –¢—Ä–µ–π–¥–∏–Ω–≥ –∞–≥–µ–Ω—Ç –Ω–∞–ø—Ä–∞–≤–∏ –¥–µ–π—Å—Ç–≤–∏–µ: {action}.\n",
        "    –ù–∞–±–ª—é–¥–µ–Ω–∏–µ: {obs}\n",
        "    –¢–µ–∫—É—â–∞ —Ü–µ–Ω–∞: {current_price}\n",
        "    –ë–∞–ª–∞–Ω—Å –ø–æ —Å–º–µ—Ç–∫–∞—Ç–∞: {account_balance}\n",
        "    –†–∞–∑–º–µ—Ä –Ω–∞ –æ—Ç–≤–æ—Ä–µ–Ω–∞ –ø–æ–∑–∏—Ü–∏—è: {open_position_units}\n",
        "\n",
        "    –û–±—è—Å–Ω–∏ –∑–∞—â–æ –º–æ–∂–µ –¥–∞ –µ –≤–∑–µ—Ç–æ —Ç–æ–≤–∞ —Ä–µ—à–µ–Ω–∏–µ, –∫–∞—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞—à —Ç—Ä–µ–π–¥—ä—Ä—Å–∫–∞ –ª–æ–≥–∏–∫–∞, –±–∞–∑–∏—Ä–∞–Ω–∞ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–µ–Ω–∞—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "GGiLfBYcA7_N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da62fb41",
        "outputId": "e42de715-dc28-4234-8cd8-ba8945381526"
      },
      "source": [
        "%%writefile main.py\n",
        "# Example entry point for running training or backtesting outside of Streamlit\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Import functions and classes from your local modules\n",
        "from data_utils import download_forex_data, add_technical_indicators, split_data\n",
        "from env.forex_env import ForexTradingEnv, calculate_metrics # Corrected import path\n",
        "from agent_utils import create_agent, train_agent, load_agent_by_type\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# --- Configuration ---\n",
        "# You can define configuration parameters here or load them from a file\n",
        "DATA_TICKER = \"EURUSD=X\"\n",
        "DATA_START_DATE = \"2022-01-01\"\n",
        "DATA_END_DATE = \"2023-01-01\"\n",
        "DATA_TIMEFRAME = \"1d\"\n",
        "LOOKBACK_WINDOW = 20\n",
        "INITIAL_AMOUNT = 100000.0\n",
        "TRAIN_SPLIT_RATIO = 0.8\n",
        "AGENT_TYPE = \"PPO\" # Or \"A2C\", \"DQN\"\n",
        "TOTAL_TIMESTEPS = 50000\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab_Models\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Colab_Checkpoints\"\n",
        "CHECKPOINT_FREQ = 5000\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Starting main script...\")\n",
        "\n",
        "    # 1. Download and process data\n",
        "    print(\"Downloading and processing data...\")\n",
        "    raw_data = download_forex_data(DATA_TICKER, DATA_START_DATE, DATA_END_DATE, DATA_TIMEFRAME)\n",
        "\n",
        "    if raw_data is None or raw_data.empty:\n",
        "        print(\"Failed to download or process raw data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    processed_data = add_technical_indicators(raw_data, atr_window=14) # Assuming default ATR window 14\n",
        "\n",
        "    if processed_data is None or processed_data.empty:\n",
        "        print(\"Failed to add technical indicators or processed data is empty. Exiting.\")\n",
        "        return\n",
        "\n",
        "    train_data, test_data = split_data(processed_data, TRAIN_SPLIT_RATIO)\n",
        "\n",
        "    if train_data is None or train_data.empty:\n",
        "        print(\"Failed to split data or train data is empty. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Data loaded and split. Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
        "\n",
        "\n",
        "    # 2. Create and train agent\n",
        "    print(f\"Creating and training {AGENT_TYPE} agent...\")\n",
        "\n",
        "    # Define environment parameters (adjust as needed)\n",
        "    env_params = {\n",
        "        'initial_amount': INITIAL_AMOUNT,\n",
        "        'lookback_window': LOOKBACK_WINDOW,\n",
        "        'buy_cost_pct': 0.001,\n",
        "        'sell_cost_pct': 0.001,\n",
        "        'max_drawdown_limit_pct': 0.10,\n",
        "        'position_size_pct': 0.1, # Example value\n",
        "        'stop_loss_pct': 0.02,\n",
        "        'take_profit_pct': 0.04,\n",
        "        'trailing_sl_pct': 0.005,\n",
        "        'lot_model': 'percent_of_capital', # Example lot model\n",
        "        'tp_reward_bonus': 0.01,\n",
        "        'sl_penalty': 0.01\n",
        "    }\n",
        "    # Create the environment for training\n",
        "    train_env_instance = ForexTradingEnv(df=train_data.copy(), **env_params) # Corrected class name\n",
        "    vec_train_env = make_vec_env(lambda: train_env_instance, n_envs=1) # Use make_vec_env for compatibility\n",
        "\n",
        "    # Define agent parameters (adjust as needed or load from config)\n",
        "    agent_params = {\n",
        "        # Example PPO params, customize based on AGENT_TYPE\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"n_steps\": 2048,\n",
        "        \"batch_size\": 64,\n",
        "        \"n_epochs\": 10,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"verbose\": 1 # Set verbose to 1 to see training output in the console\n",
        "    }\n",
        "\n",
        "    # Try loading existing agent/checkpoint first\n",
        "    print(f\"Attempting to load existing agent or latest checkpoint for {AGENT_TYPE}...\")\n",
        "    loaded_agent = load_agent_by_type(\n",
        "        path=os.path.join(SAVE_DIR, f\"valkyrie_{AGENT_TYPE.lower()}_model\"),\n",
        "        env=vec_train_env,\n",
        "        agent_type=AGENT_TYPE,\n",
        "        checkpoint_dir=CHECKPOINT_DIR # load_agent_by_type checks checkpoint_dir first\n",
        "    )\n",
        "\n",
        "    if loaded_agent:\n",
        "        print(f\"Successfully loaded agent/checkpoint for {AGENT_TYPE}. Continuing training.\")\n",
        "        agent_to_train = loaded_agent\n",
        "    else:\n",
        "        print(f\"No existing agent/checkpoint found for {AGENT_TYPE}. Creating a new agent.\")\n",
        "        agent_to_train = create_agent(AGENT_TYPE, vec_train_env, agent_params=agent_params)\n",
        "\n",
        "    if agent_to_train is None:\n",
        "        print(f\"Failed to create or load agent for {AGENT_TYPE}. Exiting training phase.\")\n",
        "        return\n",
        "\n",
        "    # Train the agent\n",
        "    print(\"Starting agent training...\")\n",
        "    trained_agent = train_agent(\n",
        "        agent_to_train,\n",
        "        total_timesteps=TOTAL_TIMESTEPS,\n",
        "        save_dir=SAVE_DIR,\n",
        "        checkpoint_dir=CHECKPOINT_DIR,\n",
        "        save_freq=CHECKPOINT_FREQ\n",
        "    )\n",
        "\n",
        "    if trained_agent:\n",
        "        print(\"Agent training finished.\")\n",
        "        # Optionally, you can add backtesting logic here using the test_data and the trained_agent\n",
        "        # from forex_dashboard import run_backtest # Assuming you move run_backtest here or implement similar logic\n",
        "        # backtesting_results, performance_metrics, trades_log = run_backtest(trained_agent, test_data, env_params)\n",
        "        # print(\"\\nBacktesting Metrics:\")\n",
        "        # print(performance_metrics)\n",
        "    else:\n",
        "        print(\"Agent training failed.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcC_HZZfaGE",
        "outputId": "ea8f480b-5603-4c47-c42c-abd2c1fe582d"
      },
      "source": [
        "%%writefile agent_utils.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
        "import gymnasium as gym\n",
        "import glob\n",
        "\n",
        "# Define a simple callback to update the progress bar in Streamlit\n",
        "class ProgressCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that updates a Streamlit progress bar and status text\n",
        "    during the training process.\n",
        "    \"\"\"\n",
        "    def __init__(self, progress_bar=None, status_text=None, verbose=0):\n",
        "        super(ProgressCallback, self).__init__(verbose)\n",
        "        self.progress_bar = progress_bar\n",
        "        self.status_text = status_text\n",
        "        self.total_timesteps_in_episode = 0\n",
        "        self.current_timesteps_in_episode = 0\n",
        "        self._prev_timesteps = 0\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        if hasattr(self.training_env, 'get_attr') and isinstance(self.training_env.get_attr('df', indices=0)[0], pd.DataFrame) and hasattr(self.training_env.get_attr('lookback_window', indices=0)[0], '__int__'):\n",
        "             df_len = len(self.training_env.get_attr('df', indices=0)[0])\n",
        "             lookback_window = self.training_env.get_attr('lookback_window', indices=0)[0]\n",
        "             self.total_timesteps_in_episode = max(0, df_len - lookback_window)\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.progress_bar is not None:\n",
        "            if self.locals.get('total_timesteps') is not None and self.locals['total_timesteps'] > 0:\n",
        "                 progress_value = min(1.0, self.num_timesteps / self.locals['total_timesteps'])\n",
        "                 self.progress_bar.progress(progress_value)\n",
        "\n",
        "\n",
        "        if self.status_text is not None:\n",
        "             if self.locals.get('total_timesteps') is not None:\n",
        "                  self.status_text.text(f\"–û–±—É—á–µ–Ω–∏–µ –≤ –ø—Ä–æ–≥—Ä–µ—Å: {self.num_timesteps}/{self.locals['total_timesteps']} —Å—Ç—ä–ø–∫–∏\")\n",
        "             else:\n",
        "                  self.status_text.text(f\"–û–±—É—á–µ–Ω–∏–µ –≤ –ø—Ä–æ–≥—Ä–µ—Å: {self.num_timesteps} —Å—Ç—ä–ø–∫–∏\")\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "# Function to create an agent instance\n",
        "# @st.cache_resource\n",
        "def create_agent(agent_type, env, agent_params=None):\n",
        "    \"\"\"\n",
        "    Creates an instance of a Stable-Baselines3 RL agent.\n",
        "\n",
        "    Args:\n",
        "        agent_type (str): The type of agent to create ('PPO', 'A2C', 'DQN').\n",
        "        env (gym.Env): The environment to train the agent on (should be a VecEnv).\n",
        "        agent_params (dict, optional): Dictionary of agent-specific parameters. Defaults to None.\n",
        "                                       Expected format: {param1: value1, ...}.\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm: The created agent instance, or None if creation fails.\n",
        "    \"\"\"\n",
        "    st.write(f\"‚öôÔ∏è –û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç —Ç–∏–ø: {agent_type}\")\n",
        "    model = None\n",
        "    try:\n",
        "        if agent_type == \"PPO\":\n",
        "            ppo_defaults = {\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"n_steps\": 2048,\n",
        "                \"batch_size\": 64,\n",
        "                \"n_epochs\": 10,\n",
        "                \"gamma\": 0.99,\n",
        "                \"gae_lambda\": 0.95,\n",
        "                \"clip_range\": 0.2,\n",
        "                \"verbose\": 0\n",
        "            }\n",
        "            final_ppo_params = {**ppo_defaults, **(agent_params if agent_params is not None else {})}\n",
        "            model = PPO(\"MlpPolicy\", env, **final_ppo_params)\n",
        "            st.write(\"‚úÖ PPO –∞–≥–µ–Ω—Ç —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "        elif agent_type == \"DQN\":\n",
        "            dqn_defaults = {\n",
        "                \"learning_rate\": 1e-4,\n",
        "                \"buffer_size\": 10000,\n",
        "                \"learning_starts\": 100,\n",
        "                \"batch_size\": 32,\n",
        "                \"gamma\": 0.99,\n",
        "                \"train_freq\": 1,\n",
        "                \"gradient_steps\": 1,\n",
        "                \"verbose\": 0\n",
        "            }\n",
        "            final_dqn_params = {**dqn_defaults, **(agent_params if agent_params is not None else {})}\n",
        "            if not isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                 st.error(f\"üö´ DQN requires a Discrete action space, but the environment has {type(env.action_space)}. Cannot create DQN agent.\")\n",
        "                 return None\n",
        "\n",
        "            model = DQN(\"MlpPolicy\", env, **final_dqn_params)\n",
        "            st.write(\"‚úÖ DQN –∞–≥–µ–Ω—Ç —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "        elif agent_type == \"A2C\":\n",
        "            a2c_defaults = {\n",
        "                \"learning_rate\": 7e-4,\n",
        "                \"n_steps\": 5,\n",
        "                \"gamma\": 0.99,\n",
        "                \"gae_lambda\": 0.95,\n",
        "                \"vf_coef\": 0.25,\n",
        "                \"ent_coef\": 0.01,\n",
        "                \"verbose\": 0\n",
        "            }\n",
        "            final_a2c_params = {**a2c_defaults, **(agent_params if agent_params is not None else {})}\n",
        "            model = A2C(\"MlpPolicy\", env, **final_a2c_params)\n",
        "            st.write(\"‚úÖ A2C –∞–≥–µ–Ω—Ç —Å—ä–∑–¥–∞–¥–µ–Ω.\")\n",
        "\n",
        "        else:\n",
        "            st.error(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç: {agent_type}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ {agent_type}: {e}\")\n",
        "        st.exception(e)\n",
        "        model = None\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to train the agent\n",
        "# @st.cache_resource\n",
        "def train_agent(agent, total_timesteps=10000, progress_bar=None, status_text=None, save_dir=\"/content/drive/MyDrive/Colab_Models\", checkpoint_dir=\"/content/drive/MyDrive/Colab_Checkpoints\", save_freq=5000):\n",
        "    \"\"\"\n",
        "    Trains the provided Stable-Baselines3 agent with checkpointing.\n",
        "\n",
        "    Args:\n",
        "        agent (stable_baselines3.common.base.BaseAlgorithm): The agent instance to train.\n",
        "        total_timesteps (int): The total number of timesteps to train for.\n",
        "        progress_bar (streamlit.delta_generator.DeltaGenerator, optional): Streamlit progress bar object. Defaults to None.\n",
        "        status_text (streamlit.delta_generator.DeltaGenerator, optional): Streamlit text object for status updates. Defaults to None.\n",
        "        save_dir (str): Directory to save the final model. Defaults to \"/content/drive/MyDrive/Colab_Models\".\n",
        "        checkpoint_dir (str): Directory to save training checkpoints. Defaults to \"/content/drive/MyDrive/Colab_Checkpoints\".\n",
        "        save_freq (int): Frequency (in timesteps) of saving checkpoints. Defaults to 5000.\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm: The trained agent instance, or None if training fails.\n",
        "    \"\"\"\n",
        "    agent_type = type(agent).__name__\n",
        "    model_name = f\"valkyrie_{agent_type}_model\"\n",
        "\n",
        "    st.write(f\"üß† –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∑–∞ {agent_type} –∞–≥–µ–Ω—Ç –∑–∞ {total_timesteps} —Å—Ç—ä–ø–∫–∏...\")\n",
        "    trained_model = None\n",
        "\n",
        "    if agent is None:\n",
        "        st.error(\"üö´ train_agent: –ü–æ–ª—É—á–µ–Ω –µ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω (None) –∞–≥–µ–Ω—Ç –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.\")\n",
        "        return None\n",
        "    if not hasattr(agent, 'env') or agent.env is None:\n",
        "         st.error(\"üö´ train_agent: –ê–≥–µ–Ω—Ç—ä—Ç –Ω–µ –µ —Å–≤—ä—Ä–∑–∞–Ω —Å –≤–∞–ª–∏–¥–Ω–∞ —Å—Ä–µ–¥–∞.\")\n",
        "         return None\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Create the ProgressCallback instance\n",
        "        progress_callback_instance = ProgressCallback(progress_bar=progress_bar, status_text=status_text)\n",
        "\n",
        "        # Create the CheckpointCallback instance\n",
        "        # Ensure checkpoint directory exists\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            st.warning(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ç–∞ –∑–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {checkpoint_dir}. –û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ...\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            st.write(f\"–°—ä–∑–¥–∞–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ —á–µ–∫–ø–æ–π–Ω—Ç–∏: {checkpoint_dir}\")\n",
        "\n",
        "        checkpoint_callback_instance = CheckpointCallback(\n",
        "            save_freq=save_freq,\n",
        "            save_path=checkpoint_dir,\n",
        "            name_prefix=f\"{agent_type}_checkpoint\",\n",
        "            save_replay_buffer=True, # Save replay buffer for DQN\n",
        "            save_vecnormalize=True # Save VecNormalize for VecEnvs\n",
        "        )\n",
        "\n",
        "        # Combine callbacks\n",
        "        callbacks = [progress_callback_instance, checkpoint_callback_instance]\n",
        "\n",
        "        st.write(\"üöÄ –ó–∞–ø–æ—á–≤–∞ –æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "        agent.learn(total_timesteps=total_timesteps, callback=callbacks)\n",
        "        st.write(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ—Ç–æ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏–∫–ª—é—á–∏.\")\n",
        "\n",
        "        trained_model = agent\n",
        "\n",
        "        # Save the final trained model with the dynamically generated name\n",
        "        final_save_path = os.path.join(save_dir, model_name)\n",
        "        save_agent(trained_model, final_save_path)\n",
        "\n",
        "        st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ –æ–±—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ –∏ –∑–∞–ø–∞–∑–µ–Ω –∫–∞—Ç–æ: {final_save_path}.zip\")\n",
        "        return trained_model\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ: {e}\")\n",
        "        st.exception(e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to save the agent\n",
        "def save_agent(agent, path):\n",
        "    \"\"\"\n",
        "    Saves the trained Stable-Baselines3 agent to a file.\n",
        "\n",
        "    Args:\n",
        "        agent (stable_baselines3.common.base.BaseAlgorithm): The agent instance to save.\n",
        "        path (str): The path (including filename, without .zip) to save the agent in Google Drive.\n",
        "    \"\"\"\n",
        "    if agent is not None:\n",
        "        try:\n",
        "            save_dir = os.path.dirname(path)\n",
        "            if not os.path.exists(save_dir):\n",
        "                 st.warning(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ç–∞ –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {save_dir}. –û–ø–∏—Ç –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ...\")\n",
        "                 os.makedirs(save_dir, exist_ok=True)\n",
        "                 st.write(f\"–°—ä–∑–¥–∞–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ: {save_dir}\")\n",
        "\n",
        "            agent.save(path)\n",
        "            st.write(f\"‚úÖ –ê–≥–µ–Ω—Ç—ä—Ç –µ –∑–∞–ø–∞–∑–µ–Ω —É—Å–ø–µ—à–Ω–æ –≤: {path}.zip\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –≤ {path}.zip: {e}\")\n",
        "            st.exception(e)\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è –ù—è–º–∞ –∞–≥–µ–Ω—Ç –∑–∞ –∑–∞–ø–∞–∑–≤–∞–Ω–µ.\")\n",
        "\n",
        "\n",
        "# Function to load the agent by type, looking for checkpoints first\n",
        "# @st.cache_resource\n",
        "def load_agent_by_type(path, env, agent_type, checkpoint_dir=\"/content/drive/MyDrive/Colab_Checkpoints\"):\n",
        "    \"\"\"\n",
        "    Loads a Stable-Baselines3 agent from a file based on its type.\n",
        "    Prioritizes loading from the latest checkpoint in checkpoint_dir if available,\n",
        "    otherwise loads from the initial model file at the specified path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The path to the initial saved agent file (.zip) or a directory containing checkpoints.\n",
        "                    If checkpoint_dir is provided, this path is used as a fallback if no checkpoints are found.\n",
        "        env (gym.Env): The environment compatible with the agent (should be a VecEnv).\n",
        "        agent_type (str): The type of agent to load ('PPO', 'A2C', 'DQN').\n",
        "        checkpoint_dir (str): Directory where training checkpoints are saved. Defaults to \"/content/drive/MyDrive/Colab_Checkpoints\".\n",
        "\n",
        "    Returns:\n",
        "        stable_baselines3.common.base.BaseAlgorithm: The loaded agent instance, or None if loading fails.\n",
        "    \"\"\"\n",
        "    st.write(f\"‚öôÔ∏è –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç —Ç–∏–ø {agent_type}...\")\n",
        "    model = None\n",
        "\n",
        "    # Try loading from the latest checkpoint first\n",
        "    if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "        checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f\"{agent_type}_checkpoint_*.zip\"))\n",
        "        if checkpoint_files:\n",
        "            # Find the latest checkpoint based on the timestep in the filename\n",
        "            latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
        "            st.info(f\"‚úÖ –ù–∞–º–µ—Ä–µ–Ω –ø–æ—Å–ª–µ–¥–µ–Ω —á–µ–∫–ø–æ–π–Ω—Ç: {latest_checkpoint}. –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç—Ç—É–∫.\")\n",
        "            try:\n",
        "                if agent_type == \"PPO\":\n",
        "                    model = PPO.load(latest_checkpoint, env=env)\n",
        "                elif agent_type == \"DQN\":\n",
        "                    if not isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                         st.error(f\"üö´ DQN requires a Discrete action space, but the environment has {type(env.action_space)}. Cannot load DQN agent from checkpoint.\")\n",
        "                         return None\n",
        "                    model = DQN.load(latest_checkpoint, env=env)\n",
        "                elif agent_type == \"A2C\":\n",
        "                    model = A2C.load(latest_checkpoint, env=env)\n",
        "                else:\n",
        "                    st.error(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç —Ç–∏–ø –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç: {agent_type}\")\n",
        "                    return None\n",
        "                st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç —Ç–∏–ø {agent_type} —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç.\")\n",
        "                return model\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç —á–µ–∫–ø–æ–π–Ω—Ç {latest_checkpoint}: {e}\")\n",
        "                st.exception(e)\n",
        "                # Fallback to loading the initial model if checkpoint loading fails\n",
        "\n",
        "\n",
        "    # If no checkpoints found or checkpoint loading failed, try loading the initial model\n",
        "    st.info(f\"‚ö†Ô∏è –ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —á–µ–∫–ø–æ–π–Ω—Ç–∏ –≤ {checkpoint_dir} –∏–ª–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ—Ç–æ —Å–µ –ø—Ä–æ–≤–∞–ª–∏. –û–ø–∏—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç: {path}\")\n",
        "    try:\n",
        "        # Construct the full path, ensuring we don't add .zip if it's already there\n",
        "        full_path = path\n",
        "        if not full_path.lower().endswith('.zip'):\n",
        "             full_path = f\"{path}.zip\"\n",
        "\n",
        "        if os.path.exists(full_path): # Check if the initial model file exists\n",
        "            if agent_type == \"PPO\":\n",
        "                model = PPO.load(full_path, env=env)\n",
        "                st.write(\"‚úÖ PPO –∞–≥–µ–Ω—Ç –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            elif agent_type == \"DQN\":\n",
        "                if not isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                     st.error(f\"üö´ DQN requires a Discrete action space, but the environment has {type(env.action_space)}. Cannot load DQN agent from initial path.\")\n",
        "                     return None\n",
        "                model = DQN.load(full_path, env=env)\n",
        "                st.write(\"‚úÖ DQN –∞–≥–µ–Ω—Ç –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            elif agent_type == \"A2C\":\n",
        "                model = A2C.load(full_path, env=env)\n",
        "                st.write(\"‚úÖ A2C –∞–≥–µ–Ω—Ç –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            else:\n",
        "                st.error(f\"‚ùå –ù–µ–ø–æ–∑–Ω–∞—Ç –∞–≥–µ–Ω—Ç —Ç–∏–ø –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç: {agent_type}\")\n",
        "                return None\n",
        "            st.success(f\"‚úÖ –ê–≥–µ–Ω—Ç —Ç–∏–ø {agent_type} —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç.\")\n",
        "            return model\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"‚ö†Ô∏è –ù–µ –µ –Ω–∞–º–µ—Ä–µ–Ω —Ñ–∞–π–ª –Ω–∞ –∞–≥–µ–Ω—Ç –∑–∞ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç: {full_path}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"üö´ –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ—Ç–æ –Ω–∞ –∞–≥–µ–Ω—Ç–∞ –æ—Ç –ø—ä—Ä–≤–æ–Ω–∞—á–∞–ª–Ω–∏—è –ø—ä—Ç {full_path}: {e}\")\n",
        "        st.exception(e)\n",
        "        return None"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting agent_utils.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}